GCN 万物之母
有复杂的谱图理论基础，用到拉普拉斯矩阵，作者还机制地做了一些没道理的小trick。

DeepWalk
就是random walk+word2vec

node2vec
基于DeepWalk，通过设计超参，让walk可以定制化地决定偏向BFSorDFSorReturn。

AGM affilication graph model 覆叠社区发现
训练一个生成模型B（V,C,M,{Pc}）。但所有参数是离散的。

BigCLAM 覆叠社区发现
对AGM改进，B(V,C,M,{Fv})，此时每个点属于某个类是个可梯度训练的概率。

GraphSAGE  图节点嵌入
用随机抽样解决GCN要获得所有邻居信息的问题，而且不用整张图计算。

GAT 图节点嵌入
对任意点计算它对不同邻居的attention，并归一化，用于信息聚合时对邻居信息的调整。

GIN GCN的改进
用sum pooling代替max or mean pooling以保证不同点映到不同嵌入，即内射.

GCPN 小分子图生成模型
是GNN+Reinforcement Learning的产物。【TODO】

MRNN 小分子图生成模型
将生成过程看作序列预测过程。
利用两个RNN，edge-RNN和node-RNN，并用SOS和EOS作为edge-RNN是否停止连接新点的标记。
并采用BFS来采样序列，以防止需要RNN记住太久的信息。

PS-VAE
【TODO】

Position-aware GNN 解决GNN的缺陷
GNN无法学习一个图中地位相同的点到不同的嵌入，所以给图中某个点做position标记，称为锚anchor，这样地位相同的点到anchor的距离不同，就能区分了。本质上是特征工程。
anchor可以是一组独立点，也可以是anchor set多个点的集合构成一个anchor。

ID-GNN 解决GNN的缺陷
Position-aware GNN解决图内的缺陷。ID-GNN解决图间的缺陷。
GNN面对长度不同的环图，嵌入是一样的。为了解决这个问题，ID-GNN在嵌入环上的某个点时，对其做标记，当计算树再找到它时，就能认出它了。对于不同的环，它再出现的位置不同，依此识别出长度不同的环。
这同样是特征工程。你可以统计计算树每层出现的“它”的数量，作为一个特征。

TranE 最基础的图节点嵌入

TransR 改良TransE
TransE无法做到1-to-n关系的表示。TransR将嵌入分成节点嵌入和关系嵌入两部分，先计算节点嵌入，再根据不同关系映射(嵌入)到不同的关系空间。

Query2box 一问多答的知识推理
它只能处理与、或、存在构成的问题，可以是连续的提问。
它定义3种基于box的函数，函数就是提问，输出就是答案。输出是个有长有宽的box，box空间内的嵌入全是答案。

Jumping Knowledge network 2018【TODO】
发现用random walk随机采样时，处于中心的点(core)总是能探索得特别宽广，以至于平均下来没学多少东西。而边缘的点常常处于一个局部树形结构(tree)，它们的random walk更稳定，也就更能学到东西。此外，2-step random walk所携带的结构信息要比更高阶的特征更有信息性，因为你走的越远，远处的不重要信息会稀释重要的近处信息。
我们知道，GNN的浅层就代表近处的信息聚合。如果我们面对工程时不知道要更注重近处还是远处，那就不如将近处远处的信息一起送给最后一层，这就是Jumping Knowledge。
该方法也能缓解前面core和tree的冲突，core的浅层信息也没多少，JK可以加强浅层信息的保留。
此时多层信息过多，就需要找办法将多层信息聚合：简单anttention；拼接后降维；max pooling；LSTM-attention。
【理论研究】
给出了一个图中节点的指标“影响力分数/影响力分布 influence score/distribution”。
并数学证明了：【TODO】
1.K层GCN中一个节点的influence distribution与random walk的k-step分布在期望上一样。
2.max pooling时，k层JK-net的任意一对点的influence score与random-walk的0~k-step到达对方的概率的线性组合的期望一样。



Louvain Algorithm 2008 社区发现算法
贪心算法。
两重循环。第一重循环将点并入其他社区，每次尽可能增加模块度Q。第二重循环将社区抽象成点，形成新图再放回第一重循环，直到第一重循环不再改变社区分类。

Fast Random Walk with restart 2007
带重启的随机游走，能给出一个自己和邻居之间的restart score，可以作为邻居的重要性来使用【TODO】

FastGCN 相比GraphSAGE更好的大图应用
https://zhuanlan.zhihu.com/p/463107215
FastGCN假设图本来无限大为G'，也就有无限个分布独立的节点v∈V'，这个图定义了一个概率空间(V',F,P)。我们得到的G是G'的子图，是从概率空间(V',F,P)中采样的一个节点集V。
如果将G看作抽样，那么GCN对节点u的特征迭代就应该写成在概率P(u)下期望的形式，即是一个积分形式。
如何获得P(u)呢？可以用蒙特卡洛方法。
【实际采样过程】
上述都是理论上的，现在说些实际的。
我先训练1个点，假设GCN有2层，那么我需要采样1和2hop的邻居节点。
我同时训练batch=256，这256个节点要聚合1-hop的节点信息，我假设256有2000个1-hop节点。我会根据2000个点的度的占比作为抽样的概率，抽出500个作为256的聚合邻居。经历第二层GCN时，继续效仿，假设抽出了200个2-hop节点。
这样，我的GCN就需要在第一层将200个2-hop节点的特征传到500个1-hop节点中，第二层GCN会把500个特征传到256个节点中。
所谓的layer-wise sampling也就是这样，是从hop的角度去采样固定数量，而不是GraphSAGE以每个节点为视角采样固定数量的邻居。



Cluster GCN 大图最好的应用
用社区发现算法获得大量小块社区，每次抽数据训练时，抽q个社区，包含它们之间的边，然后作为一个batch送到GCNs里训练。
它只是提供一种训练数据处理方法，后续训练模型可以适配任何GCNs。

Simplified GCN
将GCN的ReLU去掉，我称为乞丐版GCN，因为它公式简单，能预计算，所以没有任何算力压力。
但仍然在某些场景有不错的效果，对工业界友好。用于node classification task。

NGCF 协同过滤模型
提出了一个新的模型。由于不知道老模型都怎么做的，故不解释NGCF的特点。
NGCF是基于GCN的信息传播，加上协同过滤任务中独特的信息传递，形成的模型。

LightGCN NGCF的简化，效仿SGCN
只是简单地把NGCF的LeckyReLU激活函数and所有的线性变换去掉了，同时把节点自身信息去掉了。
LightGCN受SGCN启发，将NGCF也简化了，并通过消融实验证明该方法去掉的内容对结果影响不大。
仍然是协同过滤方面的应用。

Decagon
基于drug和protein两部分实体构成的联合异质图，旨在根据该图做drug-drug的edge prediction。用到了tensor factorization做decoder。没有特别的地方，作为一个框架可以塞其他强力的GNNs进去。

Sub-GNN 2020
提出了一个新任务：子图预测。
即给一个图，画出一个子图，让你预测子图的标签。
该任务应用于疾病预测。数据集为患者个人的phenotype/表型/性状图。某些性状的拓扑组合基于整个图的拓扑结构和内容，会产生特定的疾病。
该论文基本上沿用Position-GNN做子图的位置、邻居、结构这三个channels的定位。值得提的是，三个channels在子图内外都有定位，即既考虑子图本身情况，也考虑外部情况。

metapath2vec/metapath2vec++ 2017
这是一篇论文的两个模型，是基于random walk + skip-gram model的heterogeneous改进。且该模型强烈依赖其面对的任务，论文的具体任务是对（论文，作者，出版社，组织）这四个不同类型节点的异质图，边并没有明显的类型区分。
metapath2vec只改进了random walk方式。它定义了一些meta-path，即一些已经规定好'下一个关系是什么'的路径，以表示固定的复杂关系，例如"Author-Paper-Author"表示的是"一篇论文的共同作者"关系。
可见，meta-path策略只能对应小图。
metapath2vec++是对学习策略的改进。在skip-gram里，最后预测出现了哪些节点，预测概率是经过softmax取概率最高的前n个节点得到的。++体现在：不同类型的节点分别用各自的softmax计算。
总体上不是一个好看的模型，使用范围有限，但作为2017年的老东西可以了。

Heterogeneous Graph Attention Network 2019
这也是对已有模型从同质图到异质图的转换，采用了meta-path+attention。
主要看Figure 2。
一开始确定meta-path，假设确定了P个。然后定义meta-path based nighborhood，即若h1和h2能由某个meta-path连接，则二者是这个meta-path意义上的邻居。
【Node-Level attention】
以某个meta-path：Φ0为例。基于Φ0和中心点hi，能找到若干个hi的meta-path based nighborhood.由于meta-path相同，所以它们的类型相同，所以可以用同一个transform Matrix嵌入到某个固定空间。然后根据hi对邻居做attention，甚至做Multi-head attention。然后加权求和得到ZΦ0，这是hi基于Φ0聚合所有对应邻居的信息的向量。
不同Φj之间，由于meta-path终点的节点类型不同，所以可以适当调整transform Matrix尺寸，但要保证都嵌入到同一个空间。
以上，假如Φ一共有P个，就获得了ZΦ1~ZΦP。
【Semantic-Level Attention】
这一步是将每个ZΦi做简单的非线性变换，以作为attention分数。并加入multi-head attention，分数加和平均为最终att-score：WΦi。
然后WΦ1~WΦP再做归一化得到最终的Semantic-Level Attention。以此加权平均ZΦ1~ZΦP得到Z。
Z是中心点hi基于P个meta-path得到的一众meta-path based neighborhood的信息的综合。以此再做下游预测任务。

GATNE-T/I:Representation Learning for Attributed Multiplex Heterogeneous Network 2019
该论文来自于清华+阿里巴巴。
说起模型其实并不复杂，实际就是几个基本方法的杂糅。
【GATNE-T】
T代表Transductive Model。T处理边和点都是异质的情况，相比于I缺少节点特征，也正因如此T能作为Transductive model。
T将图分解为base embedding 和 edge embedding，前者是很多老模型的工作，后者是类似于GraphSAGE的邻域信息聚合。具体可以看P4~5公式。
值得说的是公式6，这一步是将两部分的节点hi信息加和统计了，且还有一个超参。
作者还论证了自己的公式6与MNE模型的公式7相比，自己的更优越。
【GATNE-I】
这是为了做inductive learning的T改进版本，加入了属性attribute信息。
I与T唯二的不同是base and edge embedding的初始化。将节点hi的attribute交给Hz(hi)作为初始base embedding，z基于不同的节点类型而不同。将节点hi的attribute再交给gzr(hi)，z是节点类型，r是边类型。
且不要忘了直接将attribute运算后加入最终的嵌入，其中的rou也是超参。
具体请看公式(13)。
【训练】
该模型可以作为skip-gram的信息提取部分。所以自然想到，可以用metapath2vec做训练。

Stability and Generalization of Graph Convolutional Neural Networks
偏理论。而且实用性不高。可能一辈子也用不上。【TODO】

MPNN 2017 经典论文
https://towardsdatascience.com/introduction-to-message-passing-neural-networks-e670dc103a87
上面blog是很好的介绍。
https://zhuanlan.zhihu.com/p/409399491
这是对MPNN的翻译
回想学习GraphSAGE时，它将GCN拆成几个部分，分别提出方法进行优化。这个拆的方法其实就来自于MPNN。
MPNN之前就有人研究怎么做图，有很多好的方法，后来MPNN从中摘取出最通用最好用的方法，第一步干嘛第二步干嘛，然后拼起来，所以称MPNN为一个框架，这是因为MPNN并不规定第一步"具体"干嘛，中间有个函数是要你自己设置的。
GNN是什么呢？就是MPNN所有函数设定好的一个特例。
就这么简单，没了。
MPNN的精髓就在P3的公式1和2上。

Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning 2018
这是GCN刚出的那年出的研究性论文。它研究了GCN为什么行，发现GCN其实是一种特殊的Laplacian smoothing，这在学习GCN谱图理论时已有证明。
而且该论文还发现随着GCN的深度增加，所有节点会因为聚合全图信息而趋同。
除此外，一般的图数据总是缺少大量标签，而GCN的局部滤波性和预测任务又要求大量标签，所以提出了两种数据增强的方法，co-training 和 self-training。
【co-training】最早来自于1998年的首篇半监督学习论文。细节略。
co-training利用ParWalks和GCN作为两个互相补全数据的模型进行数据扩充，进而训练。
【self-training】就是个雪球系统，用已有样本训练GCN，然后预测，把概率高的看作真实标签，加入GCN训练，循环往复。

Partially Absorbing Random Walks 2012
random walk的一种。理论功底极其扎实，看不懂。
通过拉普拉斯矩阵L，自定义的和自吸收率有关的对角矩阵A，超参数α，就可以直接计算出点i被点j吸收的概率，这个概率代表i的标签与j相同的概率。经由i对其他点的吸收概率的统计，可以发现i最有可能是某一个标签k。这就达成了预测的目的。
这一技术被用于co-training中做数据补全。

PathSim 2011
早期异质图的工作都很艰难。PathSim是第一个提出用meta-path来做异质图中点与点相似度的算法，简单却有效。注意，PathSim是基于二部图：author-paper做的算法，所以meta-path极其简单，所以并不通用。
PathSim要自己设计meta-path P，P是对称的。
PathSim针对点i和j，依据路径P统计3个值：i到j的Pij，i到i的Pii，j到j的Pjj三者的数量。并提供简单的公式s(i,j)=2Pij/(Pii+Pjj)作为ij的相似度。
论文还给出了矩阵计算的公式。

HeteSim 2014
该模型延续PathSim的名字。它的设计面对这样一个任务：衡量不同类型对象的相关性，例如作者与某顶会的相关性，若他总在该顶会发论文，则相关性应该很高。
HeteSim有3个特点：1.一致性度量：不管实体类型如何都能用同一方式度量。2.路径约束度量：基于meta-path定义一对实体的相关性。3.HeteSim是半度量测量：即满足正定性、同一性、对称性，即与度量差一个三角不等式。
直接看P9 Definition3：给一个meta-path P，给一对实体分属P的两端。in/out-neighbors内/外邻点是图论概念。看公式（1）会想到递归后如何处理两种情况：1.只剩一个点。2.只剩两个点。该问题将由下面解决。
Definition4：处理递归结束。一个点时，直接设为1（公式2）。2个点时，为了处理这种情况，论文在P10节C大篇幅论述了如何处理。
公式1本质上计算的是两点random walk下到达彼此的概率，与SimRank相似。
C：长篇大论，根本上就是在每条边上加了一个额外节点E，E保留原始边的方向。加上E后，就不存在上述问题了。
D：给了一个方便正则化HeteSim的方法，基于U和V=UT的原始邻接矩阵可以一步步得到长meta-path对实体对ab的概率。
论文剩下的部分证明了HeteSim的各种性质和数据集介绍等等内容，略。

DGI：Deep Graph Infomax 2018
这是自监督大KG上的预训练学习方法。思路是认为内部节点和所选子图之间的互信息量一定更大，外部节点反之，这才能体现节点学习到了邻居们的信息。所以采用对比学习。
这个先验判断成为DGI设计的依据。在P5的3.4给出了DGI过程。
算法内容没什么特别，选择互信息是因为它对称（对称哪里好了？），选择子图可能有点讲究。

infoGraph 2020
是DGI的改进。且提出了两个模型分别用于无监督and半监督学习。无监督方面大大超过已有模型，半监督和SOTA打平。
无监督沿用DGI并改进，将大量max和mean pooling改为sum pooling，且用GIN代替GCN，其实GIN的优点就是max改sum。还采纳Jumping knowledge network的方法，把每个节点在每一层GIN的输出做concat为该节点表示，图表示基于前者readout得来。再去maximum互信息。
还是没什么特别的。

LINE：Large-scale Information Network Embedding 2015
经典论文。给出了一阶相似和二阶相似的定义，二阶基于一阶相似度得来，与“两个点的邻居和共有邻居”有关。目的就是最大化一阶二阶相似度。
但同时该论文有很大质疑声，而且在语言数据集上的应用有欺诈，数据做过大量预处理才远超的word2vec。一阶定义不明显，二阶抄的NLP。


SDNE：structural deep network embedding 2016
算法流程在P5，图示看P3的Figure2.
该算法是一种以节点为中心的图结构的嵌入方法，并没有填入具体节点信息。
该算法可以看作是Auto-encoder-decoder。
在encoder后decoder前有互监督学习，流程本身是无监督。直接看loss function能一目了然。

DeepGCNs 2019
https://zhuanlan.zhihu.com/p/359908216
这是一种把CNNs的加深网络的技巧应用于GCN的论文，一共分仨技术：3.2~3.4 residual learning;Dense connections; Dilated Aggregation.
residual learning:就是每一层的输出要加上自己的输入再送给下一层。这个方法下的模型叫做ResGCN，能避免GCN的表示趋同。
Dense Connections:就是每一层的输出都是之前每一层和当前层的输出的concat。所以它的尺寸是逐级上涨的，当然，越深的DenseGCN block的输入内容也就越多，相当于第l层就要处理l张图，怎么处理就是block自己的问题了，可以采用mean pooling。
Dilated Aggregation：比较扯淡，就不提了，而且论文实验也没提。
至此，还有点价值的只有resGCN，denseGCN和res差不多的意思只不过方法不同，res有用是在很多论文中已经证实过的了。by the way, resGCN做到了28层和56层。
它的实验很奇葩，不是图应用，而是3d点云，一种将2d图片转化成由点组成的3d空间的任务。
（论文里的plainGCN就是不带res的普通GCN，只在最后一层汇集所有信息。vertex-wise就是逐点的意思）


DeeperGCN: All you need to train deeper gcns 2020
是DeepGCN后的工作。给出了很多有趣且实用的方法。接下来就分条叙述，会有点长。
1）聚合函数。常规上有mean max sum lstm等等。聚合函数作为MPNN的一个模块，当然可以奇思妙想出许多有道理的函数，所以作者就定义了4个新的聚合函数，其中前俩是已经有的。分别是1.具有置换不变性的信息聚合函数，为相连的两个节点计算一个共享的数字，之后作为点的权重归一化凭证。2.mean-max聚合函数，函数由参数x控制，x趋于x1时，函数整体呈现mean聚合，x趋于x2时呈现max聚合，本质上是一个调节器，调节mean和max聚合的占比。3.softmax聚合，很明显就是用softmax当作聚合权重的生成器，这在很多其他论文里已经用过了，效果很好。4.PowerMean聚合，power mean是幂平均数的意思，其实是计算了一个向量p范数，p是多少就看自己设置了。学过矩阵都知道p取特殊值对应特殊的范数，所以这个powermean其实就是把特殊范数变成一个抽象的范数而已，把问题扔给了应用人员。
2）ResGCN+，是ResGCN的模块调整后版本，原本是图聚合信息、归一化、Relu、res，现在是归一化、relu、图聚合信息、res。后者成为预激活，被实证有效。
3）GEN模型，是应用聚合函数3和4的模型。由于3和4要求feature为正值，所以需要先用式(4)做几次，然后再换3和4.然后再接式子5，完成模型。
4）DyResGEN，Dy是动态的意思，它会动态学习每一层中聚合函数的参数，3学习β，4学习p。


GraphRNN 2018
详情见CS224，把图的生成看作序列生成，通过两个rnn分别生成点序列和拓扑连接序列。

Understanding and Resolving Performance Degradation in Deep Graph Convolutional Networks 2021
这个论文貌似还有另一个名字，Effective Training Strategies for Deep Graph Neural Networks 2020
看标题知道目的，是为了解决DGCN性能下降问题，说明训练时不收敛了。
GCN分两个部分，propagation(PROP)和transformation(TRAN)，即传播操作和变换操作。随着GCN变deep，会有性能下降的问题，为了改善该问题，之前的工作会关注PROP，所以这个论文关注TRAN。
经过一系列实验，得出一个方法，叫NodeNorm节点正则化，这个是精髓，是与BatchNorm不一样的正则化方法。
在随后的一篇2020年blog中对比了让GCN加深的方法，只有NodeNorm比较好用。https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59
作者将基本的GCN拆成PROP和TRAN两部分（式2），然后分别探究两部分的效果与一起的效果，看图1可知TRAN拖后腿了。
3.3直接说他们的调查结果是方差引起了问题。所以直接提出式4的方差计算公式和式5的NodeNorm，它和一个参数p有关。在式6计算完正则化后的方差后，可以看到p>=1时才有效，可以做到缩小方差；p越小，就对方差的约束越严格。
P5还说发现了已有的LayerNorm也有逐点缩小方差的操作，且给出了式7LayerNorm公式，它有多种功能，其中的方差缩小功能正好是p=1的NodeNorm，即把方差都归到1。
4.4给出了LayerNorm的研究，发现其中的方差缩小功能才是真正起作用的功能，其他的都没啥用。
所以现在的NodeNorm是把LayerNorm中没用的部分去掉，只保留方差控制部分，然后再让它通过p可以自由调节方差控制大小。
可乐的是，在4.2有table2显示NodeNorm只能保证模型变deep后性能不会下降太厉害，基本持平，有时候都不如depth=2比depth=64还好。
(看来deep的效果还没有被模型真正开发。)

Do we need deep neural network? 2020
https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59
这是一篇blog，出自伦敦帝国理工的教授，他负责twitter的图机器学习研究。
他指出了deeper不代表效果好，许多论文其实都提到这点了，除了NodeNorm比较好，其他的随着deeper都不好，然而他们自己的论文在自己的数据上表现好，说明这些方法都太挑数据了。
最后还说了，有许多模型都拿有缺陷的benchmark在自己的数据上做对比，属于以己之长攻彼之短了，很不公平。


Revisiting “Over-smoothing” in Deep GCNs 2020
论文的目的是处理过平滑，提出了一种反过平滑的技巧，基本上是修改loss function以抵抗过平滑。
论文把GCNs看作一种MLP的一种，那么MLP中常用的正则化技巧能否对GCNs有用呢？可以试一试。我们对标签预测任务做实验。
3.1 平方差损失函数的min loss是式5，咱也不知道为什么是这个。然后就说式6是瑞利熵，一看和标准瑞利熵状态也不一样。
one-step Improvement：假设网络给出了预测X，想到达一个满足F范数约束的X2，要求X2的瑞利熵比X小，认为此时X2更好。X到X2分两步，先无F范数约束地梯度优化瑞利熵到X1，再从X1修改满足F范数约束到X2.在式子7和8可以依次得到瑞利熵的X导数、梯度下降后的X'、X'经c2倍增后变为X'',其瑞利熵不变，但X''的F范数可灵活变为c1.c2的公式没有给出，但自己可推。可见，这个过程多进行几次，瑞利熵会趋小，而且由于这个瑞利熵和标准瑞利熵不一样，它计算的是Trace迹，所以最终会优化到拉普拉斯算子的最大特征值。（具体原理不明，论文没说）
3.2 STEP1可以把GCN抽象为一种MLP，经过10、11，发现最后还是变成了12，也就是最原本的GCN。所以GCN是3.1结合MLP的结果。现在有了一个清晰的GCN认知。于是就可以在STEP2里进行常规的MLP反向传播优化W。
3.3 综上，GCN看作两步：STEP1，固定W，根据瑞利熵优化X，即图正则化；STEP2，固定X，优化W。本来GCN只需要优化W，现在把这份工作拆成了两份而不揉一起，然后清晰地发现了过平滑的原因出现在STEP1，做过多次后会朝着过平滑发展。这便是“层数越深，性能越差”的原因，层数深对应着STEP1多。
作者在这里设计了一个特殊的学习率n根据X的值实时变化，该公式来源于前述的某些推导。
4.1 simpleGCN(SGC)的over-smoothing
经计算，假设深度无穷，则输出会像马尔可夫链的稳定状态一样，只和转移矩阵有关，在SGC里就是和A有关，和输入x无关，输出u1是与A的最大特征值有关的特征向量。从STEP11和STEP2的拆开视角来看，SGC的STEP1由于过深而过平滑到输出只是u1，u1就和STEP2的W优化无关了，毕竟大伙输入都一样是u1，没得优化。 所以GCN的STEP2加入了非线性函数是抗平滑的作用。
4.2 反over-smoothing
由于上述分析，知道了STEP1对应的loss是朝着平滑去的，STEP2的loss是反平滑的。那就有改善平滑的思路了，在4.3.
4.3 Mean-subtraction均值减法，用于反过平滑。
它在应用于STEP1后，可以将最终的平滑状态从u1变成第二小特征值的特征向量(称为Fiedler向量)，该向量被用于谱图理论里面的图分区，以下blog里有解释，谱聚类也用到了，这就提供了一个微小的图划分功能。
https://qinyuenlp.com/article/cd6a39d6de0f/#%E5%9F%BA%E4%BA%8E%E6%AF%94%E4%BE%8B%E5%89%B2%E9%9B%86%E5%87%86%E5%88%99-Ratio-Cut-%E7%9A%84%E6%8E%A8%E5%AF%BC
该Fiedler向量有着“同类点的对应值相同”的特点，所以此时即使GCN很deep，也有一个粗糙的图划分能指导参数W做预测，特别是常规深度下，就会既有图划分信息，又有其本身的图信息聚合。

Deep graph neural networks with shallow subgraph samplers 2020
该论文关注子图采样。
之前有论文开发大图如何取局部子图训练的策略。
例如所有random walk策略，例如cluster GCN利用社区发现划分图。
1）有的对模型结构修改，例如GraphSAGE、GIN、DeeperGCN，还有利用dropout策略的正则化技术，有DropEdge、Bayesian-GDC。但这些技术只是避免训练时数据的过平滑问题，而不能保证实际推理中的过平滑问题。
2）有的论文考虑改良消息传播机制，利用图结构信息推理出更好的传播方案，例如APPNP和PPRGo，用个性化的PageRank重新定义了邻居连接；有的目标是缩短重要的多跳邻居，有了GDC和AM-GCN。
3）有的就纯采样上下功夫。其实GraphSAGE、clusterGCN也属于某种采样了。但本文的采样更有想法，因而设计出Shadow-GNN，用deep GNN+shallow sampler。
SHADOW-GNN：是一类聚合信息的跳数比GNN层数小的GNNs。论文在3.1举了两个例子，解释了为什么这样有用。
3.2 正式给了两个采样方式：k-hop sampler和PPR sampler。
k-hop sampler：以前的方法是，无向时，和GraphSAGE有微小的区别；有向时必须考虑方向，如果中心节点没有出度，则没有邻域。论文中没有提及有向无向的区别，但在确定k-hop的邻居们后，会取邻居们的induced subgraph，即他们之间存在的所有边都纳入子图，这是与graphSAGE不同的地方。
PPR sampler：这个不是作者的技术，但作者把它首次用在子图上。基本上是近似求每个点的personal pagerank，然后去掉分数低的，再求induced subgraph。
personal pagerank：pagerank会把自己的分数全部平均地发出去，接受邻居发出的分数。personal pagerank有一个中心点u，设置一个权重α，对于u的pagerank会保留1-α自己的,吸收α邻居的，对其他非u的点则只会吸收α邻居的。
评价：没什么卵用，但是shadow hop, deep GNN的思路解释的很清楚。给的两个sampler也能解决一些GraphSAGE存在的计算压力爆炸的问题，还有induced graph的想法很好，但是总感觉在哪里看过。

DropEdge：TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION 2020
看名字就猜个差不多，是用drop edge的方法处理over-smoothing的论文。如上篇论文所说，它只提供训练时的抗过平滑。
4 our method: DropEdge P4
4.1 按固定比例随机删除一些边，用式2表示。
预防过拟合：DropEdge比起dropout，其实更强，因为后者只是将node的特征权值归0，以让它不工作，但node的拓扑还在，其仍然提供了效果，所以直接做dropEdge才行。所以在图的视角，dropedge既能防止dropout过拟合，还通过减少信息传递而变相增加了图的最长路径，也就增大了GNN过平滑所需的deep。
逐层dropedge：这个可以大大增加随机性，但作者没细研究，就提了一嘴。
4.2 过平滑是指所有节点值都归到一个固定点上，而该节通过概念定义推导出过平滑是收敛到一个只存在图拓扑信息的子空间。
该定义与Revisiting “Over-smoothing” in Deep GCNs 2020中的推论一样，最后所有节点信息会收敛于只与邻接矩阵A有关的特征向量u1，当你放宽一点over-smoothing的空间，例如ε-smoothing，则从固定点放宽到子空间，它包含大量的拓扑信息，节点信息将基本没有。
4.3 讨论了DropEdge与Dropout、Drop Node、Graph-Sparsification的区别，其实是规范化了这几个专业名词的定义。我觉得它们的名字就有歧义。

Bayesian-GDC：Bayesian Graph Neural Networks with Adaptive Connection Sampling 2020
失策了，这个GDC是指graph drop-connect，而不是graph diffusion convolution。
该论文是一个框架，dropout/dropedge升级版 + bayesian=bayesian-GDC。
它可以通过逐层的输出控制逐层drop的贝叶斯概率，实现自适应。
具体来说，bayesian提供了随机的方法，GDC实现了逐层且逐维度的drop。
【公式太多，不看了】

GDC：Diffusion improves graph learning 2019
不要看到"扩散"就被吓到。
https://zhuanlan.zhihu.com/p/568501477
该论文其实是发明了一种消息传递方法GDC(图扩散卷积)，如果说它的特例是PersonalRank你就不害怕了。它根据图原本的信息构建出新的拓扑，以高效地信息传递。
2 该论文定义了广义的图扩散形式。
设节点间信息的转移矩阵为T，则有公式1得到广义图扩散S，它由于参数有约束Σθk=1而收敛。T可以有多种形式，可以是最初版本的GCN转移矩阵T_rw，也可以是谱图FFT版的T_sym，还有一种作者通过对邻接矩阵的微调，认为每个点都有回到自身的概率，而在T_sym中假如权重w_loop，得到T~sym，这与进阶的random walk思路一致。
若T=T_rw，θk=α(1-α)^k，则这是personalRank的图扩散矩阵，该矩阵表示了personalRank是怎么在节点间转移的。作者还提到了另一种特例，关于一个叫heat kernel的东西，就不说了。
所以，可以看到，该论文其实给出了一个高度抽象的图扩散计算方法，给出不同θ和T就是不同的扩散结果，然后根据S知道每个点将自己的信息按照什么比例传递给每个点。
3 GDC的真正过程
看Fig1可知，若一个一个点看，GDC是先在每个点上做扩散，得到各自的稠密的扩散稳定图，然后用某种方法去除分得信息太少的边，获得稀疏图，最后综合所有点的稀疏图，得到最终图。如果你愿意，可以保留边的权重，这对你的任务是有利的。若忽略边权重，那么也可应用于DCSBM这类要求边无权重的模型。
限制：GDC基于同质性假设，即它把所有点想的一样，图扩散得到的图完全基于拓扑。大多数数据对这个并不感冒，但在节点信息与拓扑高度绑定的任务上，例如链路预测，则GDC没有作用。
4 研究了GDC在谱图理论上的作用，发现它其实是个低通滤波器，前面提到的PPR和Heat kernel都是低通滤波。稀疏化操作对于大多数拉普拉斯矩阵的特征值没有影响，只对最大and嘴小特征值产生细微扰动，这对GNN不重要。
总结：该论文没什么需要讲的，它对GDC的谱图解释值得学习。

AM-GCN：Adaptive Multi-channel Graph Convolutional Networks 2020
AM-GCN：adaptive multi-channel GCN自适应多通道GCN
通过极端实验，作者发现GCN无法分辨拓扑信息和特征信息哪个重要，即把这俩信息混一起，GCN分不开。所以作者就手动给他分开呗，这就是multi-channel的意思。
直接看P3的Fig1。
channel1 拓扑图用原来的图结构，是普通的GCN.
channe2 特征图有重构过，先用余弦计算各点之间的相似度，然后用k近邻筛出更有用的边（该过程可以用GDC），然后再用GCN。
channel3、4 大多时候拓扑和特征是相关的，所以还不能只靠1、2channel。这两个channel共享参数W，希望用这个方法能从拓扑图和特征图中捕捉出共性的信息，最后的结果用加和平均得出共性信息向量。
现在有了3个信息，我们该实现自适应了。就是采用attention（式8和9）。注意这个attention对每个节点都计算一次，变换向量q对3个channel共享。
3.4 loss function：三部分，分别是对任务的、对共性信息的、对共性和个性的约束。对任务没什么好说；对共性信息，要求channel3和4的信息差别不大；对共性个性，要求其最好正交独立，利用的是HSIC：Hilbert-Schmidt Independence Criterion（式13）。
作者与同样使用GCN和类似ATT的模型作比较，全线领先，说明这个框架确实make sense。


Continuous Graph Neural Networks 2020
利用动力学方法重绘图。结果上看提高了GCN、GAT的效果，但作为2020年的论文还在和GAT比较，是不是欺负人了？
所以这只是提供了一种可能有用的图重绘方法，但在复杂模型上说不定起到反效果。
评价：知道就行。


APPNP：PREDICT THEN PROPAGATE: GRAPH NEURAL NETWORKS MEET PERSONALIZED PAGERANK 2019
该论文考虑利用personal pagerank做信息传递，而不是GCN传统的信息传递方式。
最初得到PPNP模型：1.先对每个节点做一层NN。2.用personal pagerank获得稳定的传播方式，在式2。3.结合前面两部到式3，获得最终节点表示。
优化后的APPNP：由于式2求的是personal pagerank的最终稳定状态，其中有求逆，该计算太庞大，无法实现，于是考虑用幂迭代的方法近似得到personal pagerank的效果，改成1.每个节点做NN。2.式子4传播k次，k趋于∞时等于式2。3.经过softmax激活函数。这是APPNP一层的方法，可以堆叠多层。k是超参数。
评价：直指消息传递环节，用personalRank直接得出无限层后的消息分布状况。超过GCN、GAT、JK等模型，说明了该方法有效。

PPRGo：Scaling Graph Neural Networks with Approximate PageRank
该论文在2.1对PPNP和APPNP做出评价，发现在大图上，即使使用APPNP设置K=10，对于大图也无法计算，而且APPNP要求整张图的拓扑要送进memory，大图做不了。 
该论文在P4流程图下方指出PPRGo比起APPNP可以预计算，APPNP的所有迭代都需要特征矩阵H的参与，而PPRGo将消息传递和特征计算分开了。
PPRGo想要削减工作量。如fig1所示：用pagerank类的方法计算消息传递量，然后只取top-k做计算，节点信息经fθ处理后参与进消息传递中。这是PPNP框架的改进。
PPRGo研究了一种近似得到PPR稳定状态的计算方法，绕开了PPNP中PPR计算复杂的问题，就可继续实现消息传递和特征计算的并行了，前者可预计算，后者的速度就会突飞猛进。
近似PPR论文：Local graph partitioning using pagerank vectors
该近似PPR算法可逐点计算其他点对自己的近似PPR，所以可以用分布式计算快速得到近似PPR。
评价：APPNP用幂迭代避免求大图逆矩阵，但失去了消息传递和特征计算二者并行的优点。PPRGo用近似PPR逐点计算PPR，保留了并行优点。


Tackling Over-Smoothing for General Graph Convolutional Networks 2020
该论文是DropEdge的作者团的论文，精细论证了over-smoothing发生的理论原因，并给出通用的解决办法，即DropEdge。



Graph Neural Networks Exponentially Lose Expressive Power For Node Classification 2019
该论文理论性地证明了over-smoothing的原因，它的本质是马尔可夫过程的唯一均衡态，且收敛速度由转移矩阵特征值决定。
该论文利用已有的图生成模型测试GCN，给出了复杂的数学证明，暂时看不下去。
【TODO】


GCNII：Simple and Deep Graph Convolutional Networks 2020
GCNII模型是vGCN的扩展，利用初始残差and恒等映射两项技术处理over-smoothing问题。
残差连接：ResNet的技术，ResGCN采用的技术，将每一层的输入直接接到输出。
初始残差：将初始输入的α倍(01或0.2)直接接到每一层的输出，以保证初始特征有足够的占比。
恒等映射：ResNet的技术。在W变换时，用β控制特征的1-β倍部分不被变换。若β足够小，则W失去效力，该模型退化为APPNP。可见GCNII使用了APPNP中幂迭代逼近PPR的技术。
只需要看式5就全明白了。
评价：该模型无论任何深度，模型都拥有保存浅层GCN网络信息的能力，所以无论16、32、64层效果都不会太差，但也不会超过浅层网络能力。该模型与其他模型一样，可以在浅层获得较好的提升，但还是无法激发出深层网络的复杂非线性函数拟合功能。这引发了一些思考，或许图根本不需要deep，相比于现实世界数据，它已经从浅层GCN获得了额外的信息，这已经足够它分类了，而不需要deep的帮助。


DAGNN：Towards Deeper Graph Neural Networks 2020
DA： deep adaptive GNN
该论文也发现了拓扑传播和特征变换这二者的纠缠关系，意识到随着深度下降，拓扑传播会逐渐压榨特征变换的表现空间，最终导致模型只提取出整张图的拓扑信息。所以尝试将这两部分独立。
在这之前，已有PPRGo优化PPNP得到独立进行的两部分，也有很多应用于不同领域，特别是小分子领域的框架，会分别计算特征、拓扑、二者混合的信息，然后综合预测。
3.1 提出一种衡量过平滑指标的函数SMV。
3.2 经实验发现在Cora数据集上，平滑值下降的特别快(fig2)，可Cora数据集很稀疏，模型若是因为传播过程而趋同，则不可能有这种情况。在fig3中用t-SNE降维可见，5层就开始局部趋同了。在fig4中可见，准确率确实随着layer到100以上而狂跌，平滑度也狂跌，说明此时才真正进入拓扑掌控信息主要地位的时候。
论文猜测，前期平滑度下降，准确率不降，是传播and变换的杂糅引起的。而且有一些数据，例如文本类别的预测，基本不需要拓扑数据，它根本不该关心它引用了谁，而该关心自己文本的内容，此时也需要DAGNN模型。
3.3 【最有价值and最难的部分】
理论分析了very deep的模型发生了什么。
它推导了经典的两个传播矩阵，并计算其无限幂次后得到的值，发现前一个的行完全相同，另一个的行与所有节点的度的平方根成正比，也就是说，任何特征在无限次幂的传播矩阵上传播，前者得到的特征为[aaa...aaa]，后者得到的是[xa,ya,za...ua,wa,pa]，前者所有特征无法区分，后者只有节点度的信息。
4 正式介绍DAGNN，fig5结合式8就能看懂。对于特征，只进行MLP。对于拓扑，采用SimpleGCN(SGC)的结构，分别获得1~k层SGC的输出。设置可学习参数S，S结合每层输出得到自己的attention，然后按照attention加和，最终经过softmax激活函数，得到最终节点表示。
【评价】
这其实就是SGC+jump knowledge network的思想+att+softmax,并没有任何新意。

GRAND：Graph Random Neural Networks for
Semi-Supervised Learning on Graphs 2020
该论文模型简单，利用dropnode处理over-smoothing，利用consistency training做数据增强进而加强泛化能力，它是一种要求特征稍微改变而模型预测的输出不变的设计。但不仅仅如此，论文做了如下设计。
random propagation随机传播：邻居节点的信息总是趋同，所以dropnode可以看作某种信息扰动，是信息增强的手段。每一层都做多次dropnode，相当于多次数据增强，再进行传播(利用P4的黄色标记公式)，再接MLP变换and预测。对于有标签节点，lossfunction既要求模型预测正确，还要求多次数据增强后的输出结果一致。对于无标签节点，还可以发挥余热，只优化后一个loss，即不管预测出什么，只要求它们的结果一致。
可以发现，这个过程解耦了传播和变换，但传播时设置了超参数K，K代表你想考虑多少次信息传播，K太大时A的幂计算可不简单。这是一个隐患。
实验证实了GRAND的有效性，并对其中组件进行替换以查看效果，在P8的table1，例如dropnode改成dropedge、dropout，MLP改成GCN、GAT，发现在大数据集上都不如老老实实用原版的dropnode和MLP。
【consistency training】
dropnode被视为数据增强，那么可以同时做S次数据增强，并同时优化它们，这叫一个batch。
【评价】
引入consistency training一致性训练和dropnode做数据增强，大大增加了模型可利用的数据，而且实验证实这些确实提升了模型效果。这是很有效的发现。



Revisiting Graph Neural Networks: All We Have is Low-Pass Filters 2019
经典论文。
看这个很有用
https://zhuanlan.zhihu.com/p/562174055
https://blog.csdn.net/mei1318874404/article/details/106412010
分解拉普拉斯矩阵的函数L_sym后，可以将信息传播看作是傅里叶变换+频率响应函数+逆傅里叶变换。将L的特征值称为频率，在堆叠多层GCNs后，该特征值矩阵堆叠而成的频率响应函数矩阵会根据L特征值而得出不同的值，如果L特征值低，经响应函数后的对应值高，则称为低通滤波器。L_sym正符合该要求。
图信号是全图节点的某一维特征，它在傅里叶变换后得到一个“频域”向量，我们将其视为不同频率的强度，L低频对应的频率强度在“低通滤波器”L_sym下会变大，而L高频对应的频率强度在“低通滤波器”L_sym下会变小，这就是采用L_sym多层GCNs被称为“低通滤波器”的原因。
而且，若多层GCNs后的输出仍有低频区强度高，则在xTLx的平滑度计算中，可以为L最小特征值代表的低频区获得相当高的加成，会保持平滑度不下降太厉害。
评价：如果不以空域频域变换作为解释，很难解释L发生了什么，L拆开后是“基变换、放缩、基逆变换”的过程，很难说基变换后到底代表什么，数学上它只是用另一个视角审视一个维度巨大的向量。



GB-GNN：Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks 2019
名词解释
multi-scale GNNs：泛指利用不同层的输出进行共同预测的GNN模型，例如论文jumping knowledge network中的模型。
GB：grand boosting
w.l.c.：weak learnability condition
论文贡献（原文翻译，在P2）：
1. （定理1、命题2）提出了通过boosting理论对直推式学习模型进行分析的方法，且推导出在w.l.c条件下的优化和泛化保证。
2.（定理2）我们给出了一个特定类型的multi-scale GNNs的测试误差边界，边界随节点聚合的数量而单调减小，即网络越深、最后一层接收前面各层输出越多，则边界越小。
3.提出模型GB-GNNs，利用boosting算法进行训练。效果与SOTA相当，理论与实际相匹配。
具体内容太多，另放文件写了。而且太复杂，我还不配读懂。


AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on Imbalanced Node Classification 2020
这是一个在基础GCN上应用adaboost的模型。
直接看fig1和式7就好理解了。
算法过程在P6.其中5和6是一种叫做SAMME算法的东西，用来调整预测值和节点权重。要注意，每一层用到的X(L)都是SGC迭代得到的，没有经过任何非线性变换。每一层初始化fθ的参数θ都继承于上一层学习完的θ，它将在节点权重变化后的本层继续学习出新的θ。fθ不是传统的W，而是步骤4和5。最终，AdaBoost针对某个节点，找到所有层的预测，将预测值加和，看看K分类中哪个类别k的值最高，那就是最终的预测结果。


DGN：Towards Deeper Graph Neural Networks with Differentiable Group Normalization 2020
DGN：Differentiable Group Normalization可微_组归一化
【论文贡献】
1. 给了两个量化GNN过平滑的指标，(1)组距离率R_group，definition 1。假设有C个标签类型，先把所有点的表示向量按照点的标签放到组里，第i个标签的组叫做Li。然后有式2能基于组内和组外的节点表示的距离来衡量组距离率。分母表示组内，分子表示组间，如果组间节点数量差距大，则R_group大。(2)实例信息增益G_ins，definition 2。具体定义在附录，看式3也大概知道原理，G_ins希望节点信息在交互后还能尽量保留多一点，如果过平滑，则G_ins小。
2.提出Differentiable Group Normalization，DGN，可以减轻过平滑。DGN可以软性地聚类节点且组独立地正则化，这样可以预防组间节点表示相近。
3.实证了deeperGNNs用DGN可以获得更高准确度。
【怎么应用DGN】
其实就是把R_group和G_ins放到loss函数里一起优化。
【怎么组标准化group normalization】
比较复杂。每个点属于某一类都有概率，汇总到矩阵S，怎么得到的就不知道了。分别取S的列，假设i列，表示每个点在分类i上的概率，与节点表示相乘，则得到分量。依此进行所有节点的正则化，再按λ比例加回原表示，注意每一类都要这么做。
【怎么计算聚类表示矩阵S】
S表示的是每个点在每一类上的概率。用式子7来解决，其中U是可训练矩阵。所以其实是在每一层都加一个预测层，并同时训练，以反哺group normalization。可以预见，这需要很大的算力。
【时间复杂度】与类别数量G线性关系。没想到并没有多费劲，想想也是，只不过是多了很多次预测+复杂的正则化而已。
【评价】奇思妙想，然后得到了实验论证，没有理论论证。超参较多，所以也不好用。



Training Graph Neural Networks with 1000 Layers 2021
该论文致力于处理GPU运行deepGNNs效率问题。
以下是原作者在知乎对改论文的简介
https://www.zhihu.com/question/458018028/answer/1875907943
该论文对几项已有技术做考察，发现reversible connection可逆连接可以以时间换空间，以增多30%训练时间的代价，减少50%的内存消耗。
下面需要介绍几种技术
【grouped convolution分组卷积】
最早见于CV领域的AlexNet模型，一张图片经过不同卷积核变成多张图片，称为多个特征。接下来会将多个特征分成多份，称为group，然后以group为单位进行接下来的卷积。
你会担心分开后还怎么进行信息交流。你可以在后面再交流，或者中间某步先交流一次。AlexNet并不会直接对原图片特征进行grouped convolution，这样会肉眼可见地切碎数据，AlexNet第一步做了两个一样但尺寸小的卷积。
【reversible connection】
最初见于RevNet，reversible residual network 2017。下面是RevNet的解释。
https://zhuanlan.zhihu.com/p/436621679
RevNet接收NICE模型对于流模型的启发，也做了一个非线性双射变换模型。公式很简单，以图片的像素为单位计算，没有卷积操作。
优点在于，他可以不保存中间过程，由于是双射变换，所以能从尾部反推到头，极大节省空间。
【deep equilibrium models】
略
【weight tying】
略
【本论文的方法】
图的特征矩阵X天生就有group的能力，每一维特征都可以单独计算，论文没有切的稀碎，而是D维等分成C份。式子2、3是前向计算，式子4、5、6是反向回推。这完全就是group+reversible connection。
除此外，GNNs不可或缺的是正则化和dropout类操作，这些如果不保存中间过程，也无法反推。为了解决此问题的内存消耗，论文令dropout模板通用于每一层，令normalization操作嵌入可逆GNN块里（就是均值方差归一化操作计算完直接添到W里）。
此处用到的GNN block是DeeperGCN，激活函数是ReLU，也能当作双射。


CS-GNN：Measuring and Improving the Use of Graph Information in Graph Neural Networks ICLR2020
CS：context-surrounding 上下文-周围 即节点自身信息和聚合来的周围信息
该论文把自己的信息和聚集的信息分开算。基于一个想法，认为聚集信息带噪声，所以考察聚合操作mean、max、sum对噪声和邻居信息的效果，发现mean能有效减小噪声功率，让有效信息占主要地位。该研究和GIN的实验结果不同。
还设计了两个指标：一个λf指标称为特征平滑度(定义3)，要求相连的特征要尽量不同，否则同质化严重就没法获取有效信息了；另一个λl指标衡量周围信息的质量，称为标签平滑度，你这么想，中心节点不知道周围节点都是什么标签，假如都和他一样，则收集的信息都有用，但如果不一样，则可能是对信息的干扰。我们希望相同标签的节点聚集在一起，所以有了λl作为标签平滑度。
我们希望λf大，特征差别大，信息就大；我们希望λl小，标签在图上聚集，干扰就小。
【CS-GNN模型】
λf和λl用在attention和dropout阶段。
第一步，由λl计算出一个值，若周围节点某个attention小于它，则dropnode。
第二步，由λf计算出一个值，以其作为节点特征的维度，因为大维度导致att波动，小维度表现力不够。
其他没什么特别的。要提一嘴，聚合器用sum，打了自己的脸。组合函数是concatenation。
【评价】提出两个指标，指导了drop机制和维度设置。

DGN：DIRECTIONAL GRAPH NETWORKS
方向图网络，解决的是分子图中的一个现象:各向异性。例如，一个原子和另一个原子的连接，可以是60°，可以是120°，它们的化学性质不同。
【具体没细看】效果不错，毕竟属于强化了GNN的表达能力。


GGCN：Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks 2021
【数学警告】
该论文发现图的异质性和多层堆叠都可以导致over-smoothing，然而大多数论文都只关注后者。
图的异质性理解为一个节点周围与他标签相同的点的浓度。图网络能有用，总是基于一个假设：相同标签的点之间总会产生联系。
该思路与CS-GNN中对不同标签的节点进行dropnode差不多，都是看出了标签不同的节点对中心节点预测起到反作用。
该论文从理论层面解释了异质性是如何与过平滑联系起来的，发现(1)一个中心节点，若自身周围有高异质性，或自身周围低异质性＆比邻居的度数低，都会诱发over-smoothing;(2)上述特点让邻居节点之间的消极信息有能力解耦异质性和过平滑之间的关系。
fig1发现，只有图的度数相对较高、同质性较高时，aggregation操作才有意义，才能更加确定自己的预测。
Theorem1和2证明了fig1中case1、2会导致模型无法分类。浅层时，case1、2导致标签A节点的特征向量向标签B节点的平均特征向量移动；深层时，低同质性高度数节点(case1的一种)向着标签不同的向量移动。
总之，浅层时case2的点出问题，浅到深层时case1的点出问题；对于低度节点多的图，一开始低度节点性能快速退化，继续训练，进入高度节点退化时刻，然而因为高度节点少，退化的就慢；对于邻接矩阵A，有不同的正则化方法，例如式子2、3中的节点度数是GCN中邻接矩阵A的renormalization技巧，又或者行正则化用于节点度数一致的情况。
Theorem3给了一个重磅结论：在独立性假设下，若网络允许消息可以选择传播的正负，可以在初始浅层阶段，使得不同类的节点之间的平均距离不受图的同质程度影响，因此极大减少错误率。【这正与CS-GNN的指标λl的功能类似，当网络可以判断信息质量的优劣，则可以选择性地加入信息】
【4 Model Design】
论文提出GGCN模型，G代表generalized。细节没看。
【评价】太难读了，数学证明过多，模型实现直接放弃。



TWIRLS：Graph Neural Networks Inspired by Classical Iterative Algorithms ICML 2021
有作者的视频讲解，在51：25
https://event.baai.ac.cn/live/179
【狄利克雷能量函数】
描述了图的同质程度，若同类点之间总是有边，不同点之间无边，则能量趋于0。GCN其实就是求解能量函数中使得能量为0的节点表示。通过直接求解迪利克雷能量=0，我们可以很简单地发现，其解要求所有节点的表示除以自己的度数函数相同，即所有节点的表示放缩后都一个样，这就是over-smoothing。
【针对上述问题的方法】
加个正则项，要求最终的节点表示与最初的节点表示之差的2范数小。
最终loss形式为min f(z)= (1-α)zTLZ+α||z-x||
【应用上述方法的模型】
发现APPNP、GCNII用了，这俩其实都用了personalRank，只不过GCNII说是初始残差。
这俩的迭代公式是一样的，是personalRank的转移公式，而转移公式正是上述方法下设计的loss的导数，所以APPNP、GCNII都是在迭代求解上述方法的最优解。
【对迪利克雷能量的发散思维】
既然这么多经典工作都是基于该能量函数，那是否能证明出更多模型、激活函数、滤波器、GAT等都是该函数的应用？
这正是本论文的工作。
【论文内容】
对于min f(z)，将其中的L改成任意L的函数γ(L)，可以发现：
（1）γ(L)=2I-L时，模型基于异质性假设，即节点总是与标签不同的节点相连。
（2）γ(L)=e^(tL)-I，且α=0.5时，z的解是GDC和GraphHeat中使用的热核Heat Kernel。
（3）若给min f(z)多加一个非负正则项n(z)，要求z非负，否则该值正无穷，γ(L)=L，则z=ReLU(GCN)
（4）若要求γ(L)半正定，且我们发现最优解z与γ(λ)所构成的一系列滤波器公式有关，λ是L的所有特征值之一，每一个λ对应一个滤波器，则可发现滤波器公式h(λ)的取值范围为[0，1]。γ(L)半正定的目的是让解空间为凸，模型BernNet是基于滤波器的解释设计的图滤波器，正是要求滤波器在[0，1]之间。
（5）若把min f(z)里的zTLz细化到每条边的smooth程度单独用函数ρ(smooth)设置，smooth程度为相邻节点表示之差的平方和得来，则发现，GAT的att权重hij是ρ对smooth求导得来。
【论文模型TWIRLS】together with iterative reweighted least squares
模型使用IRLS结合狄利克雷能量形成该模型。
公式很复杂，但模型画出来就很简单，在figure 4.
【评价】牛逼。论文完全看不下去，演讲很明白。不需要细究其中数学原理，知道结果就好。


ON THE BOTTLENECK OF GRAPH NEURAL NETWORKS AND ITS PRACTICAL IMPLICATIONS ICLR 2021
论文深度讨论了GNN的over-squashing问题，即瓶颈问题。该问题认为，GNN的网络会随着层数增加导致指数级节点增长，因而造成网络无法通过线性变换处理如此速度增长的数据，所有数据互相挤占信息空间，导致任意节点的影响都很小，没有重点。
论文给了一个简单的测试，将待测试模型的最后一次GNN迭代的图改成全连接图FA(fully-adjacent )，结果效果提升巨大。对此给出的解释是，该操作能缓解信息流压力。
【评价】以上是个很搪塞的解释，并没有直接说出为什么可行，不如说这个操作在理论上是在进一步把更多信息挤压到一起。但是同时，每个节点可以从其它节点中获取到相当比例的有用信息，在互相挤压中，反倒可能让有用信息增多。我觉得这才是FA有用的原因。

NetMF：Network Embedding as Matrix Factorization： Unifying DeepWalk, LINE, PTE, and node2vec 2018
【该论文有唐杰的汇报视频】
https://www.bilibili.com/video/BV1CZ4y1H7pp/?spm_id_from=333.337.search-card.all.click&vd_source=4f6afee7835fc1131e59cbed03895166
该框架把4个早期模型统一了，并以此提出了NetMF。
【4个早期模型简介】
LINE：不仅考虑一阶相似性，还考虑二阶相似性，就是隔一个点的邻居也要相似。
PTE：是LINE的异质网络版本。
node2vec：deepwalk+超参数调节randomwalk的深度和宽度。在社交网络上效果很好。
deepwalk：randomwalk+word2vec模型
【统一4个模型】
table1给出了四个模型经推导后的矩阵表示，发现形式差不多，且剩下的三个都是deepwalk的特例。
【NetMF】
模型以DeepWalk的矩阵形式做变换。既然4个模型都是一个模子修改后得到的，那我随便改改能否得到一个可用的模型呢？注意，我们基于Deepwalk矩阵公式的矩阵结果实际上是图上所有节点到另一点的随机游走概率，而如果抛去Deepwalk背景，该矩阵公式的矩阵结果表示的其实是任意两个节点间的相似度。
如果按照矩阵公式硬算，可以得到结果，但是图太大了，公式无法计算。我们说要修改4个模型的模子，那我们就以效率为先试试看，如此就有了algorithm3和4，是NetMF的两个版本，不同之处在于窗口T的大小，T是DeepWalk的概念，代表随机游走的步长。
其中的所有操作都是基于DeepWalk的矩阵求解公式变换而来。
【评价】
好论文，需要细细地读，慢慢地读，把公式全都看通是读完的标准。


SSGC：Simple Spectral Graph Convolution ICLR 2021
以下是作者自己写的论文思路
https://zhuanlan.zhihu.com/p/360448669
SSGC单看模型的话简单的一批，就是GDC的特殊形式。但它来自于一个复杂的思路。
【思路】
1. NetMF是一个统一了4个模型的框架。框架由DeepWalk抽象而来，且发现这个过程的第2步表现出了graph filter的操作，即deepwalk的核心是graph filter。其实所有GCN的核心都是graph filter，这来自于邻接矩阵T的矩阵分解。
2. 在GDC中，我们知道取特殊值时，GDC就是DeepWalk，等价于分解了一个graph diffusion矩阵，每一份的权重θ都一样。graph diffusion其实就是1~无穷次的信息传播带权加和，对应1~无穷层的GCN传播，即k∈1~无穷次的邻接矩阵T^k计算。
3. 有一个叫做markov diffusion kernel(MDK)的东西，它的公式就是GDC形式下的Deepwalk怼上特征。
4. 结合上述发现，你会得出一个结论，你不必再纠结于GDC的无穷，当你设一个迭代的K上限时，它会变成某种Deepwalk，且NetMF框架解释了K的意义，即K是Deepwalk的游走长度。你现在得到了一个NetMF但却写成了GDC的形式，且计算简单。这可以预想到节约巨大的算力。这一切归功于将它们分解为了基本的graph filter形式，否则很难看出来。
5. 根据4，结合3中MDK的简单形式，有了SSGC。其中并不是MDK的原样照搬，而是结合实际情况，加了k=0的情况，以保留节点自身信息。
6. 再研究发现，SSGC可以作为normalized laplacian regularization的近似解，后者同时保证邻接节点表示相近和节点表示与最初表示相近的公式。【为什么相近，按照式12说的带进去特殊值就知道了】
7. 再进一步，将SSGC的k=0部分单独拿出来赋予一个α参数，就可以自由调节自身信息量了。
【总结/缺点】该模型看起来简单，但却蕴含了许多内容，在效果不逊色APPNP的同时，计算成本和存储成本只有SGC的级别。缺点在于测试发现，如果feature本身是high dimension，则输出是稠密的high dimension，这不利于后续降维和分类工作。


BernNet：Learning Arbitrary Graph Spectral Filters via Bernstein Approximation 2020
chebNet是GCN的前身，用切比雪夫做滤波器近似。
GCN看chebNet计算太复杂，只取了切比雪夫的1、2项。
但GCN有个缺点，在频域上看，GCN有一段是负的，这会导致奇偶层转换时数据对结果有正负的快速转换，不利于训练。
bernNet采用bernstein多项式，限制了输出在[0,2]上，且其形式在频域上可以看作多个固定的基础滤波器，那么通过学习基础滤波器的权重，则可拟合任意滤波器。
评价：好论文。

Graph Neural Networks Exponentially Lose Expressive Power for Node Classification ICLR 2020
两个日本人的论文，数学基础极为扎实。牛逼。
理论证明了GCN随着K层加深，节点特征会收敛到一个子空间，且收敛与邻接矩阵有关，收敛速度呈指数级。
ReLU只会加重收敛速度。
评价：别去看论文，知道结果就行，你看不懂。

A Note on Over-Smoothing for Graph Neural Networks ICLR 2020
作者沿着Graph Neural Networks Exponentially Lose Expressive Power for Node Classification的继续研究over-smoothing。
上个论文我都没看，这个论文找不到解读之前我是不会看的。【TODO】

GPR-GNN：ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK ICLR 2021
该论文是基于空域卷积的简单模型，是基于APPNP的变种，认为appnp中采用的PPR劣于generalized PageRank，理由是前者用幂次迭代求解ppr矩阵，感受野为自身+初始残差，后者是每一层正常传播后加权。GPR-GNN还为每一层的正常传播加了个可学习权重。
公式非常简单，与一些模型基本没区别，有的模型单纯把每层输出都stacking一起再通过MLP，而GPR-GNN做出了突破，经过MLP之前先给一次权重学习，除了有大小之分，还有正负之分。
但在频域派看来，该模型是纯粹的空域卷积，没有任何解释性。用频域视角，该模型是一个在空域的滤波器操作。
【评价】generalized pagerank是论文作者2019年的论文，其他模型的影子太多，且改动没有足够的解释，故争议较大。

PairNorm：Tackling Oversmoothing in GNNs ICLR 2020
很简单的模型，只对loss做改变。
我们假设是同配图，目的是相邻的点相似，不邻的点不相似。
PairNorm让相邻点的2范数之和+不同点的2范数之和为一常数，当模型训练时，会让前者减小↓，因此后者会自然增大↑，满足我们的要求。该常数可由初始节点特征的PairNorm值获得，论文也给出了其近似值2n^2s^2（式12），s是超参数，n是节点数。


1. Supervised Neural Networks for the Classification of Structures. IEEE TNN 1997.
该论文是第一个用图结构的论文，有纪念意义。

2. Graphical-Based Learning Environments for Pattern Recognition. SSPR/SPR 2004
该论文可以说是GNN的雏形，具有充分的数学基础。下面的文章解释得很好。
https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#7885087
该论文想到了图的信息流通，并给出了不动点理论，证明了该信息流通是有稳态的，可以保证模型的稳定性。
模型会维护原本的节点特征x_n、迭代的节点隐态特征h_n、边的原本特征x_c、迭代的边的隐态特征h_c。综合送入神经网络，获得节点或边的隐态特征。
除了不动点理论保证模型稳定，还要保证神经网络f的稳定，这需要f是一个压缩映射，保证压缩映射的方法是让f限制隐态矩阵H的偏导矩阵(即雅可比矩阵)，使其范数小于c∈(0,1)的常数，这个c就是压缩映射要求的缩小限制。模型对其设置惩罚项，使得loss=原本loss+λ*max(雅可比矩阵的范数-c,0)，分析发现若f不是压缩函数，则max会很大。
【缺点】
不动点理论保证了模型稳定，但也让节点状态趋同了。这便是过平滑over-smoothing。
【注】原论文符号不太相同，以这里为准，看着容易一些。

3.A new model for learning in graph domains. IJCNN 2005.
【缺少资源】
论文没法下载，在这里
https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains
有blog讲解
https://zhuanlan.zhihu.com/p/370489272
【愤怒】这不就是论文2嘛，换了个名字又发出来了。可能是投不同期刊的原因。

4.Graph Neural Networks for Ranking Web Pages. WI 2005.
是论文2作者的论文的应用。
https://www.cnblogs.com/seaman1900/p/15947125.html
不需要看。


5.Automatic Generation of Complementary Descriptors with Molecular Graph Networks. J.Chem.Inf.Model. 2005. 
【缺少资源】

6. Neural Network for Graphs: A Contextual Constructive Approach. IEEE TNN 2009.
【缺少资源】
该模型从supernode的视角看待图信息的迭代问题，认为从低层到高层的信息汇聚需要衡量信息的权重。
该论文开始给GNN加东西了，单纯的信息加和已经不能满足现实需要了。


7.Spectral Networks and Locally Connected Networks on Graphs. ICLR 2014. 
该论文是GCN的早期工作，将GNN推广到了谱图领域。

8.Deep Convolutional Networks on Graph-Structured Data. arxiv 2015. 
该论文是论文7的延续，以应用在更实际的工程上。
其实就是有些数据没有现成的图结构，作者用相似矩阵or核函数度量距离矩阵，以构建图结构。
可以不看。


9.Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. NIPS 2016. 
这就是大名鼎鼎的chebNet，在频域用切比雪夫进行滤波的模型。

10.Diffusion-Convolutional Neural Networks. NIPS 2016.
该论文比论文14、Kipf的GCN还要早一年。
它的做法看起来很笨。
一个节点，分别找到它的1~K-hop邻居，k∈1~K，挨个计算k-hop邻居特征的加和均值。如此，有了自己的、1-hop的、...、K-hop的节点特征集。
如果你想预测一个点的标签，拿出该点的0~Khop特征，尺寸应为（K+1）*，L为节点固有的特征维度。然后设一相同尺寸的W，与（K+1）-hop节点特征做点乘，并分别加和L个维度的值，作为节点最终的表示，再拿着这L*1尺寸的节点表示做预测。

11. Gated Graph Sequence Neural Networks. ICLR 2016.
【Keywords】
gate、GG-NNs
【blog】
https://blog.csdn.net/Orangetc/article/details/115986351
【做法】
这段时间全都是空域卷积，大伙都是想到什么办法就往上凑，只要有提高就行。
(1)先对齐特征维度,(2)结合邻居们获得节点的表示，(3~6)对节点信息进行GRU操作。
【评价】知道就行。现在没几个人这么做，因为只是RNN的东西照搬，很难有解释性。

12. Learning Convolutional Neural Networks for Graphs. ICML 2016.
该论文尝试把图问题转化到图像问题，以此运用成熟的卷积技术来解决图问题。
论文里的解释就是一坨屎。以下文章里有图很好地解释了算法过程。
https://mp.weixin.qq.com/s?__biz=MzA5ODUxOTA5Mg==&mid=2652576556&idx=2&sn=9ed5ed02550916382e128fe055e4d866&chksm=8b7fea77bc086361bd0e4bd1b791734f526269976d377b58ba8c7845f76bb1f27b900e3aa2da&scene=27
简述算法：1.按一定规则给每个图中节点标号。2.找几个节点作为构建序列的中心，作者取标号的前w个。3.根据这w个点做广度优先寻找邻居节点，直到找到s-1个邻居。4.将这w个子图的广度优先序列作为序列，进行特征的排列，得到l*s*w矩阵，l是节点固有特征。5.对l*s*w矩阵归一化。6.这就可以送到CNN里了。


13.Semantic Object Parsing with Graph LSTM. ECCV 2016. 
不需要细看。
Graph LSTM是将图数据应用到LSTM单元的模型。看fig3可见一斑，整个过程仿照一轮LSTM的计算方法。
该论文应用于语义分析领域，不具备通用性。


14.Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
这就是GCN，基于chebNet的简化。


15.Inductive Representation Learning on Large Graphs. NIPS 2017
这个是GraphSAGE。
之前的GCN都是基于transductive learning，当图加入新节点时，模型必须重新训练。
论文说GraphSAGE可以做归纳式学习。怎么做到的呢？通过sample采样。如果你的输入数据就是缺一块少一块，那么图中多加一个点，你的inductive learning模型也会比transductive learning更加solid。
除此外，还有对aggregation的试验，试验了GCN的max，mean，LSTM，pool，发现各有各的优势。

16.Geometric deep learning on graphs and manifolds using mixture model cnns. CVPR 2017. 
该论文提出了一个统一的框架，它允许将已有的CNN架构推广到非欧空间(图/流形)。
该论文提出了一个我没注意到的点：一个图任务的GCN用一种基于图本身的L拉普拉斯矩阵分解出D^0.5，该操作成为图的傅里叶变换。当换到另一个图上，则该GCN一定失效，因为另一个图的D是不同的。
【模型MoNet】
本文提出的MoNet，它是一种允许在非欧空间(图/流形)上设计CNN架构的通用框架。
【TODO】太难了，这是两个领域的通用框架。


17. Graph Attention Networks. ICLR 2018. 
GAT，不多做说明。

18.Covariant Compositional Networks For Learning Graphs. ICLR 2018.
【keywords】
置换不变性、协变组合网络。
【正文】
CCN，协变组合网络。 
该论文看过，一度认为是垃圾论文。
现有的GCNs为了解决置换不变性，会不计顺序地聚合邻居节点。这很正确，但作者却认为该行为是顾此失彼，丢失了一些表达能力。
【TODO】看不懂，证明太多。


19.Graph Partition Neural Networks for Semi-Supervised Classification. ICLR 2018.
【keywords】
分段传播
【abstract】
3.2指出，虽然我们假设图信息是同时传播的，但有时候信息就不是同时传播，有时候信息还只沿着固定的节点序列传播，而不是挨着的都传播。这俩现象导致了GPNN，graph partittion neural networks的出现。
论文在最后还讨论了如何做初始化节点标签和节点分类任务。
【GPNN】
假设已经有了分割的子图。
1.子图内先正常同步传播信息。
2.再在子图间传播。
这样的好处是，如果想让所有点都接受到所有点的信息，GPNN的分段传播信息数量要比同步传播的数量少。一次点到点的信息传播称为1个信息数量。
【图分割】
然后解决图分割问题。该问题是NP-hard问题。
当然可以直接用现有方法，例如谱分割spectral partition method。但太慢了。
所以作者基于multi-seed flood fill partition algorithm多种子泛洪算法做修改。不多作介绍了。
【节点特征＆分类任务】
有时候节点特征缺失，讲的是如何补全数据。
分类任务不必它多说。
【评价】也算是开了个另类的头，尝试分割图然后分两步做信息传播，然而缺少理论证明，也无法直接缓解他提出的两个问题。



20.Inference in Probabilistic Graphical Models by Graph Neural Networks. ICLR Workshop 2018. 
【keywords】
概率图模型、GG-NNs、信念传播
【正文】
该论文尝试学习出隐马尔可夫模型中的转移矩阵。都知道转移矩阵其实是一张随机图，边上写着有向的转移概率，该论文尝试用图网络推测出边的概率。
belief propagation信念传播算法是一个贝叶斯派的算法，通过整张图的计算优化，得到某节点状态的概率，但BP只用于tree结构，有loop时难以使用。
本论文所使用模型在loopy graphs上当然可以用，优于BP，且适用于比论文所用data更大的图，或结构更不同的图。
【历史工作】
信念传播BP：这种建模办法会将图变成A的样子，所有边都有变量。
双向马尔可夫随机场：该建模方法借助Ising模型or玻尔兹曼机，将图变成B的样子，结合GGNN得到message-GNN。
【Model】
论文采用Gate graph neural Networks GG-NNs以获得节点之间的长距离传播信息。
在该模型下，分别尝试了figure1中的b概率图模型(message)和c图模型(node)两种信息传播方法，实验发现结果差不多。
【评价】
该论文尝试用GNNs的方式直接推得图中节点和边的概率分布，若能成功将是个大发现，虽然暂时效果一般，但还是有发展的动力。

21.Structure-Aware Convolutional Neural Networks. NeurIPS 2018. 
不用看了。

21.Bayesian Semi-supervised Learning with Graph Gaussian Processes. NeurIPS 2018. 
图的高斯过程指用高斯分布建模图的节点特征。希望高斯过程受其本身信息和邻居信息控制，本身控制均值，本身和邻居构成方差，由式1表示。
一通看不懂的解释后，得到高斯过程等价于式3。
【TODO】数学内容太多，细看也要好几天。


22.Adaptive Graph Convolutional Neural Networks. AAAI 2018.
【keywords】
任意图结构、SGC-LL、SGCLL、分子图、图分类 
【abstract】
是针对于分子图的模型，可以以任意类型的图结构作为输入。提出了一个叫做SGC-LL的谱图卷积方法，以此创建的模型叫做AGCN。
SGC-LL：spectral graph convolution layer with graph Laplacian Learning，是一个谱图卷积核。
AGCN：adaptive GCN，基于SGC-LL构建。
【SGC-LL】
把图放进去做预测的图卷积核。具体看Algorighm 1。
X是分子图集合，取Xi为一个分子图，它自带一个Li是拉普拉斯矩阵。用式6计算分子图各点之间的距离Dij，它是对称的，M=W^TW，W是待学习参数。Dij用于计算高斯核Gij，最终得到Ahat_i是图Xi的节点相似矩阵。计算它的拉普拉斯矩阵L_res_i，这便是通过W学习来的拉普拉斯残差矩阵，在步骤4通过可学习参数α得到最终拉普拉斯矩阵L_hat，得到L_hat的谱Λ=F(L_hat)，将其带入到式4的gθ(Λ)，再结合式8得到图表示Yi。SGC-LL这里设置一个函数ΣF(L,X,Γ)^k=L_hat=gθ(Λ)，L指图原本的L，这是类似chebNet的方法。注意式8的可学习参数W和b。
至此，可发现待学习参数有M=W^TW的W是D*D维，有α是单值，有式8的W是D*D维，有b(应该)是1*D维加给所有节点特征。D是节点特征xi的维度。发现这些参数没有一个与图的尺寸有关。
一定要注意，式6、7、步骤3、4，再计算Λ，这一套才是函数F的操作。
【AGCN】
直接看fig3就行，解释一下各个模块。
graph max pooling：对于单个图Xi的所有节点最终表示Yi(N维*D维)，挨个考察节点，新特征为考察中心节点极其1-hop邻居节点的特征，更新为最大的。
graph gather：是简单的所有节点特征相加，以表示图特征。
bilateral filter：这个方法是抄的这个论文Superpixel convolutional networks using bilateral inceptions，以对抗over-fitting。
【评价】效果与以往持平，但给出了任意图的处理办法，很好。

23.Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification. WWW 2018. 
直接看这篇文章
https://blog.csdn.net/qq_45312141/article/details/106904716
基于两个假设：局部一致性，邻近点标签相同；全局一致性，邻居相同的中心节点有相同标签。
以此构建出如图figure2的模型图。下面一步步讲解。
【convA】
普通的GCN卷积。
【convP】
通过random walk生成频率矩阵F，进而生成PPMI矩阵。具体为algorithm 1：
1）设置random walk步长q一般是1，随机采样长度w，每个点走γ次random walk，初始化F为0.
2）以公式8，即均匀随机的方式走q=1步，得到一条路径S长度为w，记录S用w-1个节点对(xn,xm)。这可以有重复。
3）对每个(xn,xm)，F(n,m)+=1，F(m,n)+=1。
PPMI矩阵计算法：式9，可发现PPMI衡量的是任意两点的相关度。
【loss】
L0是正常的交叉熵loss。
Lreg要求convA和convP的输出概率相似，用均方误差。
【评价】是微小的进步，但效果很好，多个数据集比GCN高。仍不是突破性的进展。

24.Learning Steady-States of Iterative Algorithms over Graphs. ICML 2018. 
蚂蚁金服的论文。提出了一个学习框架，应对不同的图问题同时可以处理大图，且有效、高效地得到稳态解。
【SSE，stochastic steady-state algorithm】
该论文先提出了一个优化目标，具体在式9、10、11.
可以理解为式9是传播过程，10是预测过程，11是将整个过程写为带约束的最小化公式，约束要求稳态下邻居的嵌入{hu}经过Tθ的聚合等于中心节点的嵌入hv。
如何达到这个优化目标是中心问题。论文提出了一种交替机制，类似于EM算法，论文认为这看起来像强化学习RL。
（与GCN的不同在于，GCN把节点嵌入稳态过程与模型训练过程混在一起，更新也是一起，所以两部分都可能不稳定。）
【优化学习】
直接看algorithm 1。注意参数，W1W2是聚合函数Tθ的，V1V2是预测函数g的，K代表迭代次数，nh代表迭代嵌入h的次数，nf代表迭代预测值的次数。
1）在每一次计算中，分两步。
2）第一步，用公式12迭代更新h，共nh次，每次重新随机要迭代的N个节点。
3）第二步，用步骤9梯度下降法更新W1W2V1V2，每次采样M个节点，共nf次。
【评价】
普通的GCN必须保留整张图的中间过程，在BP时才能更新。而SSE将优化过程看作reinforcement learning，于是坦然地接受了随机局部更新的方法，在效果不差的同时，还只用记录部分节点的中间特征，大大减少内存压力。
且若希望从1-hop的迭代转到T-hop的迭代，SSE并不要多余的空间，而GCN却要与T成比例的空间以记录中间过程。
【疑问】还是不懂为什么它效果能好，看起来是分离了传播和预测两个过程的优化，且即使局部地更新，还能保证朝着稳态发展。


25.Graph Capsule Convolutional Neural Networks. ICML 2018 Workshop.
一眼懂这是基于胶囊网络+GCN来的。
【胶囊网络】
最初用于CV，希望胶囊网络能帮助解决图片角度的问题。
它的解释可以看这篇文章。
https://blog.csdn.net/littlle_yan/article/details/112961981
可以学习的是W，权重c是由一个固定操作推出来的，趋势是平均向量更靠近哪个输入向量，对应的权重c就越高。
【GCAPS-CNN】
不用看，没发展了，浪费时间。


26.Capsule Graph Neural Network. ICLR 2019. 
同上，略。

27.Graph Wavelet Neural Network. ICLR 2019. 
据说是发现小波变换比傅里叶变换高效，进而搞出来的模型。那么只需要知道小波变换是什么就好了。
【小波变换】
普通傅里叶变换是把一个波变成多个周期和相位不同的sin波叠加，sin波是个无穷的波。这就要求合成波一定是无穷且周期的。
而小波变换可以应对突变波。小波的特点是只在某个时间范围内有强烈的变化，其他时间特别平稳，这就能有效处理突变的波。
【图FFT的缺点】
1.图的傅里叶变换是由L分解出来的一组基向量矩阵D，它不好分解。
2.D本身是稠密的，大图的话非0值太多，计算量就大。
3.L的特征值(即频率)倾向于表现全图特征，无法突出局部特征。
【图小波变换】
我们知道，图的FFT其实只是换了个坐标系，那么图的小波变换是什么呢？能否克服FFT的缺点？
小波变换看公式3.该公式的逆为小波变换，可以轻松地对Gs=g(λs)=e^λs中的λs求负得到。
φs是n个基小波的组合，节点向量与其相乘得到的是与各个基小波的相似度，且该小波变换设置为与λ相关。s是放缩参数。整体下来，小波变换也和FFT一样是随着图拓扑而固定的形式，所以可以对不同图适配相对合适的基小波。
此时F就不是FFT中对λ频率的放缩了，而是对基小波的放缩。（注意不要疑惑为什么训练F，GCN简化了cheb，所以有能力把可训练参数移到后面的W，但chebNet就得在F上训练，因此GWNN也得这样）
【直观理解】
所谓图的基小波变换，可以看作一次固定的正负FFT操作+放缩+固定的正负FFT操作的逆。若无放缩，则前后操作抵消。


28.Deep Graph Infomax. ICLR 2019.
看这篇文章
https://zhuanlan.zhihu.com/p/58682802
DGI在42+中简单记录过，说的不清楚，这里再讲讲。
DGI是一个无监督模型，依据“相邻节点的互信息相近”的假设构建模型，认为传统的基于邻接节点相似度的训练方法不够全面。互信息可以更好衡量。
【互信息】
互信息来自于概率论和信息论，用来衡量一个随机变量中包含另一个随机变量的信息的量，可以理解为相关程度。
它的公式是I(X,Y)=KL(p(x,y)||p(x)p(y))。xy越相近，则xy越不独立，联合概率与边缘概率的分布差别越大，KL越大。
由于KL理论上无上界，所以改用JS(X,Y)=1/2KL(P||(P+Q)/2)+1/2KL(Q||(P+Q)/2)，p=p(x,y),q=p(x)p(y).可以发现JS同样符合上述规律，xy越相近JS越大。
【DGI模型】
直接看3.4的图就行。
(X,A)是全图的(节点特征矩阵，邻接矩阵)，经过C的扰动变成另一番模样(Xhat,Ahat)作为反例，C具体未知。ε 代表聚合操作，文中根据不同训练场景采用不同方式，例如直推式学习采用GCN(式3)，大图的归纳式学习是式4、5，后面还有式678用于多图的归纳式学习。R是获得图表示s的方法，作者试了很多个，发现所有节点加权平均效果最好(式9)。。。D是判别器，节点分类or图分类都一样，形式就是式10，hi是节点特征，s是图特征，即R的结果。
loss会综合正反例的输出，它的形式其实是基于更复杂的结论，在f-GAN中实现了用GAN估算各种散度的方式，f-GAN推出式(1)就是JS散度的估算，它其实就是负采样估计。
【评价】模型很简单，思路来源很复杂。

29.Predict then Propagate: Graph Neural Networks meet Personalized PageRank. ICLR 2019. 
这是APPNP。它先做出了PPNP。
【PPNP】
与普通的random walk方法不同，PPNP先做预测，然后传播。所以就如figure1一样，先由一次NN得到hi，再由personal pagerank得到稳态的传播方式，即公式2，一次性地得到无数层传播后的状态，即公式3.
它难在怎么获得personalRank的稳态，求逆实在是太难了。
【APPNP】
对PPNP改进，在于用幂迭代方法，此时有些回退到GCN的样子了，只不过现在还是PPNP的先预测、后传播，且一次APPNP层传播几次可以设置。具体看式4.
【优点】
相比于GCN，虽然零件看起来相似，但GCN基于频域的变换要求它必须每层都学习变换的参数，而APPNP只有最初的一次NN的参数，效果却丝毫不输GCN。这其实为SimpleGCN揭示了一些东西，或许根本不需要那么多参数。

30.LanczosNet: Multi-Scale Deep Graph Convolutional Networks. ICLR 2019.
核心是一个经典算法：Lanczos算法。它的功能是将一个对称矩阵正交变换成一个对称三对角矩阵，可以使L变成稀疏阵。
除此外，论文还设计了一个矩阵幂迭代方法来近似得到Lanczos算法的结果，还设计了一个可学习的谱滤波器。
【略看】知道大概就行，对写论文没有帮助。


31.Invariant and Equivariant Graph Networks. ICLR 2019. 
【先解释什么叫invariant和equivariant】
invariant不变性，指函数的输入改变时，输出不变。
equivariant等变性，指函数的输入改变时，输出也改变。
两个概念最初与CNN产生联系。CNN中，卷积操作就是等变性，而池化操作，特别是max pooling，有着近似的不变性。
如果模型有这两个性质，则有能力达成这两种要求：如果要求模型回答猫的位置，则可训练获得等变性；如果要求模型回答图中动物的种类，则可训练获得不变性。
【回到论文】
作者通过数学证明得到了一个能近似所有现有等变性模型的模型。
【数学警告】【TODO】


32.GMNN: Graph Markov Neural Networks. ICML 2019. 
看起来是用CRF建模图数据，采用GNN的传播方法作为其中某一环节的映射。模型整体利用伪似然变分EM算法优化。E-step用GNN学习node feature以正确预测节点标签，M-step用另一个GNN建模标签之间的依赖关系。
【数学警告】【TODO】

33.Position-aware Graph Neural Networks. ICML 2019. 
经典，不用多说。

34.Disentangled Graph Convolutional Networks. ICML 2019. 
【blog链接】
https://zhuanlan.zhihu.com/p/372731381
【简介】
论文认为图结构中，节点和邻居节点的连接受隐性因子控制，具体地说，一个邻居受一个隐性因子控制连接中心节点，一个隐性因子可以控制多个邻居。这其实是认为图的边也是异质的意思。
据此设计路由机制，将不同类别的邻居送入不同的聚合通道，再最后交给中心节点，一个disenConv就完成了。
【data和效果】
该论文使用的数据有6个。前3个是常见半监督图分类数据集，有标签的节点数量极少；后三个没有图特征，只有图结构，是多标签图数据，随着训练集数据从10%到90%可见各个模型性能都在上升。DisenGCN在前个任务上提升不大，但在后者提升巨大。
【DisenConv设计】
我们并不知道具体的隐性因子个数，所以只能设置超参数K为隐性因子个数，对应K个通道。
1.用transR的机制把中心节点以及邻居节点的特征分别映射到K个子空间。
2.分别计算K个子空间中，邻居映射与中心映射的相似度，此时并不归一化。
3.每个邻居的K个相似度评分做softmax的归一化。这就代表了一个邻居节点对K个通道的参与度。注意，这并不像假设的那样，认为一个邻居只与一个通道有关，而是类似于80%与通道1有关，15%与通道2有关。。。。。。
4.根据各个邻居的参与度，重新计算各个通道的embedding，并做2范数归一化。
5.各个通道的embedding拼接就是最终中心节点的embedding。
【DisenGCN】
就是DisenConv的多层堆叠。
【评价】
对边有潜在分类的图有大用。对于半监督任务只是堆叠参数造成的性能提升。


35.Stochastic Blockmodels meet Graph Neural Networks. ICML 2019. 
【什么是Stochastic Blockmodels】
简称SBM，是古早的图生成模型，它认为节点分属不同的社区，社区间和社区内节点相连存在概率，然后依照概率生成边。
具体模型看这篇文章https://zhuanlan.zhihu.com/p/485607150?utm_medium=social&utm_oi=810864260879122432
SBM形式写的很清楚，C是连续的概率，所以建议用VAE的方法来训练。
注：该论文中使用的SBMs是overlapping SBM，即允许覆叠社区存在。（从论文式1看，我没看出与基础SBM有什么区别，或许是z不需要通过softmax而是同时除以最大分量，以保证都在[0,1]区间）
【Deep Generative OSBM】
利用GCN和SGVB(特殊的VAE)加强OSBM的训练。
【先说decoder部分】
设节点特征由两部分组成，一个是bn∈{0,1}^K，是二值向量，代表节点n属于K个社区的哪几个；另一个是rn，代表节点n对各个社区的权重。zn=bn*rn点乘。设函数f(实验设为NN+relu)改变zn->fn，再计算σ(fn^Tfm)=p(nm有边)。
bn∈{0,1}^K由伯努利分布建模，利用断棍构造stick-breaking construction方法可以获得社区数K
rn就是普通的高斯分布了.
bn和rn会在encoder部分生成，且采用VAE的方式生成。
【encoder部分】
用VAE的方法得到vk、bn、rn，这里不知道vk是什么，但提到这里用的模型是SGVB，是一种特殊的VAE，所以或许是这个模型特有的参数。
在这里，vk是beta分布，bn是伯努利分布，rn是高斯分布，除了高斯分布知道在VAE里如何重参数化，剩下两个在section 4的标记部分。
其余的和VAE一样。
【loss】
在式7，很容易看懂，略。

36.Learning Discrete Structures for Graph Neural Networks. ICML 2019.
【思想】
作者发现现实世界里的图很多都残缺or有噪声，拿到的图是脏的。所以需要一个robust的模型。
联合学习是早就有的方法，如果我们能建模噪声and残缺的分布，即图中边的分布，然后基于分布抽样许多图，在这些图上得到一个稳定的GCN，那么该GCN是robust的。
【模型框架】
两个模型，一个GCN，另一个图生成器。节点和特征是固定的，边是生成的。
图生成器可以用常见的算法KNN，我们会指定K和一个距离D，对每个结点找K个最近邻，如果它们中有距离比D还大的，就扔掉。但发现这个方法的缺点是太依赖这两个超参数了。所以该论文想用伯努利分布给每个边创建概率模型，这样就有可能根据结果通过VAE的手段学习概率图模型的参数θ了。至于如何计算伯努利参数，或许一个简单的边预测模型就可以胜任。
利用图生成器得到tao个图，优化tao次GCN的参数W，此时的loss称为内目标函数。再优化外目标参数，以更新图生成器的参数θ。
比起叫做内外双层的联合模型，我更想将其看为一整个模型的上下两部分，训练时固定图生成模型优化下层GCN，再固定下层GCN优化图生成模型，两个时候的loss有稍微的不同。且由于计算期望要穷尽所有可能的图拓扑，太难算了，所以退而求其次，按照式9，优化GCN时只生成S个图拓扑，然后根据S个图拓扑优化S次W，把这S次优化的结果保存，去获得一个能在这S个GCN上综合来看获得最好结果的θ。
【疑问】仍然不知道具体的伯努利图生成模型怎么设计，用什么参数，什么优化方法。



37.MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML 2019. 
论文作者说：GCN只能学习相邻节点的关系，而不能学习邻里混合关系。不知道什么叫邻里混合关系，但从figure1可以知道大概是每层都从全图学习的意思。
【怎么全图学习呢？】
一次卷积是1-hop，多次卷积就是n-hop。
论文先给了一个叫做delta算子的东西，以2-hop delta算子为例，说它有学习更多的能力。
delta算子的能力其实体现在它可以计算减法，这就可以将GCN从同配图发展到了异配图。
以此为例，mixhop就有了思路。
【Mixhop】
直接看式4就行。很明显，这其实就是在堆参数，先堆出了n-hop，但还没看出如何引入delta算子中正负号调节的功能。
这一点是从proof of theorem2中看出来的，作者发现，在特殊的设置下，H2可以表示为delta算子的形式。
【计算复杂度】
对于A^3H，我们要先算A^3，再算A^3H，这样的时间复杂度最小。
这也很正常，A是固定的，完全可以提前计算好A，A^2,A^3......而且A还是稀疏矩阵。
【怎么表示输出？】
Mixhop最后的节点输出是包含了1~n-hop信息的，如何融合他们？
在4.1给出了答案，qk是待训练参数。
【学习邻接矩阵幂的架构】
在Mixhop中，我们每一层mixhop都要维护多个Wj，Wj对应j-hop。问题是，Wi和Wj之间的尺寸应该一样吗？或许对不同任务来说，它应该不一样。
这里没看懂什么意思，但暂且记录。
1. 构建一个宽的网络，例如设置200维。只在深度上做选择(?)。
2.训练模型，用L2 group lasso正则化Wj的每一列。L2的作用是让每个变量贴近0.
3.如果验证集准确度达到顶峰，即无法再优化时，测量每个Wj的L2范数。设一个门槛值，统计一下每个Wj的列的L2范数有几个超过了门槛值。作者选择的门槛值是能让模型收缩到与基线模型GCN尺寸相同的门槛值。
4.从各个Wj中删除一些列，其实就是保留L2范数最大的K个列。K的设置应该和3有关，但论文没有写明。
5.用正常的L2正则化代替L2 group lasso，重新训练。
【group lasso】
lasso是岭回归的核心，它用L2范数约束参数们趋于0.
group lasso是一种lasso的扩展，将参数们分好几组，组内做L2范数约束，这样设计下，模型就可以让某一组的参数归0，而让其他组的参数工作，那么归0的参数我们就不需要了。这样变相地让模型减少参数的方法叫做group lasso。


38.DEMO-Net: Degree-specific Graph Neural Networks for Node and Graph Classification. KDD 2019.
看的blog。
【思路】
作者发现传统GCNs做消息聚合，会丢失一个重要信息，就是它有几个邻居。假如你3个邻居和4个邻居聚合完是一样的值，那就不合理了。
现有的处理图同构算法有Weisfeiler-Lehman图同构测试算法，它通过给图的节点“标号->聚合->哈希->图之间比对”的方法，看在图哈希平稳的状况下，图节点是否也同质，以此判断同构。
基于此思路，提出了DEMO-net，它能做到：只有在节点特征相同、度数相同时，才有能力让两节点表示一致。
【DEMO-net】
公式5是DEMO-net的迭代式，其中fs是关于节点特征迭代，fdeg(v)与节点度有关，deg(v)就是节点v的度。
所有节点共享fs，但fdegv是与度相关的节点邻居特征压缩函数。注，fs和fdegv看似是点乘，其实是拼接concatenation。
作者设置fs是简单的全连接层+激活函数，而对fdegv有两个方法：通过全局参数Wg和度相关参数Wdegv做出度的区别；通过哈希核方法，可以缓和有些任务的度太大的问题（这里没看懂）。


39.Graph Representation Learning via Hard and Channel-Wise Attention Networks. KDD 2019. 
注：X∈R^(d*N)
【abstract】
图ATT算子(GAO)在大图上消耗太多算力。GAO属于软ATT，不是相似度一类的硬ATT，而后者表现更好。
在该论文中，提供一个硬GAO(hGAO)和一个channel级别的GAO(cGAO).
hGAO只关注重要的节点，因此让模型的消耗比用传统GAO消耗变少，且做到了更好的效果。
cGAO是为了进一步减少算力消耗，传统GAO的算力消耗与邻接矩阵A挂钩，cGAO不会这样。
hGAO在节点、图嵌入上都有比GAO更好的性能。cGAO因为大大减少算力消耗，所以可以将图模型应用于大图的ATT。
总之，从table1看，hGAO省时间，cGAO省空间。
【hGAO做法】
这个需要解释一下，在式4~10.
1.是一次对每个节点的rank分的基本计算y。
2.对每个节点：
3.根据节点i邻接的几个点的rank值，挑出前k个rank分大的，记录下其k个编号idxi。
4.根据id提取出k个点的特征。
5.根据id提取出相应点的rank值，并初步经过sigmoid压缩。
6.根据压缩后的rank值，放缩节点特征。
7.根据式1的attn计算法，可知，中心节点xi与所有放缩过的节点进行相似度计算，并softmax它们获得占比分配，最后根据占比加和邻居特征，得到最终表示zi。
注意到，这里其实是有一次微小的soft衡量，然后通过筛选过程减少整体的计算量，中间还插了一次sigmoid非线性计算以和前面的rank形成一次完整的神经网络前向传播，再通过相似度计算的简单硬计算获得最终ATT。
【cGAO做法】
用式11当作channel的ATT，然后让节点内的信息重新融合。从公式可以看出，所有信息的交互都仅限于点内，而不对点外做交互。这感觉不太有直观上的可解释性，但确实减小了计算压力。采用cGAO的GAT效果能比GraphSAGE高是很惊讶的。
【评价】
感觉我的那个想法也能像他一样，水一篇论文。


40.Graph Learning-Convolutional Networks. CVPR 2019. 
结合图生成+节点预测的模型，说后者帮助前者修复图。
在我看来，并不是一个make sense的模型。
给定的节点对于已有的节点标签，一定有一个最优的拓扑，但往往不是实际拓扑。该模型其实是在有限约束范围内找了一个更好的拓扑罢了，与实际拓扑背道而驰。
所以是一个不值得关注的模型。

41.Data Representation and Learning with Graph Diffusion-Embedding Networks. CVPR 2019.
一般来说，图扩散模型都是计算出一个最终的扩散样式作为最终节点传播表示，难点在于最终样式的获得。有些例如appnp用幂迭代近似得到最终扩散样式。
论文中提到了两个扩散方法，randomwalk with restart、拉普拉斯正则化。作者用这两个做扩散，再结合GCN框架，希望得到一个soft的传播模型。
效果上确实好了很多。
和NO.40一样，都是尝试模型配件的替换or拼接，只关注于最终效果而不关注效率。


42.Label Efficient Semi-Supervised Learning via Graph Filtering. CVPR 2019.
论文改良了两个结构。
【Generalized LP：对label propagation的改良】
古早的模型并没有太多数据，那时候的一个模型叫做label propagation，认为节点的标签可以随着拓扑传播，且相邻节点的标签Y相近。于是有了式4，认为传播后稳定的标签Z能让式4最小。α是控制二者关系的超参数。
式4有封闭解式5。通过图频域分析，发现式4做的其实是对Y的低通滤波，且是一个“输入Y、滤波、分类”的过程。
该方法将Y改成特征X，通过“数据输入、滤波、分类”的过程得到最终的Z。
（并没有什么创新，只是说明了从已有的老方法扩展到已有的方法上的直观思路）
【improve GCN】
GCN由于从chebNet简化，大大削减对滤波器的拟合能力，所以要用多层。而多层又带来过拟合。
作者粗暴地把Lsym(即式11的Ws)做k幂计算，即一层GCN传播k次，以获得更高的低频放大。
【评价】做法太牵强。



43.SPAGAN: Shortest Path Graph Attention Network. IJCAI 2019. 
https://zhuanlan.zhihu.com/p/347388487
该论文用最短路径的方式代替attention。具体操作还是得看过程。
【SPAGAN】
1.先用迪杰斯特拉算法算出图的最短路径。初始时每条路径的权重相同。
2.根据一个思想，离中心节点越近的节点，信息传递越多。所以我们设置一个超参数c，要求c跳之内的所有路径长度排个序，再设置一超参数r作为中心节点度数d的放大参数。SPAGAN会取top-k=d*r的最近路径作为信息聚合的基础路径。
3.top-k里一定有长短不一的路径。先对长度相同的路径做处理。将一条路径的所有节点的特征加和平均作为路径特征，再附上α作为attention，最后加和作为相同路径的信息聚合。式6还用了multi-head attention，设置head数为K。α并不直接是参数，而是通过式7和8计算，是常用的soft-attention。
4.通过3得到了相同长度基础路径的聚合，现通过式9将不同长度路径再通过式10的attention聚合，与3的步骤一样。
5.迭代。注意1中采用的是路径权重相同，但迭代时就该更新了，要用上一次的attention作为下一次的weight。
实验发现，只有2层迭代效果不错，这与GCN只迭代2层也吻合，但我认为这是由于权重更新的方式不稳定导致的。
【评价】
该论文的亮点在于提出了一种新的信息聚合方式。缺点在于权重更新方法不make sense，应该有更solid的更新方法，或许给每个边设计一个attention值可以做。



44.Topology Optimization based Graph Convolutional Network. IJCAI 2019. 
【abstract】
作者认为直接给的图拓扑有噪声和稀疏特性，所以直接用就不合适。而且GCN过度削减了切比雪夫的滤波器拟合能力。
作者要充分运用潜在信息修复图拓扑，提出TO-GCN。
有趣的是，该模型的滤波器是固定的，只有分类器可学习。
【3.1 网络拓扑修复】
通过标签提取一些特征本意是提供了与标签强相关的特征，等于是用答案推出答案，一般来说不予采纳。但该论文提出用标签推测出拓扑。
通过对同配图的假设“相邻节点标签相似”，得到式9，再经约束放松得到式10，其本质是要求预测Y与真实标签Z相近且相连节点的y相近，这是很常见的loss。
通过论文Labels vs. pairwise constraints: A unified view
of label propagation and constrained spectral clustering【TODO】，有结论式11的优化目标与式10相同。
式11中的ZZT元素zij表示节点ij是否属于同类，恰巧Y的目的又是希望相连节点分类相似，所以可以认为ZZT是在指明一种完美的拓扑，即只有相同类别的点互相连接，所以式11的YYT可以认为是完美拓扑的近似。
于是就有了式12，是式11的变量替换，其中O就是待优化的拓扑矩阵。
由于式12只对相同标签进行了约束，当不同标签大量相连时，不会引起loss增加，所以需要再增加约束，得到式13.
注：所有式子的元素定义都可以从上下文中找到。
【3.2TO-GCN应用】
可以先用TO-GCN获得拓扑，再在新拓扑上做其他分类预测，称作P-GCN。但不够solid。
更有道理的办法是将分类预测和TO-GCN的loss绑一起，然后迭代更新二者的参数。
【3.3优化】
这是对反向传播的优化，过程可以省略，直接看式17.
为了加速优化，可以先行优化式13。
【评价】
不如NO.41简单高效。同样是不尊重数据科学，但人家做的好。



45.Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification. IJCAI 2019. 
【keywords】
coarsening、图分类、硬收缩
【abstract】
论文目的是扩大卷积感受野，做法简介在黄标区，看不懂。
【模型算法】
作者建立了一个对称模型。
在前半部分称为coarsening，前向传播仍用传统GCN，但每一次传播后，会通过硬方法将图拓扑收缩，具体为将拓扑层面相关度高的节点捏到一起，且通过一系列公式更新拓扑矩阵、节点特征等信息。
收缩后在下一次的GCN中就相当于扩大了感受野。
收缩后再修复的后半部分称为refining，由于我们是硬收缩，所以也可以原路还原。
【coarsening】
具体在algorithm 1，很简单的过程。这里给出的是一次coarsening的流程。
1.经过GCN。
2.开始收缩。现在只看节点的拓扑，其他的什么都不看。
3.将拓扑地位相同的点先行融合，并标记其为“已融合”。
4.对剩余未标记点，计算其邻居数量(未指明是否统计3中已标记节点)，并以升序将节点排列。
5.重复6~7直到上述升序列表为空。
6.取列表首个节点vj，统计器所有邻居节点，通过式2拓扑相关计算，看点对的重要程度。具体为，两个节点度数越少，重要程度越高。
7.取vj最重要的邻居vk，将vj融合到vk里，并标记二者。将二者从升序列表里移除。注意，如果周围全都是已标记节点，那么该节点不必融合。
8.至此，所有节点都有被标记。
9.通过式3计算融合矩阵Mi，它在refining阶段会作为还原图的凭证，在Mi里会记录是谁融合到谁。可见figure2理解过程。
10.通过式4计算融合后节点特征。Gi是所有节点的特征矩阵，通过MiT计算，可见其实是将融合点对的特征简单相加。
11.通过式5计算融合后拓扑矩阵Ai。它的对角元素不一定为0.
【refining】
refining也很简单。每一层refining，都先经过GCN，然后通过式6反向还原拓扑。注意到式6先是将融合点的特征copy给被融合的两个点，然后将先前对称层的特征再加进来，意图加强融合点之间的不同之处，或者说coarsening其实是在计算融合点的相同之处。
【loss】
式9，信息熵。


46.Masked Graph Convolutional Network. IJCAI 2019. 
作者放宽了GAT的要求，额外增加了一个可训练的mask参数M在式16和17，以边为单位，对不同属性传播时的权重设置可学习参数M。
M的设计在式18，基于一个指标叫做一致性度量（黄标部分）。一致性度量是带负号的，所以中心节点j对于其邻居节点p们，在属性t上，邻居与中心差别越大，一致性越低，越说明j的这个属性区别于邻居，但又因为同配图假设，所以我们可以猜测这个属性t对于j不需要同配假设，需要我们用一个小的mask掩掉，所以式18给了一个exp(一致性度量)函数，这是个增函数，所以mask相对的也小，正好符合我们的要求。
除此外，作者还设置了一个专门对属性t的全局参数σ，以让属性t得到一个全局的调整。
所以现在可见，对于一个节点j有一个只对于它的M，它的周围节点的表示以属性为单位考察，与j的相应属性差别越大，j的mask_t就要越小。
【评价】
提升微小，不怎么样，很难说是增加参数导致的，还是真的有效。


47.Dual Self-Paced Graph Convolutional Network: Towards Reducing Attribute Distortions Induced by Topology. IJCAI 2019. 
【略读】
【DSP-GCN】
dual体现在从node-level和edge-level着手，进行self-paced learning自步学习，依照预测标签的可信度逐渐添加未标注节点作为新的数据。
【实验】
这其实是一种雪球系统，比联合学习还是差点意思。
提升不高。



48.Bayesian graph convolutional neural networks for semi-supervised classification. AAAI 2019.
【abstract】
19年扎堆出现了一批针对图拓扑不完整or有噪音的处理办法，该模型也是其中之一。针对这种情况，最直接的建模就是将图看作是一个高斯过程生成的图，自然想到用贝叶斯后验估计，在已有一个图的情况下估计图标签。
【贝叶斯变换】
式3的期望求法很好理解。我们希望充分利用已有信息，所以出现了式5。
式5希望从已观测到的图信息Gobs出发，先得到控制图拓扑生成的参数族λ，根据λ可以推测出任何图拓扑G出现的概率p(G|λ)，G再结合特征X和标签Y得到使得X->Y的W的分布，最终得到对待预测数据Z的期望。
式5无疑是复杂的，硬算要得到好多确切的连续分布才行，我们连一个都无法做到，更别说一连串了。但式5已经经过了处理，它反推可以推出左式，右式的优点会在到式6的近似推断上得到体现。
此时想到MCMC方法进行近似采样。虽然无法得到λ的分布公式进而采样，但给一个值进行具体的数值计算是可以的，所以MCMC方法会将采样工作交给另一个外部引入的简单分布，以做出近似推断。若分布公式简单，还能直接进行采样来近似，貌似式6就是这么认为才得到的。
式6是式5的蒙特卡洛近似，用V次采样将p(λ|Gobs)得到，得到的其实是V个λv。每个λv又要指导生成G拓扑，所以又要采样NG次(i=1~NG)，得到的是G_i_v。Giv最终结合X和Y采样W，这里采样S次，于是有了Wsiv。
每一次采样都是从已知变量到未知变量的一次确定，所以从Gobs开始，一步步确认到G和W，且Y是需要由W和G结合X得到的，所以Y被WGX替代，才有了式6，否则应该表示为p(Z|W,X,Y).
【具体方法】
λ到底怎么建模？W用什么模型？在这一节都会说明。
λ由模型assortative mixed membership stochastic block model提供，λ={π, β}，该模型的优点式能很好地描述相关性强的社区结构（例如paper引文网络）。
实验中发现，通过Gobs采样{π, β}会导致Giv的生成有很大方差，所以只好将{π, β}的生成看作一个必然而不是概率，我们只取argmax{π, β|Gobs}。
W就用GCN。



49.GeniePath: Graph Neural Networks with Adaptive Receptive Paths. AAAI 2019. 
【abstract】
提出了一种新的RW算法GeniePath，它可以兼顾广度与深度，分为两个结构，一学习不同尺寸邻居的重要性，二抽取并滤波从不同hops聚合来的信号。
实验表明，在大图上效果不错。
与SPAGAN想法一样，但是是靠LSTM这类带记忆的结构做。
【GeniePath做法】
可以直接看式5和6，是att-aggregation+LSTM的结构。aggregate是基本的信息聚合手段，LSTM可以为中心节点记忆不同长路径的信息，这是已经隐性分配过权重的信息，所以LSTM可以说是直接将计算好的深度信息给中心节点，并根据新信息融合出新的隐性信息，以指导下一次更深的传播。
LSTM模块可替换。论文还提出另一个版本GeniePath-lazy，具体没看，也是类似LSTM的过程。
【评价】
用LSTM等带记忆功能的RNN做隐性带权信息聚合的想法很高明，也很smooth。

50.Gaussian-Induced Convolution for Graphs. AAAI 2019.
【问题提出】
这个论文尝试处理我关心的问题。
在figure1中，作者以a~d图作为1-hop图例，告诉大家，不同的聚合方法存在的问题。
对于平均加和聚合，ab在图边权重和节点权重都不完全相同的情况下，能得到相同的聚合信息，这是问题。
有一个新的方法，按照邻居边的权重给邻居排序，按照排序分别赋予一个只与排序名次i有关的聚合函数fi。我们先把b的v3去掉，再按照新方法对bc进行计算，发现即使边的权重不同，排名和点特征相同，也会导致相同聚合信息，这又是问题。更有甚者在图d，邻边权重都一样，根本没法排序。
在图g给出了一个论文的新方法，edge-induced GMM边诱导高斯混合模型，以标记子图。具体不知怎么做，但与卷积核类似。
除此外还给了一个vertex-induced GMM，但没有在图中描述。
edge和vertex共同工作，称为Gaussian-induced convolution(GIG)高斯诱导卷积。
【GIG】
先看图2.从图上可以看出，以一点为视角，先经过edge-induced GMM进行拓扑不变的聚合，然后经过vertex-induced GMM做图的coarsening收缩，接下来迭代二者，直至收缩到一点。
【TODO】看不懂



51.Fisher-Bures Adversary Graph Convolutional Networks. UAI 2019. 
【TODO】【略】
涉及量子信息几何、流形、fisher信息等内容，不研究数学的别看。

52.N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification. UAI 2019. 
N就是gcn加强版的意思，纯粹的堆叠。
直接看P5算法过程，其实没什么特别的。
但是论证了N-GCN是无监督randomwalk和半监督GCN的结合体，所以理论上有更高的上限和通用能力。

53.Confidence-based Graph Convolutional Networks for Semi-Supervised Learning. AISTATS 2019.
【abstract】
该论文提出一种置信度机制以替代ATT机制，获得了各向异性的能力。
【confGCN】
该方法其实是Graph-based transduction with confidence 2012 的接力。
参考上述论文，本论文用基于对称曼哈顿距离的协方差矩阵来定义两个节点之间的距离。
这个公式里的对角协方差矩阵Σu和Σv其实是μu和μv改成对角阵。所以论文说，如果协方差矩阵有特征值特别大，则说明节点对自己的预测很自信，此时应该给予大的置信度，可惜此处的公式无论μu-μv差距如何，都无法在求逆的特征值下得到大的distance；如果协方差矩阵没有大特征值，则说明uv没有对自己特别自信的，所以就该赋予小置信度，此时发现除非μu-μv差距特别小，否则distance都会偏大。
总结：置信度应该大的距离小，置信度应该小的距离大。
因此对距离求个逆，r_uv=1/d(u,v)，获得r与置信度的正相关。
此时r就可以作为置信度的替代品了，作为权重参与接下来的GCN计算。
【评价】
只是引入了一个新的非对称消息聚合方式。


54.Lovasz Convolutional Networks. AISTATS 2019.
这是一篇基于特定函数的GCN改造。
利用Lovasz embedding+GCN，得到的LCN在多个专有数据集上得到更好的效果。
与该工作相似的早期工作在Spectral norm regularization of orthonormal representations for graph transduction. 2015，它也是利用Lovasz embedding，但没有结合GCN。
所以本文也没有太多创新，好在实验够多，同时论证了为什么Lovasz embedding要比纯GCN更好。
【不用细看】



55.Provably Powerful Graph Networks. NeurIPS 2019. 
【keywords】
图同构、WL、isomorphism test
【abstract】
这是No.31数学特别多的论文作者的后续工作。
【TODO】




56.Graph Agreement Models for Semi-Supervised Learning. NeurIPS 2019. 
【keywords】
联合学习、一致性、标签一致性、同配异配
【blog】
https://zhuanlan.zhihu.com/p/577789621
纯硬翻，不如看英文
【abstract】
普通GNN都是基于同配假设，但总是会有不同配的局部。GCN在传播时会把所有节点的嵌入都抹平到低频，更加导致同配异配分不开。我们急需另一个方法可以对同配/异配连接的边做预测，拉近同配、拉远异配。
因此有了本模型GAM，它搭配GNN一起使用。
【GAM】
如果按同配异配来分，所有已知的相连节点的边都有同配0异配1的表示，我们将通过两个节点的嵌入预测这条边。
看Figure2就懂了。
再利用另一个模型NGM的loss做改进，原版loss想保证节点预测准确的同时，保证有标签-有标签、有标签-无标签、无标签-无标签，3者的嵌入表示距离都较小，并用λ调整权重，可以让有标签-有标签的λ大一点，其他的就没那么重要，设置λ小一点。
GAM对上述loss的改进在式1.其中，当然要求标签预测正确。然后，对L-L，要求一个节点的嵌入与另一个节点的标签距离接近，这是一个小改动；对L-U，要求U的嵌入与L的标签距离接近，这也make sense， 已标注节点周围的点必须得与它标签相近。对U-U，就只要求预测之间距离接近了。
除此外，loss中本来的w是边权重，现在改了，用g(xi,xj,wij)，g代表一致性计算函数，可以是任何神经网络，要接收两个节点的表示and本来的权重以计算新的权重作为一致性度量，就像figure2描述的一样。
作者对g的设计被BERT刺激到了，他认为必须设计一个表达能力强的，所以仔细看figure2，它分为3部分，编码、聚合、预测，你可以随意设计其中的模块。
【训练】
g是一个GAM模型，它与计算距离的d相乘，而d(y,f)的f也是一个GCN模型，这俩不能一起训练。所以设计如下训练过程。
1.先训练GAM，用有限的标注数据，得到g0。
2.固定g0，训练f，得到f0.
3.用f0预测所有未标注数据，采用那些前M个概率高的预测当作真实标签，加入标注组。可以用2范数排序。
4.再固定f0，用已标注节点训练g0->g1.
【评价】
又一个联合学习的雪球系统模型，稳定性有限。


57.Graph-Based Semi-Supervised Learning with Non-ignorable Non-response. NeurIPS 2019. 
【TODO】
【keywords】
nonignorable nonresponse、不可忽略缺失
【abstract】
数学要求高，只做简单记录。
作者认为对图标签的假设有误，假设节点标签是独立遵循某个分布是不对的。图一定是有损的，即nonignorable nonresponse。
一般想到的是贝叶斯那一套建模，但作者可能是数学系的，编了一个看不懂的东西出来。
总之别看，我搞不懂。

58.A Flexible Generative Framework for Graph-based Semi-supervised Learning. NeurIPS 2019. 
【keywords】
【abstract】
该模型尝试统一node edge graph三个层面的信息，并采用概率模型建模。
【3 Approach】
【3.1 问题定义】
是一些基本符号的定义，还定义了loss function。其中R是图正则化函数。
【3.2 灵活的基于图的半监督生成框架】
生成过程。将问题定义为 p(X,Y,G)=p(G|X,Y)p(Y|X)p(X)，即认为一个图的获得要经过：先从分布中获得若干点X，X经条件概率获得Y，XY共同条件概率获得G的拓扑。
具体的p的定义，作者想要做“flexible”一些，意思是这些概率p的参数θ要可微，即使是在PMFs上(概率质量函数，即离散分布的概率密度函数)。
作者不寄希望于通过对Y求积分计算pθ(G|X)，计算量太大。作者直接将X视为离散分布，p(X)不存在，直接从X计算。于是目标变成了pθ(G,Y|X)，这就是前面直接对Y求积分的原因。
模型推断。我们的目标是获得Ymiss，所以定义后验分布pθ(Ymiss|X,Yobs,G)。然而我们认为pθ太复杂，没法积分，甚至没法采样，所以希望用一个简单分布qΦ来近似pθ。这是MCMC里常用的手法。注意，我们并不是没法求pθ的值，pθ在后面被建模成SBM，在已知θ的情况下，我们完全可以获得Ymiss属于各个类别的概率。
模型学习。我们将p(Yobs,G|X)看作是对p(Ymiss,Yobs,G|X)的Ymiss~qΦ积分得来，模型目的是maxp(Yobs,G|X)，在引入qΦ后得到一个ELBO，最大化ELBO就是目标。(这里要看论文公式)
【3.3 实例】
对p(G|X,Y)，我们可以用latent space model LSM建模，也可以用我熟悉的stochastic block model SBM建模。
对qΦ(Ymiss|X,Yobs,G)，得先有一个强力的预测Ymiss的模型，就采用GCN或GAT。作者说，这样用神经网络的计算，其实是利用平均场理论的近似，该理论也被用于变分推断里减少计算压力。
【3.4 训练】
有监督loss。先用GCN做有监督训练，之后将GCN的输出作为任意Ymiss的qθ。
最终loss是一大串，除了ELBO和GCN的loss，还有3个新增的正则化项，分别对应SBM对G的loss，KL散度保证pθ和qΦ相似，SBM对Yobs的loss。
【评价】
这类模型真该好好熟悉熟悉，现在有点懂了。


59.Semi-Implicit Graph Variational Auto-Encoders. NeurIPS 2019. 
【keywords】
VGAE、SIG-VAE、GAE、SIVI
【abstract】
semi-implicit graph variational auto-encoder半隐式图变分自编码器是本论文提出的模型，目的是扩展VGAE对图数据的灵活性。（所以先去看VGAE）
论文还提到了GAE，就是VGAE去掉分布，纯粹的映射encoder-decoder。
【SIVI】
semi-implicit variational inference半隐式变分推断。
这是比变分推断表达能力更高的模型，在多模态和偏态分布skewed distributions（区别于正态分布）。
正常的VI是由μ和σ确定分布，所以μ和σ是确定的，但SIVI认为μ和σ还服从一个分布。以此基础上得到的μσ的分布有更强的表达性。
且通过对中间变量的积分，可以直接得到隐式参数对结果分布的控制hΦ(Z)。
SIVI用近似推断方法，用q近似h，引出ELBO的优化，进而求解。
【Normalizing flow NF】
这其实就是一种流模型，认为隐变量Z与X一一对应，且对于Z的分布，有唯一X分布对应。
与VAE不同的点在于，VAE的隐变量Z是X反推出来的，是一个近似推断，缺少FLOW的一一对应。所以生成的图总是糊糊的。
【SIVI-VGAE】
适合直接看公式，可以看到encoder部分，用GNN获得节点表示hu，再通过C次标准分布qt的噪声扰动，得到最终认为的对分布的扰动lc。在此基础上，生成μ和Σ。
【NF-VGAE】
用NF强化VGAE。
直接看公式，是对decoder的改进，不希望通过抽样生成原图，而是直接将分布映射为图的分布，再去计算期望最大。
【SIG-VAE】
该方法是VGAE结合SIVI和normalizing flow的新技术，表达能力更强，更具解释性。其实就是把上面对encoder和decoder的改进拼一起。
SIVI：semi-implicit variational inference
【评价】
别细看，知道有这个东西就行。

60.Hyperbolic Graph Neural Networks. NeurIPS 2019.
【keywords】
黎曼流形、几何表示学习、双曲图神经网络
【评论】
跑，快跑！


61.Hyperbolic Graph Convolutional Neural Networks. NeurIPS 2019.
【keywords】
黎曼流形、几何表示学习、双曲图神经网络
【评论】
同上，跑，快跑！

62.Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels. NeurIPS 2019.
【keywords】
量子线性代数、正切核
【评价】
不是我该学的。

63.SNEQ: Semi-supervised Attributed Network Embedding with Attention-based Quantisation. AAAI 2020. 
【keywords】
SNEQ、框架、product quantisation、codebook、
【abstract】
SNEQ更像是一个工程模型，没有什么理论突破，而是为离散->连续空间的嵌入设计了很多约束。
【SNEQ】
【2.1 训练集设置＆自适应嵌入loss】
在图中抽取三元组(a,b,c)，a是中心节点，b要与a相近，c要与a相远。所有图上的度量都利用最短路径。
以节点特征xa，xb，xc经过普通非线性函数式1，可以堆叠多层，总之得到一个embedding，认为这是从节点特征->连续空间嵌入的一一映射。
该embedding会被式2的adaptive margin loss约束。
【2.2 半监督语义嵌入】
利用2.1得到的embedding再做文章，要求不同label的节点有一个固定距离Mc，且要体现在连续空间里，即D(i,j)=Mc s.t. i≠j，loss在式3。
可惜论文作者实验得知，如果对太多点有这样的约束，会影响图结构生成，或许不会回归。【idea】能不能给个范围，而不是一个固定值Mc？
【2.3 self-attention的深度量化】
product quantisation是一个快速索引算法，给一个向量，它可以索引到与其表示相近的向量。【TODO】PQ看懂了，但是它怎么用的没看懂。
【评价】
扫一眼图就行。


64.Going Deep: Graph Convolutional Ladder-Shape Networks. AAAI 2020.
【keywords】
图分类、节点分类、对称模型、GCLN、deep、
【abstract】
它能看见一点ASAP的影子，模型看图就能看懂，注意它只说了收缩是神经元减一半，说明该模型只是收缩了节点表示的维度。且利用jump做扩张过程的补充。
每一层都是GCN。
【GCLN+Diffpool】
这是可以结合的。节点维度从小到大，而节点数量却一直在被diffpool聚合。那么jump连接就要改改了，但论文里没说。
这样结合后就可以处理图分类问题了。
【评价】
真没什么好说的。8页小论文，有价值的只有这个结构。

65.Co-GCN for Multi-View Semi-Supervised Learning. AAAI 2020. 
【缺少资源】AAAI没了，不会被下了吧？

66.Graph Representation Learning via Ladder Gamma Variational Autoencoders. AAAI 2020. 
【缺少资源】同找不到。

67.GSSNN: Graph Smoothing Splines Neural Networks. AAAI 2020. 
【keywords】
样条函数、平滑样条、光滑样条、smoothing spline、
【abstract】
平滑样条早已被证明在分类问题上对拟合误差提供了一个光滑的作用，以防止过拟合。
【评价】
该方法是统计机器学习中学到的东西，作者将其结合到GCN的loss上。无需多说，没有再能创新的了。

68.Effective Decoding in Graph Auto-Encoder using Triadic Closure. AAAI 2020. 
【keywords】
VGAE、VAE、三元闭包、triadic closure、三元组闭包、triad decoder、三元组解码器、
【abstract】
回忆一下SIG-VAE，是用SIVI两层隐变量(优化encoder)+NF流模型直接映射分布(优化decoder)。
常规来说，encoder都是基于GCN，decoder根据encoder的结果重构图的结构，但只通过考虑两个点之间是否有边，所以是一个链接预测任务，所以这个方法忽视了边与边之间的交互性。(有些牵强的说法)
另一方面，做整个图的边缘预测，太费算力了。
本论文利用triadic closure性质，该性质是一个极端但却在现实数据中很实用的性质，说(A,B,C)三个点，若AB、AC有强联系，则BC必有强联系。
本论文提出一个triad decoder三元组解码器，直接预测三个点之间有没有三条边。
本论文将这个triad decoder用在VGAEs上。实验表明，在链接预测、节点聚类、图生成上，这个三元组闭包性质很好用。
【Triad Decoder】
直接看图就能看懂。
先用1*1卷积核，看公式其实是对三者占比训练的一个加和。再经过full connection+ReLU，得到1*3的向量，再加上旁路的边预测，最后获得最终的三元组边预测。
可以发现，若F输出的都是0，则退化为两点预测。
【TVGA TGA】
triad decoder可以结合任意VGAEs，这里对VGAE和GAE做改造，得到TVGA和TGA。
【loss】
是简单的正负例信息熵公式。若有采样，则加上KL散度。
【三元组采样】
如果完全随机采样，由于大部分数据太稀疏，所以采样不到多少真正符合三元闭包性质的三元组。
以下为设计的一个采样方法：纯随机一个节点i；从N(i)中用概率p抽样下一个节点j，1-p概率抽N(i)之外的节点j；k点从j出发，与上面一样的采样方法。
【图生成任务】
链接预测、节点聚类任务就不说了。对于图生成任务，由于TGA基于GAE得到，所以不具备生成功能。所以只考虑TVGA。
假设TVGA已经训练完毕。假设要生成一个N'个节点的图，则在分布中岁己采样N'个点，然后随机采样K个三元组，用triad decoder生成K个三元组的连边权重，将其放在邻接矩阵Ahat中。注意，Ahat应该是对称的，所以对有对称点的位置，作者将Aij一半一半分给Aij和Aji。
这里假设图必须是连通图。从NetGAN获得的启发，这样做：1.每个节点根据周围边权重作为概率采样一条边，加到图里；2.对同一节点继续按概率添加边，直到某一次随机到的边已添加。TVGA对此做一点小改动，不是逐点选边，而是整张图按照Ahat权重选边，直到重复。
【实验】
TVGA和TGA在经典三个数据集上都有SOTA，但random sampling效果不好。


69.An Attention-based Graph Neural Network for Heterogeneous Structural Learning. AAAI 2020. 
【keywords】
异质图、HIN、HAN、metapath2vec、HetSANN、
【blog】
https://blog.csdn.net/weixin_45848585/article/details/122901123
【abstract】
对于异质图，它其实是很强的异配图，所以不好直接用GCNs。基于meta-path的方法，例如metapath2vec，又需要领域专家设计meta-path，而且这很显然是信息有损的。
HetSANN的结构在figure2，数据流在figure3，看得出来核心结构叫做Type-aware attention layer。
【type-aware attention layer】
1. transformation operation：将不同空间的特征用不同的映射方法映射到同一个空间P中。映射是线性映射，每一个layer只有一层。从这里就有M个映射的说法了，而不是在multi-attention才有，且后面的M个multi-att就是对应这里的M个不同的映射。
2.aggregation of neighborhood：用multi-head attention。设置ATT公式为式2，attention scoreing function f可以任意设置，作者设置不同的attention函数形式一样，具体在式3，注意参数只在同类比较中共享。
softmax函数在式4，是不分类别的softmax。
3.映射有了，attention有了，然后就是信息融合(式5)。最后的M次信息会拼接(式6)，再加上residue mechanism(式6).
4.注意，1~3对每种节点都计算，所以存在一个有向图/无向图的问题。对于有向图，会认为其反向边存在，并且双向边的attention score一样，这在attention scoring function中已经定义好了。
5.我们必须给1中不同空间节点的映射加上约束，要保证一个类型pi的节点表示变到另一个空间pj，在pj中自身映射到pj，再变回到pi，等同于自己在自己空间的映射pi->pi。上述映射分别对应1中的Wii,Wij,Wji,Wjj。式12就是设计的约束。
【评价】
是看起来就不太稳定的框架，但却效果很好。


70.Fast and Deep Graph Neural Networks. AAAI 2020.
【keywords】
deep、fast、FDGNN、fast and deep GNN、
【abstract】
看figure1和黄标记就行。
有趣的是，FDGNN会初始化所有层的表示为zero values。这样就能看懂图的意思了。所有节点最初的迭代都是依靠节点的label。
输出y是所有节点加和再转换(式3)，所以这是一个图分类模型。
如果对式2做出稳定性分析，得到WI和WH的初始化设置，然后固定，则大大减少计算消耗。剩下需要计算的只有式3的Wφ和WY，而Wφ被设置为L2正则约束，所以唯一自由的参数只有WY。
【评价】
这是纯粹的图结构嵌入模型，不带节点feature，只有label。


71.Hypergraph Label Propagation Network. AAAI 2020. 
【缺少资源】

72.Learning Signed Network Embedding via Graph Attention. AAAI 2020. 
【缺少资源】

73.GraLSP: Graph Neural Networks with Local Structural Patterns. AAAI 2020.
【keywords】
节点分类、图结构、图拓扑、GraLSP、匿名游走、anonymous walk、
【abstract】
通过匿名游走辅助GNNs获得结构知识。
【GraLSP】
看figure2就可以了。直接基于random walk的路径做结构f:w->u的学习，太聪明了。
u参与到attention函数λ和放大函数q的计算。
【改进】既然要做结构学习，为什么没有用induced graph呢？然后就可以再借一个图结构学习模型了。
【loss】
对匿名游走做了约束，认为如果从一个节点出发产生两种匿名游走，但他们描述的本应是同一个邻域，所以它们应该表示相近。这是有条件的，要求匿名路径出现的频率要比图平均的频率要高。
这里的loss Lwalk还给出了负采样wn。
【评价】
真可以改改，但问题是用了Induced graph后怎么得到结构embedding u，GraLSP是拿anonymous walk糊弄过去了。而有后续论文论证random walk都是GDC和chebNet等的特例版本。



74.ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations. AAAI 2020. 
【keywords】
coarsening、图分类、收缩、聚类收缩、
【abstract ＆ conclusion】
ASAP和DiffPool等一样，是对图分类任务的模型。
ASAP adaptive structure aware pooling，是一个针对图结构的稀疏且可微分的池化方法。
ASAP层级化地聚类区域子图，以学习丰富的图结构信息表示。
还提出了一个self-att方法，Masters2Token，可以让ASAP更好地捕捉聚类节点里的成员信息。
还提出了一个LEConv模型，以评价上述节点聚类针对其本身和全局的重要性。
ASAP利用LEConv计算聚类分数，基于此再进行针对聚类的采样。
ASAP也能为聚类计算稀疏的边的权重，以在大图上捕捉边连接信息。
【正文】
https://www.cnblogs.com/BlairGrowing/p/16583892.html
【TopK pooling】
通过对特征X简单的线性转换，得到分数y。
再取top-k的y，用tanh放缩到-1~1。
同时节点特征X只取top-k，邻接矩阵A只取top-k。
一看就很粗糙，其任何优点都是因为它粗糙。
【SAGPool】
self-attention graph pooling.
除了评分用GNN，其他全和topKpooling一样。
【Token2Token】
这类self-attention选择中心和邻居两者的信息，经W变换后拼接再经v到单位值作为原始分数，在统计完所有邻居分树后，再由softmax缩放。
【source2token】
将节点的分数视为全局程度的分数，不由中心节点而改变，所以相较于t2t，去掉了中心节点的信息。
【ASAP】
应该记住table1对现有pooling方法的总结。
看figure1讲解ASAP过程。
ASAP以图的每个节点为中心，以h-hop内节点为簇，用Master2Token方法计算簇内每个节点的attention，记录在S∈R^(簇数*点数)，再通过LEConv算法计算簇的中心节点得分，再利用这个对每个节点的打分看每个簇都有多少分，拿到top-k的簇，再用某些方法将这些簇捏成收缩图。
【Master2Token】
这是针对ASAP得到的聚类做的attention方法。
给定的数据是以vi为中心的一个h-hop以内所有节点的点簇。M2T抽取点簇每个维度最大的值拼成簇的master节点特征，并基于此，对每个簇节点计算attention。注，vi也在簇里。
最后按照attention得到簇的表示xic记录在节点vi中。
【LEConv】
在式8，可以看到是按照M2T得到的簇嵌入xic计算分数，只计算相邻的节点，可以看到，相邻簇嵌入的差距越大，得分Φi越大。
为了让它能够被训练，将Φi作为簇特征Xc的放大倍数，然后通过函数TopK(TopK pooling的方法)计算top-kN的簇。
接下来就是按照top-kN的簇的编号提取出S和X，其余的都不要了，因为某种程度上，不要的部分已经融合进了簇表示中，特别是k∈(0,1]比较大时，很难丢下哪个节点的信息。
【得到新的A和X】
根据LEConv得到的S，很容易得到新的A和X，X不必说已经得到了，A的公式就在10。
论文设置A=A+I，目的不明，没说，在DiffPool里就是A，此时A=STAS的aii代表原本簇内的边数*2，aij代表簇之间的边数。若设A=A+I，会加强簇内的邻接程度，在下一个ASAP的GCN中让信息更难以流动。
【评价】
很复杂的系统，稳定性不高，想做成功一定要很多trick。


75.Multi‐Stage Self­Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes. AAAI 2020. 
【keywords】
自监督、稀疏标签、Multi-Stage self-supervised training algorithm、M3S、
【blog】
https://www.cnblogs.com/owoRuch/p/15685682.html
【abstract】
用于处理图节点标签太少的自监督学习方法。
之前看的多是联合学习，或基于某种方法不断添加置信度高的节点预测，或让embedding更加make sense。
【M3S】
figure1给出了算法流程图。
这其实是一个简单的筛选未标注节点置信度的方法。
1.先用已标注节点训练一个GCNs
2.用deepcluster例如k-means聚类未标记节点。
3.为聚类的节点设计伪标签，标签设计为：聚类质心离哪个已标注节点的类别质心近，就选哪个。
4.若伪标签与预测标签一致，放入待选区。再从待选区的每一类找k个置信度最高的预测。
【评价】
问题在于怎么设置k-means的k。这个机制检测伪标签和预测标签的一致性，双重保险检测置信度，但这两个的底层机制一样，而且万一一个类别的已标注数据有多种表述不同的子类型，那k-means+对齐机制就废了。
【改进】【想法】
希望设计一个多段检测节点预测置信度的算法，例如多层GCN的每一层都预测一下，只加入那些随着GCN更加deep而预测置信度越来越高的节点。
这个想法很严苛，比起直接添加置信度高的节点更加make sense，能加入的节点也更少。原本直接添加置信度高的节点要满足两个条件：已标注节点的预测置信度高；未标注节点的特征和邻域特征与已标注高置信度节点相似。而已标注节点的置信度高->loss低，这在初始标注节点少的时候很常见，让loss低不是难事。所以这里很可能把最终置信度嵌入低的节点当作高的，从而误导表示有差别但真实标签不同的节点被赋予错误标签。
但我提出的方法不一样，如果节点随着接收的信息摄入越来越高，置信度也越来越高，说明它在随着模型的分辨能力一起提升。我认为这种更有说服力。




78.Adaptive Structural Fingerprints for Graph Attention Networks. ICLR 2020.
【keywords】
GAT、ADSF、指纹感受野、
【blog】
https://zhuanlan.zhihu.com/p/382164189
【abstract】
贡献点在于，不局限于一阶邻域感受野，而是对每个节点的动态感受野，称为指纹感受野。
论文实验得知，当GAT考虑k-hop邻域聚合时，效果会崩。作者认为是over-smoothing导致的。但k=3也不至于在几层情况下造成极大的over-smoothing，所以大概是作者根本就是把k-hop的邻居都拉成1-hop做了attention。
也从另一个方面说明想要用k-hop-ATT绕过同配/异配图结构是不可能的。
【改进】如果我先把2-hop的按照att score聚合到1-hop，再把1-hop按照att score聚合到中心节点，能否变好？
【ADFS:adaptive structural fingerprint】
1.中心节点i，取k-hop邻域节点为子图。
2.每个子图节点对i都有非负权重wi∈R^ni，ni是i的k-hop邻居数量，初始化可以为：高斯核函数wi(j)，它有一个可学习参数h；或是本文使用的办法，让距离k与权重u成负相关，即距离k越小，权重u越大。
3.在子图做random walk with restart，ei初始化为子图的one-hot向量。更新wi，无限迭代后其解析解在式1。所以这一步其实是在做PPNP or APPNP。并认为最终的wi是周围节点对中心的attention score，这其实很合理，只不过PPNP认为是中心节点往其他节点传信息，而本文是其他节点给中心节点信息。
【评价】
据说代码里k-hop实际用的是k-BFS，其实没关系。
改进在于GAT中attention部分用APPNP的传播稳态代替attention。

83.Geom-GCN: Geometric Graph Convolutional Networks. ICLR 2020. 
【keywords】
异配图、geom-GCN、框架、非对称
【blog】
https://blog.csdn.net/confusingbird/article/details/107104497
【abstract】
作者研究了现阶段GCN等MPNN结构的模型的缺点，发现它们通常陷入两个问题：邻域信息聚合只收集信息，无法在多步后获得有效的邻域拓扑信息，毕竟当初的设想是邻域只需要传信息而已，如果MPNN做深了，又陷入over-smoothing问题；图通常不总是完全同配的，节点总要与标签不同的节点连接，此时MPNN没有能力区分这种局部异配，特别是在异配图里，MPNN聚合的都是与自己无关的信息，想要获得远处与自己标签相同的节点信息不可能。
基于此，作者设计了一套框架，在Figure1。
对于原始离散拓扑图A1，用某种方法映射到潜在的连续空间A2，这样节点之间就有了距离。取一个中心节点的局部放到B1，我们认为只有局部的节点与中心节点之间有强关联，且必须要求在A1中中心节点的邻居必须在中心节点的一定范围内。对B1做一些改动到B2，包括：认为半径ρ范围内的节点与中心节点强相关；认为所有拓扑邻居都要在B2里；对B2分割成3*3，每一块都对应一个关系τ。接下来进入C图的计算，C就是B2加了一些信息聚合的辅助线，它包含双层聚合：虚线表示的低层聚合和实线表示的高层聚合，蓝绿分别表示拓扑层面的邻居聚合和潜在空间层面的邻居聚合(以ρ为半径的邻居)，计算都在式1。
框架搭好了，剩下的就是将零件填进去。
【历史工作总结】
1.为了克服MPNNs丢失邻居拓扑信息的问题，GEOM-GCN用潜在空间给节点距离建模，用bi-level聚合。已存在的工作尝试学习隐式的structure-like信息，例如非对称性的GAT，LGCL、gate GG-NN。还有我没看的协方差组合网络CCN利用的是协方差架构学习结构层面的表示。而GEOM-GCN的特点是描述特别显式、有解释性。
特别是作者发现该模型因为提供了一个潜在空间的拓扑，所以也可以成为上述其他模型的上游。
2.MPNNs第二个问题是难捕捉远距离节点信息，这个时候只有JK-Net还算考虑了长距离，但也只是任由MLP发挥的res-net操作。
【GEOM-GCN】
【A1->A2 离散拓扑空间到连续潜在空间：】
 给出了3个办法：Isomap(机器学习降维算法，等距特征映射)、Poincare embedding(2017年的嵌入方法，长相就和A2一样，是个球)、struc2vec(经典图结构学习算法2017)。
【B1->B2，位置划分】
如果是2-dimension，位置设定不像图中3*3，而是table1的2*2。高维自己划分，也可参考前人的工作。
【C】
这里就是实例化e、m、h的公式。
e聚合的是low-level，用GCN的框架，其中提到的kronecker delta function就是0-1函数，用节点uv的度做u特征的放缩。
m聚合的是high-level，但是是文字描述的，看起来就是把所有e都concatenation起来。但这样描述太模糊，具体来说应该是按照e所在位置concatenation+MLP得到embedding，再结合蓝、绿双方的embedding做mean aggregation，这样解释才符合论文的描述。
h是简单的MLP+ReLU。
【评价】
很适合仔细读，里面对近期针对结构信息的工作总结写的很多，能改动的地方也有很多，作为框架类模型很适合硕士生水论文。


89.GraphSAINT: Graph Sampling Based Inductive Learning Method. ICLR 2020. 
该模型突出一个子图采样，类似于clusterGCN那个方向。大框架都是先用抽样器从G中抽取子图，再在子图上构建GCN。所以重点在于如何抽样子图。
GraphSAINT的效果要碾压clusterGCN。
【4个抽样器】
论文共给出4个抽样器，分别基于点、基于边、基于random walk、基于多维random walk。
其中，基于边的是下功夫最多的。
基于点：节点抽样概率看节点的度，度越大，概率越大。从全图抽样节点，然后导出其诱导子图induced subgraph。
基于边：边e的概率依据节点的度，边连接uv点，deg(u)是度，p(e)正比于1/deg(u)+1/deg(v)。可知，度越大，边被抽的概率越小。同样依此得到诱导子图induced subgraph。
基于随机游走：从中心节点随机向外走，走的点就是要聚合的点。同样依此得到诱导子图induced subgraph。
基于多维随机游走：略。
具体做法全写在algorithm 2中。
【注，对于基于边的采样方法】
基于边的抽样公式经过论文证明知是方差最小的抽样方法。 且给出了基于边的前向传播时保证无偏性的参数α的公式，它根据抽样概率而来；给出了让loss无偏的参数λ的公式。都用绿色标记出来了。
【其余方法】
论文也给出了大概的α和λ设置方法，也用绿色标出了。
【总结】
GraphSAINT它是一个graph级别的采样，而不是layer级别的。它通过无偏化loss和方差最小化抽样方法的处理，使其成为比clusterGCN更优秀的采样方法。

1.ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK ||  ICLR2021 || 310+
【keywords】
GPR-GNN、GPR、random walk、
【blog】
https://blog.csdn.net/qq_40926715/article/details/126829553
【abstract】
GPR，即Generalized PageRank，其数学等价就是给多个步长的PageRank，并对其附上参数，训练时自适应地训练这些参数。
相比于APPNP，其实就是PPR和GPR的区别。
【数据集/任务】
在cSBM构建的不同异质度的图上测试，GPRGNN最好。
在cora、citeseer、pubmed、computers、photo、chameleon、squirrel、actor、texas、cornell上，效果最好。
【评价】
由于在将GPR操作等效为monomial basis(即模型后半段操作)，等效后其实是一个滤波器样式的工作原理，所以该模型被人诟病为“空域的滤波”，因而“缺乏可解释性”。

2.Beyond Low-frequency Information in Graph Convolutional Networks || AAAI2021 || 260+
【keywords】
FAGCN、异质图、
【blog】
https://zhuanlan.zhihu.com/p/487837100
【贡献】
1.证明了低频高频都有用。
2.提出了FAGCN。
3.理论层面证明了FAGCN的优越性。
【abstract】
很简单的想法，分别提取低频和高频信息，然后用门控方式自动调整二者的比例。
低频or高频，是靠式2的两个公式，FL其实就是GCN多一个 ε ，FH将1阶cheb polynorm的θ1和θ0为正比，放在传播里就是获得自己与邻居的差值。
之前看论文的时候脑子糊涂了，Remark1中说2-order GCN对λ的改变是(1-λ)^2，其实这是扯淡，结合cheb polynorm看，其实应该是(2-λ)^2，否则(1-λ)^2就变成带通滤波器了。
遗憾的是，虽然FAGCN用αij控制了低通高通信号的比例，但是还是把两部分数据加到了一起。【改进】而且只用相邻两个节点就判定权重，不感觉很草率吗？
【数据集/任务】
节点分类任务,cora、citeseer、pubmed、chameleon、squirrel、actor。
FAGCN同质图表现很好，异质图也比已有模型略高，而且由于针对异质图，所以多层堆叠效果也不减。
【评价】
提高太少了吧，不超过1%。


3.Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs ||  NeurIPS2020 || 442 
【keywords】
H2GCN、异质图、
【blog】
【abstract】
看20页的figure5.它从空域分割自身和邻居的信息，不敢多堆叠，只用2层，此时其实已经不是严格的自身和邻居信息划分了。最后将所有层的输出融一起去预测。
【数据集/任务】
【评价】

4.Block Modeling-Guided Graph Convolutional Neural Networks || AAAI2022 || 19 
【keywords】
BM-GCN、
【abstract】
这篇论文在introduction部分细数了H2GCN、MixHop、GGCN、GPR-GNN，说MixHop和GPR-GNN这种扩展相同操作的方法不令人满意，GGCN为每个邻居设一学习的权重太昂贵。
异质性算法问题有俩：破坏拓扑，例如Geom-GCN；聚合机制太局限，例如Mixhop、GPR-GNN、GGCN。
引出本文的方法：同类的邻居用一个方法聚合，不同类邻居用另一方法聚合。即设置俩聚合函数。
Block matrix的矩阵形式好好学习一下。
Eq7:AY计算聚合信息中的标签分布，YTAY的元素代表i类节点和j类节点连接的概率。YTA其实就是AY，统计聚合标签信息，YTAE的每一行元素都一样，i行的所有元素都代表一次聚合中所有节点总共聚合到的i类标签数量。所以Eq7的H是在统计后做归一化。
Eq8：H的hij代表类别i连接类别j的概率。Q=HHT的qij代表类别i和类别j连接其他类的分布的相似度。
Eq12：QBT的i行j列代表第j个节点连接到第i类节点的概率，Ω=BQBT的ωij代表节点i和节点j在整体视角上连接的概率。
BM-GCN的思路是，先统计总共的类-类连接概率，然后看具体的节点-节点连接概率。是以边为单位的数据增强，如果节点i和j的主导类别是a和b，且a和b类的节点在宏观统计中总是连接，则加强这条ij边。
BM-GCN认为，两个类的连接模式若相似，则互相之间的信息传递也该多。连接模式的相似程度矩阵由Q表示，任意两个节点ij的MLP软标签预测BiBj可以作为联合概率分布，结合Q，以计算ij之间应该有的信息交互量，这是一个计算信息交互量期望的公式11，其矩阵形式为式12，以12重新调整带自环的邻接矩阵A+βI，即式13，再做以1-hop为调整的归一化式14，认为一个节点的一份信息就是靠着这个矩阵的传播方法传播的。式15表示图卷积靠每层自身的残差+传播。
【数据集/任务】
节点分类，texas、squirrel、chameleon、cora、citeseer、pubmed.
【评价】
BM-GCN的假设是，相同传播方式的类节点之间应该有更多的信息交流，此举在原本邻接矩阵A的基础上，加强了已有的同质边的权重，(看似)加强了异质但类传播模式相似的边的权重，后者是BM-GCN比GAT多的优点，借此相似的两类可以在传播时获得彼此的信息，结合自身，就可发现自己是两类中的哪一个。
所以所谓的BM-GCN，其实是发现了一种数据增强的方式，且作为基于MLP的方法，把MLP+GCN的模型融合实质隐藏了起来。

5.Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns || SIGKDD2021 || 48 
【keywords】
WRGAT、WRGNN、
【abstract】
关键在于如何理解式3和4.
M是一个统计图中节点各类之间连接边数的矩阵，Mgh代表图中类g节点连接到类h节点的边数再除以总边数2m(无向边*2)。
在重写Mgh公式3后，发现可以这样解释Eq3：针对节点i走向节点j，其基底概率是Aij/di，即i对邻居有均匀概率，然后认为πi是真正走下这一步的概率，所以这里计算的是“从i走到j的期望”。也可以解释为，本来各个边在随机游走中走到的概率是不同的，度越大走到的概率越小，但是在πi的调整下，所有边走过的期望都相同了。

你可以启发式地自己设定α，也可以用式6的期望。
figure3展示了不同模型在不同程度的局部同质性下的预测效果，发现越异质(<0)效果越差。
【模型】
作者希望生成一个新图，新图的分类性更高。新图应该满足:1）还是原先的节点和特征。2）用无关模型的方式对原始拓扑进行编码。3）对原始图中的邻近信息编码。
因此，作者的模型分为两个部分，1）从原始图进化到计算图。2）根据计算图再做GNNs。
具体方法再algorithm1.再接传统GNNs等。
其最大的贡献就是用邻居节点的度的排序作为一种结构相似度的衡量手段。
【数据集/任务】
节点分类，chameleon、squirrel、actor、cornell、texas、wisconsin、cora、citeseer、pubmed。异质图效果好很多。
比mixhop、geom-gcn、h2gcn等都好。
【评价】
没法评价，还没看完。

6.Diverse message passing for attribute with heterophily || NeurIPS2021 || 27 
【keywords】
DMP、attribute-wise message passing、
【abstract】
该论文发现另一个规律，如果按照式3计算以属性为单位的同质性，会发现整体同质性高的图，平均属性同质性也高。同时又发现，如果逐个观察不同属性的同质性，它们又横跨同质到异质，所以只看整体同质性大就按同质来做，是亏待了同质性低的属性。
因此，根据属性为单位，来调整不同属性的传播方式，是自然而出的想法。
只要把式2中的c从标量值变成向量值，并与h点乘，就能以属性为单位调整传播方式。这就是attribute-wise message  passing，或element-wise message passing。
此时可学习参数变多了，over-fitting的毛病显现出来。作者设计了两个策略以减少可学习参数：1)让c被一个可学习函数(式5)约束。2）考虑让所有邻居都用一套聚合向量，这让element-wise 变成了 channel-wise。
如果结合1)和2）那么就是希望找一个函数，能生成一个对所有邻居通用的聚合向量cv，作者认为对hv的cv和对邻居的cv都应该被hv和avg(hu)决定，所以用两个式7分别定义两个cv。
经研究发现，式6的最终版本其实是式8这个1-order chebNet的进化版本，只需要求所有节点对自己和邻居的传播方式一致。
【数据集/任务】
节点分类，texas、wisconsin、actor、squirrel、chameleon、cornell、citeseer、pubmed、cora。
DMP有多个版本，DMP-1代表用了策略1），DMP-2就是策略2）。DMP-DEG是式8的DMP退化版本，其形式为1-order chebNet，比GCN好。DMP其实只是改良了消息传播阶段，所以可以应用到GNNs上。对于自身和邻居信息的组合方式，可以效仿H2GCN和GraphSAGE的拼接-Con，也可以效仿GCN和GAT的加和-Sum，两种方法的DMP都有实现，相比之下Con更优秀。
理论分析下，DMP1简化为DMP2，DMP2退化为DMP-DEG，所以性能逐步递减，实验结果也正如此。
【评价】
又把我的idea抢了，channel-wise MP也没了，人家的实验做的还好，分析的也好。

7.Graph Neural Networks with Heterophily || AAAI2021 || 156 
【keywords】
CPGNN、
【abstract】
这里提到一个观点：有一些类别的节点总是喜欢连到一起，例如诈骗分子总是和客户连到一起。
因此定义了兼容性矩阵H，其公式与BM-GCN的block matrix一样，所以它们其实都是在发现数据增强的规律。
然而CPGNN的发展方向不同。BM-GCN是认为应该为连接模式相同的邻居节点加强边的权重，以让模型使节点互保。
CPGNN是其他做法。
首先，如果图上节点标签齐全，我们当然能获得H，但是问题就在于它不全。所以像BM-GCN一样，CPGNN也用MLP或2阶chebNet得到所有节点的软标签。
接下来，借着GNNs的模式，希望靠之前发现的“有些类对总是相连”的发现来加强一下软标签，基本思路是：如果H发现类别ij总相连，而中心节点的软标签i的邻居正好有很多j，则应该加强i是i的确定性。
看公式6，确实是这么做的。先用A收集邻居B(k-1)的软标签，然后用H乘，以指导节点，告诉它你周围的j类多，你该加强i类。若中心节点是i，则加强，若不是i，则是削弱了该节点软标签的确信度。
【数据集/任务】
节点分类，texas、squirrel、chameleon、citeseer、pubmed、cora。第一阶段用cheby的总是更好，可惜不是一个统一的模型在所有数据集上表现良好，有一些甚至只是单科状元，其他的数据集弱到离谱。
【评价】
该方法太不稳定，太看数据集了。但却广泛地被用作各个论文的baseline。

8.Graph Pointer Neural Networks || AAAI2022 || 11 
【keywords】
GPNN、
【abstract】
论文希望用pointer neural network一样的指针网络，为中心节点指出图中与自己相似的节点。
步骤很简单，如下：
1.按照BFS对节点周围采样序列。
2.将序列的节点按照GCN嵌入。
3.嵌入后的序列送入encoder，输出e∈{e1,e2，...，eL}备用。
4.eL送入decoder，初始送入d0=[start]，输出d1，并根据公式12~13看输出哪个节点xc1，备用。把d1再送入下一个decoder，重复上述行为，直到输出[end]或达到m个节点。此时拥有确定的o={xc1,....,xcm}这些节点，拼成一个m*d的矩阵。
5.最后拿o做简单的卷积池化操作，论文给的是1D卷积、mean or max pooling做聚类，得到z。
6.预测的时候，把节点i最初的特征x、2中嵌入的x^，5的z，拼一起，写作式6：x_final=concat(xW，x^，z)，再去预测。并用交叉熵做loss。
【数据集/任务】
节点分类：chameleon、squirrel、actor、cornell、texas、wisconsin。方法效果超过H2GCN、Geom-GCN、MixHop、Node2Seq。
【评价】
竟然真的有用，感觉不是太泛用。

9.Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification? 
Revisiting Heterophily For Graph Neural Networks|| ICLR2022 reject NeurIPS2022 || 53
【keywords】
ACM、adaptive channel mixing、ACM-GNN、ACM-GCN、
【abstract】
与No.33一样，是No.33的加强版。
【理论分析及新的同质性度量，或叫做可分性度量】
这里才知道所有基于拉普拉斯矩阵L衍生出来的矩阵叫做affinity matrix亲和矩阵(转换矩阵)，其实就是在计算图边权重。
2.2总结了计算不同程度同质性的公式，包括节点级、边级、类级，还有最常见的类同质度量hk。
论文提出了一个新的度量相似度的方法：definition 1，aggregation similarity score
其以一阶信息聚合后的信息为基础，计算聚合后的节点信息相似度。再以式6计算此时同类节点与异类节点之间相似度之间谁大谁小，并统计符合同质性的节点个数。
该式子的目的是，看一看消息聚合之后，同类节点是否还能与同类节点保持高相似性，与异类节点保持低相似性。若该值高，则说明聚类后，节点特征仍然可分。
作者实验说明，该式子在大多数数据集上都获得0.5~1的得分，故希望将其拉到0~1，于是有了式7的modify版本。
3.2通过Hedge指标生成不同同质度的图，发现在异质到一定程度后，简单模型SGC-1和GCN反倒效果好起来了。因为此时周围的异质点信息足够多，已经足够模型通过异质信息的集合反过来代表节点类别了。
虽然此时在figure2中的abc图呈U字型，但处于左端的极端异质图在d中其实处在右端，也就是Hagg认为它们即使消息聚合后仍能让异类节点可区分。从这个角度看，Hagg更能反应一个图的真实复杂性，即只有半同半异才最差，极端的同和异都很好。
theorem 1列出了聚合后相似矩阵中同类点和异类点相似度期望之差，发现差别最小的时候是在h≈0.14的时候，h指Hedge的值。0.14正好符合figure2的abc最低点，即在Hagg度量下，同质点与异质点差别最小的时候，就是节点最不好区分的时候，正合其他同质度量的效果最低点。
所以我们基于频域的滤波目的要改改了，极度异质(高频)和同质(低频)的信息都该得到。
【ACM-GNN】
4.2~4.3作者要在空域做滤波，其中低通代表普通聚类，高通代表自身特征减去低通聚类特征(式12)。这本质上是两种消息聚合过程。
在P7的STEP1~3说明了如何操作。
1.分别用低通、高通、MLP做GNN。这里分为两种，区别在于先ReLU还是先传播。
2.用上述三个节点特征分别做attention score计算，并以attention score作为聚合三个通道信息的权重。T是温度，一开始T很高，导致大家除以T后的attention score都差不多，到后面随着T的减小，attention score会逐渐起作用。
3. 结合attention的α把三部分节点特征揉一起，通过ReLU。
4. 1~3是一层的操作，多层需重复操作。
我们命名使用step1中第一个方法的是ACM，第二个方法的是ACMII。
P9的6.2展示了GCN+和GCN++的来源，GCN+指融合了MLP(A)的X，GCN++指再加上GCNII形式的残差链接，具体见appendix B。
【Appendix B中对GCNII的讨论和GCN+、GCN++】
【B.1 GCNII和ACMII的冲突】
GCNII可以融入ACM，但无法融入ACMII。
很简单的原因，ACMII是A*ReLU(HW)，但是GCNII要求H先与A做自环权重的学习，但是ACMII中，A和H中间有ReLU挡着。所以ACMII和GCNII无法融合。
【B.2 GCN+和GCN++】
GCN+是看上了MLP(X)+MLP(A)这个在LINKX和GloGNN存在的trick，这个trick在chameleon和squirrel中有大作用。
[对于添加A的GCN+]P21在红字高亮与ACM-GCN的区别：
step1：除了低维、高维、自身这三维信息，为每一层增添ReLU(AW(l))，W(l)是按层的。
step2：本来是直接做attention score计算的，但是这里新增一次row-wise normalization。
step3：本来没有A的时候是option1，有了之后是option2，其实没有大变化，只是单纯地增加了attention A的内容。
[对于添加GCNII形式的GCN++]P21红色高亮是GCN+和GCN++的区别：
step1：再增加一个针对原始特征X的嵌入Hx。
step2：没有更新。
step3：无论是没有A的option1还是有A的option2，都做同样的在最终layer输出处加上step1获得的Hx。
为什么说添加GCNII机制只是简单地增加residual connection呢？因为GCNII是“A的自环权重调整”+“残差权重调整”这两种思想的结合。前者不能实现，那就实现后者。
【数据集/任务】
节点分类任务，cornell、wisconsin、texas、film、chameleon、squirrel、core、citeseer、pubmed。
与GAT、APPNP、GPRGNN、H2GCN、MixHop、FAGCN等相比，ACM-GNN及其变种有较多达到SOTA，但没有一个通用的。
【评价】
论文提出的新指标很有用。根本上还是没有分开低频、高频信息，却只在层内分别计算，attention过后又合到一处。
空域高频无非是看自己与邻居的差别，差别有用的基础是自身节点特征与邻居特征/拓扑稳定，否则差别也无法作为判断依据。
没想到当年对该模型如此看不上，但是却实打实的有效果。
该论文可以说是集大成者了，包括低频、高频、残差、A trick、attention融合。

10.Node Similarity Preserving Graph Convolutional Networks  || WSDM2021 web search and data mining || 130
【keywords】
SimP-GCN、
【abstract】
作者在3.1给出了传统GCN过平滑的新解释。作者发现，如果以式3的过平滑任务为目标，则其loss函数式4在原图特征f0处的导数恰好是GCN的迭代式。这说明，GCN迭代其实就是在原图基础上的过平滑优化，其目的根本上就是过平滑。
作者设计了3.2的试验，其中A是原图的邻接矩阵，代表结构，Af是knn下的邻接矩阵，代表特征，Ah是GCN学习后的knn邻接矩阵，不代表任何事。比对发现，GCN其实保留的是结构相似度，而不是特征相似度，无论图是同质还是异质。
根据此结论，那么就自然地得到了基于knn的保持特征相似度的能力。
【SimP-GCN】
模型结构很简单，全在P4的4.1.2~4。
1.根据原图得到knn图，k自取。
2.自适应结合原图和knn图的聚合信息。
3.然后再自适应学习自环权重。
4.2~3是结构信息，充当GCN原本的L，进行GCN计算。
5.2~4循环，直到最后。
6.loss是节点分类的loss。同时要求其中的第l层输出Hl满足另一个loss，要求此时的同类型节点对的差值映射fw要与原本的节点对相似度相同。这里用到了负采样。
【数据集/任务】
【评价】
2021年就开始用kNN加强A。这是它总被引用的原因吧。

11.Non-Local Graph Neural Networks  || IEEE2021 || 103 能被引用都是因为绪论
【keywords】
non-local GNN、NL-GNN、NL-GCN、
【abstract】
是一个小论文，只提出了模型。模型如下
1.local embedding。用MLP或GNN先聚合一次。
2.attention-guided sorting。用QKV的self-attention获得每个节点的attention score，并以此为所有节点排序。
3.以attention score为节点调整权重，然后用1D Conv，窗口大小2s+1聚合排序相邻的节点信息。
4.最终的预测靠聚合信息和1的聚合信息一起。
【数据集/任务】
节点分类任务，cora、citeseer、pubmed、ogbn-arxiv、snap-patents、chameleon、squirrel、actor、cornell、texas、wisconsin。
实验只与GEOM-GCN、H2GCN、FAGCN比较了，而且只在自己优势的时候展现出来。值得注意的是，初次聚合用MLP、GCN都比GAT好。
【评价】
是很简单的想法，但是强烈怀疑这样真的make sense吗？或许这样的本质其实是抵抗由过平滑带来的信息趋同。
而且为什么GAT那么差？

12.Powerful Graph Convolutioal Networks with Adaptive Propagation Mechanism for Homophily and Heterophily  || AAAI2022 || 30
【keywords】
HOG-GCN、
【abstract】
该论文瞄准传播方式，就像NL-GCN一个思路。
该模型分俩阶段：同质度矩阵估计，为每个点对之间估计一个传播权重；同质引导传播，即用前一个步骤的结果指导传播方式。
【homophily degree matrix etimation】
这一部分分两部分合成：相似度矩阵S；基于拓扑的标签传播矩阵T。
对于相似度矩阵，它基于节点特征相似度。首先经过多层MLP，然后直接计算任意两点的相似度，形成相似度矩阵S，公式1~4描述了过程。
对于标签传播矩阵T，我们认为标签沿着拓扑传播，就像GCN一样，但我们的传播方式有小差别：式5展现的是传统同质图的标签传播方法，而我们采用GPR-GNN一样的方式，用式6和权重参数矩阵T改变k-hop邻域的传播权重，这是邻域内逐个点对的调节，而且只与拓扑有关。最后我们希望在T调节下的标签传播可以保证已标注节点的标签正确。
然后就是S和T的动态调节，用式9合成为H。
【homophily-guided propagation】
这里仍然是两部分，一个是自身特征的简单变换，另一个是用H调节Ak的权重并归一化的复杂变换。两部分仍然是由可训练参数调节比重。
注：k=2效果最好。
【数据集/任务】
节点分类任务，texas、wisconsin、cornell、film、cora、citeseer、pubmed。
这七个数据集效果都很好，不是第一就是第二。对比模型有H2GCN、CPGNN-MLP、CPGNN-Cheby、GPR-GNN、AM-GCN、Geom-GCN。
【评价】
HOG-GCN的思路是，加强相似节点的连接性。作者想到了根据相似度调节1-order节点，和根据label传播得到的节点相似度。根据label传播，但却不像APPNP那样，而是GPR-GNN的按照不同层级邻居的权重划分。

13.Simple and Deep Graph Convolutional Networks ||  PMLR2020 || 866 
【keywords】
GCNII、
【abstract】
整个模型都在式5了，用了两个技术：初始残差；单位映射。
从异配角度看，初始残差保留了原始特征，单位映射保留了自身嵌入特征，其中邻居特征的影响力较小。
【数据集/任务】
GCNII作为GCN的扩展，融合了JKNet的思想，必然不会太差。实际在同配图表现良好，且因为其异配适应性而避免了over-smoothing，可以堆叠32层。
在异配图的表现也很好，chameleon数值上超过了H2GCN。在Cora、Citeseer、pumbed、chameleon、cornell、texas、wisconsin的表现，超过了APPNP、JKNet、Geom-GCN。
【评价】
无法评价。这条路做到头了。

14.Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks || IEEE2022 || 129 
【abstract】
论文的贡献：
1.理论：分析一层和多层GNN下节点动态变化的趋势。将节点建模为随机向量，从理论上分析为什么不同类的节点难以区分(即过平滑)，例如度的原因、异质性的原因。
2.见解：发现(1)“高异质率节点”和“低异质性节点且degree低与其邻居(即度的差异)”会触发过平滑问题；(2)允许邻居间发送signed message可以将异质性和过平滑两个问题解耦。
3.强力模型：提出GGCN，允许节点间的负交互，并通过某种手段可以补偿那些degree低的节点受到的影响。
【2】
符号定义，特殊的有：
vi的特征用fi表示，而不是xi。
hi是vi的同质率，是node-wise homophily。
【3】
本节分析过平滑和异质性，但不像以前一样从无限层GNN上分析，而是由浅入深层的动态分析。
（说实话，没啥分析的了，看过后面论文的已经知道过平滑是高同质性和均匀聚合的杰作了）
setup：实验做了不失一般性的简化，设计一个二分类图，第一类的节点v1∈V1特征服从均值μ方差Σ，第二类节点v2∈V2特征服从-μ和Σ。分析使用没有非线性层的GCN。
【3.1 异质率和过拟合】
本研究显示，(1)节点的同质率(2)节点degree相对于其邻居degree的多少，这两个因素控制了节点表示由浅入深的动态变换，具体来说，控制节点表示是否逐步靠近另一个类的初始平均特征表示。
Figure1显示：
· case1中，任意取一个节点i，节点i同质率low，自己的degree=3，邻居平均degree=2。其实不用管邻居degree怎么样，同质率low就决定了1次聚合后，i的表示难以区分。
· case2中，节点同质率稍高，会让可分性稍微下降。
· case3中，节点同质率极高，而且同类邻居们几乎无邻居，那么它们是一个抱团孤岛，会极大地提高其中的所有节点的可分性。
对浅层进行分析：初始阶段
Theorem1说明，低同质率，或高同质率但是相对于邻居的degree较低，此时vi的1次GNN传播的fi期望E(fi)会缩小，具体请看Eq1~2。且按照之前设计的节点方差Σ，一次传播后，这个方差会减小到不足原Σ的1/2。但是要注意，这个方差Σ缩小是仅针对vi的，虽然vi的同类节点的Σ也会同样缩小，但是它们的μ变化不定，总体依然可能有一个大方差。
对深层进行分析：发展阶段
上面说了，两种节点：低同质率，或高同质率但是相对于邻居的degree较低。它俩由于可分性降低，前者还会翻转表示到另一种，进而导致后者也出现状况，跟着翻转，场面很混乱。总之，这两种都可能反转表示。
假设l层后，表示缩小到γ倍，有反转的表示是-γ倍。设一个新概念：efffective homophily h_il，指第l层实时的vi的同质率。有下列结论。
Theorem2(发展阶段):在深层传播中，当h_il<0.5时，相对degree较高节点们会误分类。可按照Eq1，用h_il代替hi，计算第l+1层fi的期望E(f_il)，发现高degree更容易导致特征f不可分和反转。可见，这个结论在浅层深层都有效，只要当时的h<0.5。
其实这可以理解。高degree总是更好地表示出符合其homophily分布的邻居，所以h<0.5在vi眼里degree越高越清晰，而这稳定地让vi聚合到反向信息，导致其出现特征反转。一反转，低homophily变高，但degree还是高，会逐渐巩固其反转的特征，然后毫无意外地，同质导致不可分。
summary of theoretical results总结了上述两个theorem的结论。
【3.2 异质和过平滑上的有符号信息传递】
GAT把不相似的节点特征传送量减小，但是我说过，这本质上是先让小团体抱团，再以较慢速度逐步走向过平滑。
这一节中，作者证明带符号的消息传递很重要。
setup：消息传递中，同类节点传递正常信息，异类节点传递带负号信息，这是最好的情况，但是总有那些传错的。定义m_il是vi在l层接收到的错误信息率，即统计“明明是非同类，却发送正信息 ＆ 同类却发送负信息”的数量。定义e_il=E(m_il)，所以e_il是站在当下，预测未来的错误率期望。
Theorem3：在独立性假设下(即所有同类节点的特征独立同分布)，假如模型允许消息以+-号传递，则在初始阶段(浅层)不同类节点间的平均距离不受hi影响，即在+-号加持下，节点保持其可分性。然而会被大的错误率e_il使其不可分，可以从Eq4看出，e>0.5时，节点可能会反转；e<0.5且邻居平均degree<f(e)时，E(fi)必定缩小；其他情况会增强E(fi)。
【4 Model Design】
论文提出GGCN模型，G代表generalized。
在Eq12上，看起来就是正负都有的结合，然后加各种可训练权重，还加了个skip-gram(一个未加解释的trick)。
【评价】
理论分析更重要，模型可略。

15.Node2Seq: Towards Trainable Convolutions in Graph Neural Networks || IEEE2021 || 2
【keywords】
Node2Seq、
【abstract】
这个是看前面NL-GCN看到的，提供了排序、1D卷积的思路。
直接说模型：公式全在式3~9，1-order邻居。
1.先把中心节点及其邻居特征排起来，然后用W1转宽度。
2.这里的xi就是1计算后的中心节点的嵌入，所以这里是计算中心节点与其余节点的相似度分数si。
3.接下来排rank，只排邻居节点。这里的Ahat=A+I，所以中心节点i的邻居还包括自己。
4.重排1得到的嵌入。
5.1D卷积。但这里不知道是怎么个1D法，好像是竖着的。
6.用readout将不同节点以参数权重聚合到一起，得到out。
7.将1的xi经W2+out作为最终节点表示。
(唯一的问题就是1D卷积怎么卷的)
再说另一个模型：2-order邻居怎么做
1.还是W1获得嵌入。
2.还是带自己的att score计算。
3.假设1的1-hop邻居有3个，加自己4个。我们设超参数l，即要从1-hop邻居里找l个用于聚合。剩下的4-l个从2-hop邻居里聚合。这就是local selection和global selection。
4.根据attention score给4个点排序。注意，这个4一定是与中心节点的邻居数一致的。
5.从1中把4拿到的节点的特征排成矩阵。
6.接下来还是1D卷积、readout、跳跃连接。与上面的算法一样。
【数据集/任务】
节点分类，cora、citeseer、amason、chameleon、squirrel、actor、cornell、texas、wisconsin。
只与最基础的GCN、GAT、SGC相比，效果好。毕竟是个小模型，参数量也不大。
【评价】
这其实就是NL-GCN的青春版，唯二不同就是attention score的计算，Node2seq用简单的相似度和MLP，NL-GCN用QKV attention和GNN/MLP。

16.GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily || 3
【keywords】
GCN-SL、
【abstract】
3个贡献：
1)把谱聚类(SC)应用于捕获长期依赖关系，并提出高效谱聚类(ESC)算法。
2)GCN-SL可以学习一个可优化的邻接矩阵。还使用了特殊的数据预处理技术，以克服过拟合。 
3)GCN-SL从节点、边缘两方面处理异质图，并做了结合。
【3.1谱聚类SC和高效谱聚类ESC】
GCN-SL打算用谱聚类的方式基于节点相似度将节点分成几个簇，簇的数量与节点类型数相同。依此加强潜在类内节点的连接权重，将图强行变成同配图。这个思路与许多利用节点相似度矩阵的模型(BM-GCN、CPCNN)相同。
但问题是，谱聚类需要分解拉普拉斯矩阵，简化后需要分解的是G，对大图很不友好。作者提出一个高效谱聚类(ESC efficient-spectral-clustering)，将相似度矩阵S=XXT，X是节点特征矩阵，使得G=PPT，进而只需要求矩阵P的奇异值分解，其中分解后的V在计算PPT时消掉了，U是G的特征向量矩阵，Σ是特征值的开方矩阵。要想获得最小的迹，需要挑选Σ中最小的k个特征值，找到它们在U中对应的特征向量，这k个特征向量就是要找的H。为什么呢？因为UTLU=Σ，当我们找的H是最小特征值对应的特征矩阵时，HTLH得到的就是最小值。
!!! 一般谱聚类算法中，得到了k个特征向量U∈n*k，但U不是非0则1的矩阵，无法当作图分割矩阵。一般会把U当作n个节点的特征，基于此进行k-means聚类，得到的聚类结果为最终的H。
而GCN-SL目的不在此，得到k个特征向量U后，我们可以直接将其看作n个节点在k维空间中的嵌入，且该嵌入的聚类包含了一定的谱聚类规则下的相似度信息。
【3.1续 ESC-ANCH】
ESC还是不够高效，计算节点相似度矩阵S=XXT这里问题很大，节点特征维度d过高会不好计算。
针对这个问题，提出ESC-ANCH。首先，随机抽出m个点(称为锚点)构成M∈m*d，m<<n且m<d。然后计算R=XMT∈n*m，Rij代表节点i对锚点的相似度。再计算RRT=S，S就是之前的S=XXT，用RRT代替了。此时的理解很重要，要知道此时Sij表示的值大小代表着"节点i对m个锚点的相似度"与"节点j对m个锚点的相似度"的相似度，即如果i和j对m个锚点的相似度都一致，则Sij总是会很大，我们会倾向于认为ij相似度很大，即使此时不一定准确。
好消息是这个lame的方法缺少大大减少了计算量，只需要O(nmc).
【3.2 re-connected graph】
这里很奇怪。作者仍然希望用前人的老办法设计一个基于节点相似度的同质点增强矩阵，但在初始特征的基础上增加了映射到低维的函数。所以还是要计算所有节点的节点相似度，然而上一节却设计出了一个ESC-ANCH简化这一步骤，搞不懂他们在干嘛。
总之这里做了特殊处理，节点相似度范围在-1~1，并设计一个超参数>0，低于它的归0.
还提出2个问题，作者怕初始特征维度低，会导致相似度矩阵失效，且GCN会导致过拟合。作者把全图节点的某一维的值+0.5，就避免了上述问题？？？？？？？
总之，记结果为A*。
【3.3 GCN结构学习】
1. 结合初始特征X和谱聚类特征F(3.1中我记为U)，计算了两方面综合下的节点特征表示矩阵H，作者提供了式10和11两种方法。
2.H再结合3.2给的相似度矩阵A*，行正则化后，将其作为传播矩阵，用于H的传播，但只传播1次。
3.用邻接矩阵A传播H，传播K次，式13。
4.COMBINE以下4个值：H、H在A的K-1和K次输出、H在A*的1次输出。作者说它们各有优点，所以用concatenate。并用与这个拼接后的庞然大物一样尺寸的参数矩阵W计算点乘，以强调重要的维度。再根据它的输出做readout。
【数据集/任务】
节点分类，cora、citeseer、pubmed、squirrel、chameleon、cornell、texas、wisconsin.
对比MixHop、Geom-GCN、H2GCN，模型在以上8个数据集上有5个SOTA，不是SOTA的也不差。
【评价】
读着恶心，有太多没来由的trick，太多突然出现的名词。
什么是initialized graph？NLGNN又哪来的？
但怎么就效果那么好呢？

17.Universal Graph Convolutional Networks || NeurIPS2021 || 35 
【keywords】
U-GCN、UGCN、
【abstract】
作者要综合1-hop、2-hop和knn邻居三方面数据，且对于2-hop邻居太多的情况，作者设置只允许路径数>1的才算邻居。
knn要计算相似度，相似度函数用jaccard similarity、cosine或高斯核都行。这里用cosine。而且用Ball-tree算法简化knn搜索难度。
knn用2层GAT变换，1-hop和2-hop是2层GAT的两个输出。
现在，一个节点i有了3个表示，如何融合呢？用式7~10.
先各自通过非线性网络获得一个标量值，再将3个标量值正则化为占比，用占比融合三个表示。
【数据集/任务】
节点分类任务，cora、pubmed、citeseer、cornell、chameleon、squirrel、wisconsin、texas，8个数据集。
U-GCN超过了JK-NET，GraphSAGE，Geom-GCN，GCN-LAP。
【评价】
实验设计是为了解释性，单论模型其实很简单通用。

18.Tree Decomposed Graph Neural Network || CIKM21 || 34
【keywords】
TDGNN、
【abstract】
设计了实验fig1，说明了节点的不同层同质性有很大差别，基本规律是，同质图的近端节点总是有较高同质性，随着距离变远，越来越异质。异质图虽然近端节点就没有同类节点，但可惜，因为远端节点太多了，所以远端同质节点占比也没那么多，但至少也比近端节点多。
因此，启发作者希望将树形计算图按照不同层赋以不同的聚合方式，以此适应同质图和异质图。差不多就是将深层的树拆成多个只包含中心节点和某一层节点的树(fig2)。
式4和5给出了拆分树的形式化定义。式4还给出了trick，当一个节点同时在k层和k-1层时，只保存k-1层的连接。然而当节点同时在k层和k-2层时，却同时保存了，疑惑。
然后，作者又说，树分解后只保留了最浅路径，抛弃了多路径的信息，容易过拟合。所以在第k层的树分解中，要加入一些信息，具体来说就是加强那些在k~K层有其他路径的节点权重，体现在式8.
式9给了两种TDGNN，-s均匀加和层间表示，-w自适应。
loss是信息熵。
【数据集/任务】
半监督节点分类任务，标记节点很少，每一类只有20个。cora、citeseer、pubmed。超过APPNP、DAGNN、GCNII。-s的效果更好。
全监督节点分类任务，48%都被标记。cora、citeseer、pubmed、cornell、texas、wisconsin、actor。优于Geom-GCN、APPNP、DAGNN、GCNII。
【评价】
论文范本。方法不难，关键在于设计实验证明思路正确。

19.2-hop Neighbor Class Similarity (2NCS): A graph structural metric indicative of graph neural network performance || AAAI2023 || 2
【keywords】
2NCS、
【abstract】
通过评价2阶邻居与中心节点的类相似度，可解释小范围内GCN对高异质图仍然有用的原因。其实就是2阶GCN获得了同类节点特征。
小论文，只提出了2NCS指标。
【评价】
根本不用它提出。这个论文是错误的，最新研究指出，2-hop GCN对高异质性图有用的原因是其邻居分布特征反哺的。

20.A Graph Neural Network with Negative Message Passing for Graph Coloring || 2023 ||1
【keywords】
graph coloring、GNN-1N、
【abstract】
论文要解决的是图着色问题，要输出一个邻接节点染色不同，同时最小化使用的颜色数量k的图。或是它的另一种问题，规定颜色数量，最小化邻接节点颜色相同的染色方法。
以打出租车为例。图的节点是客户，染色是出租车。假设人少车多，则我们极端点，可以给每个点分一个染色，但出于其他考虑，我们还是希望少地分配这些出租车，但至少保证临近的客人不会分到同个车，否则他俩要同时抢一辆车。如果车少人多，那就顾不得这些了，只能让染色数量固定，然后最小化邻接同色的情况。
对于特定的问题：图着色问题，有特殊的解决办法。
论文基于GraphSAGE，但更新时完全令邻居以负值合并。


21.Beyond Real-world Benchmark Datasets: An Empirical Study of Node Classification with GNNs || NeurIPS2022 || 4
【keywords】
图生成、GenCAT、
【github】
https://github.com/seijimaekawa/empirical-study-of-GNNs
【abstract】
现实世界图还是太少太杂。很难衡量模型效果。
提供一种细粒度的图生成器，用于测试模型。
有一小段异配图综述，提到了FSGNN和SOTA的LINKX，但后者缺乏对同质图的效果。
【数据集总结】
总结了Benchmark datasets。
同质图有cora pubmed citreseer是常用的引文网络，有来自亚马逊购物网站的computer photo数据集，有Reddit的reddit数据集(节点是帖子，边代表两个帖子被同一个人评论)，ogbn-arxiv和ogbn-papers是两个超大型同质图，亚马逊也放出一个ogbn-products也是超大型同质图。
异质图有texas wisconsin cornell是三个大学内网页之间链接的图，actor是演员在同一个wiki百科里出现的共现网络，chameleon squirrel是wiki上特定主题的共现网络，最近出现一个超大异质图""，由5个社交网络penn94/pokec/genius/deezer Europe/twitch gamers，还有两个引文网络arxiv year/snap patents，wiki百科网页网络wiki，酒店餐厅评论数据集YelpChi，共9个数据集组成。
【细粒度图数据生成方法】
GenCAT是 Gencat: Generating attributed graphs with controlled relationships between classes,  attributes, and topology 2021
里的方法。先进的点在于，可以从类大小、类间边缘比例、属性值、图大小，共4个方向控制图生成。
然后教了一下怎么用GenCAT。
【实验】
作者给了详细的实验设计，连显卡型号都说了。从Cora里拿基础数据生成图，然后测试16个GNN模型。
测试结果可以看到，类数据分布从平均到不平均变化时性能呈现高低高低的走势，同质率从低到高时性能呈现高低高。
更细节的实验设计就略了。
【结论】
在细致的实验里，从fig2可以看到，大多数异质图GNN效果和MLP差不多，没拉开差距。
讽刺的是实验里针对异质性最好的反而是被骂了一遍的基于空域滤波的GPRGNN。
【评价】
是严格设计实验测试已有模型水平的论文。

22.What Do Graph Convolutional Neural Networks Learn?  preprint 
【abstract】
不用看。这个实验做的不好。
它提出，对于给定的图，邻域的潜在分布对GCN影响最大。那不是废话吗，你要用邻域信息，同质到异质性能差那么多，当然影响最大了。

23.Unsupervised Network Embedding Beyond Homophily || Transactions on Machine Learning Research 2022 || 0
【keywords】
SELENE、无监督、
【abstract】
该论文做的是无监督的图节点嵌入。提到了DGI、GMI、SDCN、GBT等该领域模型。
r-ego network就是r-hop邻居的induced子图。
不是太懂如果做无监督分类，在图3是怎么做GAE、H2GCN等模型的聚类的，是用已有标签+拓扑做聚类吗？而且怎么这些方法不管是同质异质都是性能随着同质率增高单调增加？哦不是，是失去了监督信号后只靠特征让邻接节点表示接近。所以它虽然是H2GCN的皮，但失去了对异质图的效果。
在了解No.24后知道，式3、4指的是要求在不同数据增强下的图节点表示也能一致。
通过fig4知道，本论文利用No.24的方法，对节点信息和图结构信息两方面分别做No.24。除此外，还做了encoder-decoder的压缩还原，都是自监督做法。
【数据集/任务】
9种经典数据集，竟然全面碾压了GAE、GraphSAGE、H2GCN、FAGCN、GPRGNN。
【评价】
其实就是拿No.24的套上图数据。但是为什么在异质图上也有效呢？作者从节点特征和GNN两方面做预训练区分倒是与效果很好的concat方法一致，不排除堆参数的可能性。
【链接】
https://github.com/zhiqiangzhongddu/SELENE

24.Barlow twins: Self-supervised learning via redundancy reduction || PMLR 2021 || 1257
【keywords】
自监督、图像、CV、
【blog】
https://blog.csdn.net/qq_36560894/article/details/115037398
【abstract】
很简单的思路，通过不同的方法对同一批图像做不同的数据增强，然后让网络学习表示，学习完要求同一图像不同增强下的表示一致。
关键公式在1和2。先从总数据X中采样一批数据Y，然后数据增强得到YA和YB，嵌入后得到ZA和ZB向量，要求其满足相同图像的相似度大，不同图像相似度小。
【细节】
这个方法看起来特别简单。不知道怎么效果那么好的，而且据说维度越高效果反而越好。
【目的】
自监督学习完的模型可以用于其他任务的模型迁移。
【评价】
怪了，为什么2021年的看起来很简单的模型效果还那么好。
这个论文主要解决图像领域问题，但是其思想

25.UD-GNN: Uncertainty-aware Debiased Training on Semi-Homophilous Graphs | KDD2022 
【keywords】
UD-GNN、
【blog】
https://event.baai.ac.cn/live/510 1小时17分左右
【abstract】
以node-wise level衡量同质率时，可以统计每个图中异质节点/同质节点的数量。如果把同质率从0-1等分4段，分别称为强异质、弱异质、弱同质、强同质，则观察经典的9个数据集，可以发现cora、citeseer、pubmed超50%节点强同质，squirrel、chameleon、wisconsin超50%节点强异质，cora-full、pokcc、penn94是不强不弱，4类里没有超50%的。
UD-GNN瞄准不强不弱的这一部分数据集做工作。从多个论文研究中也可以发现，这部分的确是效果不太好的部分。
通过逐节点观察模型GCN、GAT、H2GCN、GPRGNN在半同质率的数据集上运行，发现H2GCN和GPRGNN相对于GCN、GAT的优势在于对强异质、弱异质的大量提升，且保持弱同质、强同质性能不掉。
现有模型在同质异质不同节点上的性能有偏(这是当然的)，作者认为我们恰好可以利用它，因为不管怎么样，在同质率高的节点上的预测也是极其可靠的，可以作为数据增强。所以UD-GNN不是一个从头设计的模型。
这里的关键就在于，对于邻居标签不知道的节点i，你给出了图节点训练后的软标签，接着如何衡量i的同质率，或者说，如何衡量同质率是否可靠？
我们武断地认为，本来就同质率高的节点，它的抗干扰性肯定很强，多加干扰也不会太改变它的预测。那么我们就用加干扰的方式看看哪些节点比较可信。
【UD-GNN】
看fig3.
1.简单来说，dropout其实可以解释为贝叶斯神经网络，作者从分布来取具体的dropout。看不懂，但这不影响接下来的操作。只要知道，第一步，从一个模型架构Wb上通过dropout获得t个模型。
2.训练这t个模型，但是loss是Eq.4。这个loss要求所有t个模型的综合预测要准，且Wb有L2限制。
2.通过t个模型的预测，可以求单个图节点预测的期望，也可以看看方差。显然，方差大的就是不确定度高的。
3.上面本质是在做dropout训练Wb，只不过一轮里统计了不同dropout的输出。然后我们看看Wb里哪些参数偷懒没干活(值->0的参数)，将其作为重点训练对象、
4.非重点训练对象可以看作是对置信度高的图节点有很强的预测性能，我们好上加好，拿置信度高的图节点再去训练这些非重点训练对象。
5.接着对于重点训练对象，我们用置信度不高的图节点训练，让它们有点用。
6.结束。
之所以分成重点和非重点两部分参数，是因为作者潜在地将对于同质、异质预测的参数分成了两部分，这也符合实际观察，一定有一部分稳定的参数持续地让同质图节点有很高的预测性能。
【数据集/任务】
数据集有cSBM生成的、Penn94、Cora-full、Ogbn-arxiv这些不同不异的数据集。
对比模型有Mixhop、GPRGNN、JKNet、WRGAT、U-GCN、H2GCN、CPGNN。
实验结果表明，模型在用了UDGNN方法后，虽然对同质节点的预测方差增大了一些，但是对异质的方差大幅度降低。整体上的ACC都有提升。
【评价】
这个想法特别好。

26.Approximate Graph Propagation KDD2021
【keywords】
PPR、AGP、效率、
【blog】
https://zhuanlan.zhihu.com/p/394099492
【abstract】
先解释tabel1.
PageRank：初始每个节点值为1/n，π代表无穷次传播后的节点特征。那么π都有什么呢？应该包含其0~∞阶邻居给它的信息。那么i∈0~∞，可以知道了，0阶保留α个自己，1阶邻居先通过AD-1看有多少个，然后它们各给出1-α，自己再留下其中的α。以此类推。
PPR：比起PageRank，PPR只针对中心节点传播，其余的都当作看不到。这种方法其实在∞次迭代后与PageRank结果一样，但是若从中间截断，则能获得一个中心到四周的传染式传播。
其余略。
【铺垫和方法AGP】
【monte-carlo随机游走 2005】如果把全图初始节点特征x看作概率分布，我们可以用monte-carlo随机游走来估计任一节点的最终表示。这是一种不用全图更新的PageRank近似方法。具体来说，我们以x作为概率分布选择一个点，然后按照某种规则随机游走，直到依概率停下。在重复选择点、重复游走后，得到了许多路径，可以统计每个点作为终点的次数，其在整体中占的比例可以作为最终的表示估计。
【确定性传播 2006】
这个方法将传播问题拆成每一层传播结果的汇总问题。既然每一层传播都留下一些信息不再传播，那么就这样按层把不传播的信息汇总吧。
这里提到为每个节点的第i次传播维护两个变量residue和reserve，分别记录图传播在第i次走到节点的概率和走到节点且停止的概率。因此，在第i->i+1的传播中，residue会贡献自己的一部分给reserve，剩下的再传给邻居们i+1层的residue，相应地自己会更新为别人给他的传播。
这个方法的好处是方差小，即没有太大的随机性，不像随机游走，采样可能不符合真实分布情况。
但缺点是如果想单独知道节点i传播到j的概率，就很难了，因为确定性传播按层分，每一层都是对整个图节点的reserve和residue的更新，所以计算量很大。
【AGP】
综合上述两个方法，提出通用算法AGP。
tips：由现实数据观察得出假设，如果要近似传播误差小于φ，则应传播次数L=log1/φ。
我们会武断地在传播L次后将最终估计表示为所有的reverse和最后剩下的residue的加和，并认为剩下的residue就是大概的误差(它会越来越小)。那么只要residue在小到我们满足了后，就可以不改变图中节点的估计了。这让我们的图更新有了更大的灵活度，我们可以只在局部更新。而传播并不是我以为的将1-α平分给邻居们，在平分基础上还要看邻居的度，大就少给，小就多给。基于此，知道了度小的容易造成residue过大，那么在更新时可以从度大小来有序地更新节点。而且观察发现，多个节点传播给中心节点的residue的总和的量级，与单个节点传播的量级一致，所以我们甚至可以再次武断地认为，当看到节点u的residue大时，更新它，它会给邻居v增加residue，只从u给的这个residue就可以武断判断v的residue是否过大，而不是把v的邻居的residue汇总后再判断。当然了，这样肯定会引起一些人的不满，所以作者还在有序更新的基础上增加随机更新，即使有一些增量比较小，但还是随机地让他们更新。当设计好采样概率时(P5高亮)，这些本不更新的节点的期望增量与更新增量一致，所以该采样概率无偏。
【新问题】
上面对本不该更新的点更新，然后做了采样。这个采样其实也是扫描了一遍点逐个摇骰子，这就导致一共扫了两遍点，时间复杂度相乘，无法接受。
需要找一个不用逐一扫描点，且采样独立的方法。Subset Sampling是2012年的技术，它能保证采样时间复杂度与输出大小在同一级别。
具体来说，该方法预设每个节点的采样概率与度成反比，这样就和上述我们面对的问题一样了。如果一共有n个点，则分称logn组，第k组以[2^k,2^k+1]的度数划分，以保证同组的度数差在2倍以内。我们武断地认为同组的采样概率都以组中原本最大采样概率为准(即尽可能多地采样)，这样，在组内就是一个二项分布的采样了，组内有几个点，就采几次。注意，这个分组可以提前分好，不用重复分的，采样的时候也不用扫描这些点，只需要按数量二项采样。要是害怕前面的武断采样有偏，那还可以对已采样节点做拒绝采样。
总结，分组采样时间复杂度是logn，采出的点的个数与n个点的概率和Σpi差不多，对这些点做拒绝采样的复杂度就是Σpi，总和是O(Σpi+logn)。原本的复杂度是O(n)。有趣就在Σpi一般很小，至少没有O(n)大。这就是所谓的时间复杂度与输出大小同一级别，输出大小就是指采样的节点数量Σpi。
【数据集/任务】
【评价】

27.Instant Graph Neural Networks for Dynamic Graphs KDD2022
【keywords】
动态图、大图、InstantGNN、
【blog】
https://event.baai.ac.cn/live/510 22：00开始
【abstract】
该论文将No.26的AGP思想挪用到动态图。
论文核心内容其实是Lemma1的证明(在P13给出详细证明)。证明了一个关于reverse和residue不受时间影响维护的以节点为单位的恒等式。在该等式上观察可见，在特定条件下(指a=1，b0时)给有向图新增一条边，只对边的尾部节点的恒等式的右侧有影响，这说明至少在一次更新的范围内，该节点的信息失真，有residue超过阈值的风险。所以此时更新尾部节点，将风险消除，这可能会导致邻近节点接收到传播后也超出误差范围，这里就用到AGP的方法了，不断更新residue过大的节点，随机更新一些相关但residue小的节点。
【数据集/任务】
都是大数据集，比的是运算速度。准确率与AGP相同，但动态图更新速度比AGP快1~2个数量级。
关键就在于它不是漫无目的的更新，不需要随机挑节点来更新，只要从被影响节点开始往外探测就好。
【评价】
通过公式确定了近似估计并不用像AGP一样频繁地更新节点，这是主要贡献。

28.Heterogeneous Graph Neural Network via Attribute Completion WWW2021
【keywords】
异质图数据补全、
【blog】
https://www.jianshu.com/p/f0ddf70cfe18
【abstract】
HIN：异质信息网络。
该论文是基于metapath的方法。
作者介绍了一些异质图模型：HAN、MAGNN、GTN、HGT。这些工作的前提是所有节点属性完整，然而数据集属性缺失不可避免。
属性缺失可以分为两类：在DBLP数据中，目的是预测author节点分类，但是author没有属性；在IMDB数据中，要预测movie节点分类，但除movie外的actor和director节点没有属性。这是该论文要做的问题。
上述情况的一般数据缺失处理方法：1.one-hot补全，但是它没有语义信息，让同类节点也互相独立了；2.加和/平均归因：拿邻居的特征的加和/平均作为自己的特征，这又让自己的类别过于模糊了。
【属性补全方法】
基本思路是从已有数据中找到节点拓扑信息，以邻居环境作为缺失节点的信息。
基本思路是用metapath提取高维拓扑信息，结合弱监督方法(mask一些已知的节点特征，并以他们为数据来训练模型)。还结合attention机制(常规操作)。再结合具体任务(task-guide)。
【具体过程】
看fig2就够了。很简单的结构。
先drop一些已知特征的节点特征，然后用方法来估计这些节点特征，具体就是用metapath把每个节点的拓扑嵌入算出来，作为attention依据，假如我要补全节点1，它有节点345是邻居，则拿1与345的拓扑嵌入attention作为加和345特征的权重，赋给1。
接着会让这个复原的loss最小，还会具体选一个任务来让这个补全图来做，复原loss和任务loss就是总loss。
【评价】
其实不是很难的框架，不知道为什么2021年还能有这种 东西。关键假设就是觉得拓扑相似的点的属性也应该相似，但逃不掉要从1阶邻居上吸血的悲剧。不是metapath能用的数据集都没法用这个方法。

29.MAGNN：Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding
【keywords】
【abstract】
很简单的模型。用于metapath适用的异质图。
一条metapath可以定义metapath两端的两个节点是否连接，所以每定义一条metapath，就诱导出一个新的图。
作者设计多个metapath，然后根据诱导图计算目标节点表示，再用attention综合多个metapath诱导子图的表示。具体的attention计算在6，是bert式的attention。
【数据集/任务】
不出意外地，使用的是IMDB、DBLP、Last.fm三个数据集。
作为对比模型，有LINE、node2vec、Esim、metapath2vec、HERec、GCN、GAT、HAN。
比HAN环比性能高1~3%.
【评价】
不是我要做的数据集类型。

30.BA-GNN: On Learning Bias-Aware Graph Neural Network IEEE 2022
【keywords】
BA-GNN、不变学习、invarient representant learning、
【abstract】
这个方法从CV领域最近几年新出的不变学习方向而来。
不变学习的最初论文是Invariant Risk Minimization.
【不变学习】
https://zhuanlan.zhihu.com/p/567666715
其基础思想是学习到图中对结果影响不变的部分，例如一个用于分辨狗的网络，狗处在什么背景里，对预测狗(应该)没有影响，然而网络确实会根据背景来做判断，这是不对滴。
我们将关键的图像中的狗的部分叫做不变特征，其余的就是可变特征(或虚假特征 spurious feature)。
具体请见不变特征学习记录。
【BA-GNN】
看P6黄标往后。
对不变特征的学习好理解，但是P6的C.bias-Aware environment Clustering看不懂。
略。
【数据集/任务】
数据集没有同质率的介绍，有cora、citeseer、pubmed、coauthor-CS、coauthor-Physics、Amazon-Computers、Amazon-Photo。
BA-GNN与其他不变学习一样，是可以套用在其他方法上的。所以作者将其套用在GCN、GAT、APPNP、GraphSAGE上，上述数据集上均有1%左右的提升。有些寒酸了。
【评价】

31.Exploring Edge Disentanglement for Node Classification WWW2022
【abstract】
不看。是无关领域。
该方向认为不同节点连接有原因，不同原因导致边天然存在不同属性。这个方向是为了找出这些属性。
【数据集/任务】
数据集就4个，模型比的也少。
【评价】
纯纯cjb。

32.Heterophily-Aware Graph Attention Network 2023
【keywords】
HA-GAT、
【abstract】
论文发现，对于不同异质性的边，其准确率完全不一样。所以问题就在于如何察觉到边的同质率，以辅助网络判断。
【HA-GAT】
先看4，看看怎么计算异质边的权重。
首先要提供边两侧节点ij的软标签yi和yj，然后外积形成k*k矩阵m，k就是类别数量。m的每个元素都代表相关边的硬概率，假如yi是a类概率为1，yj是b类概率为1，那么mab=1，其余为0，ij的边就是这样一条异质边，我们就识别出来了。注意，mij只为节点i和j服务，mij的尺寸是R^k*k，如果其中最大的值在对角线上，那么说明ij类别相同，并且所在行列能指引出所相同的类别。
我们会为所有mij配备一个通用的尺寸相同的k*k参数矩阵对其做内积(对应项相乘求和)，它可以看作各种边预测的可靠性矩阵。注意，这里Eq4要求权重不可为负，并为权重事先定义了超参数λ作为学习率。另外，对于自环的边会单独建模，这种边肯定是同质边，就不需要m了。
上面说的内积就可以作为边的attention权重，进而得到某个点对于其连边的attention score。Eq6是很简单的形式。接着就可以用于Eq7的GAT了。z
GCN一共用了2层.
上面就是HA-GAT的全部内容。
【数据集/任务】
很惊人，它的效果竟然可以好到连squirrel都有69%，chameleon有72，可以说完全碾压了。
【评价】
我得好好验证一下了，这东西真有这么好用吗？


33.Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification ||  ICLR2022 ||
【keywords】
与No.9是同一篇，是22年的半成品版本。ACM-GCN、
【abstract】
作者直接指出，通过超参数的调整，基础的GCN其实比H2GCN、CPGNN、GPRGNN等效果更好。这引出一个问题，GCN这种基于同质假设的模型，不一定总在异质图上表现不好。举个例子，假如所有异质率高的节点，它的异质邻居们总是保持一个分布，那么聚合后，纵观所有节点，它们的特征仅仅是多了一个常数的倍数罢了。这让聚类本身仍然是同质。
通过fig3的实验，可以看到两个数据集上都是异质率越低反而效果越好。如果依照概率γ再给图添加随机的异质边，就是在打破分布，就会导致异质模型越来越差。
这个发现告诉我们，如果模型能根据软标签+邻居标签分析出该软标签是否可靠，那么将提高模型的性能。
具体来说，综合软标签和邻居的表示，有这样一个规律：
1.软标签有稳定分布，邻居符合分布，则加强概率。
2.软标签有稳定分布，邻居不符合分布，则削弱概率。
3.软标签无稳定分布，邻居对软标签没有影响。
但可惜模型并不是按照这个结论来设计的。
【评价】
这个想法符合我当初设计的模型思路，通过邻居和自身的综合判断邻居是否应该参与加强预测，若该判断准确，则可加强效果，其性能底线是MLP。

34.SlenderGNN: ACCURATE, ROBUST, AND INTERPRETABLE GNN, AND THE REASONS FOR ITS SUCCESS
【abstract】
太理论的东西我不看了，只注意到它采用了多维度数据拼接的方式，并实验得到持平效果。
其中常规的有结构信息、自身信息、GCN卷积信息，不常规的有no-self-loop的卷积，以应对heterophily graph。这与我的想法有相似，但他不够大胆。
【SlenderGNN】
式3.
模型拼接4部分表示。其中，g是PCA操作+L2正则化，U是在邻接矩阵A上做SVD。
【数据集/任务】
所有模型的数据分配为2.5 2.5 95，常规来说只有同质图这么设计，异质图设计为60 20 20。所以不要硬对比acc。
【评价】
是性能优化。
里面的Distinguishing Factor 1~3 很合我的想法。

35.Make Heterophily Graphs Better Fit GNN: A Graph Rewiring
Approach
【abstract】
很奇怪的论文。出自ACM2017，但是2022年上的arxiv。
内容是现在广泛使用的方法：图邻接矩阵增强。
具体就是事先用其他手段计算出相似节点，然后增加它们的连边。这种手段下，可增强异质结点的同质性。
【数据集/任务】
作为数据增强方法，为GCN、GAT、GraphSAGE、APPNP、GCNII、H2GCN增强，获得大幅提高。
它只分训练、测试集，然后固定epoch，每个batch可能训练100、1000、5000、10000个点。像squirrel一共才5000+个点的，batch10000就是整个数据集的训练集一起训练。
【DHGR】
这是数据增广的主要方法，需要训练。
它会先根据Ytrain硬性k次图卷积的方式获得节点标签表示，然后计算标签相似度矩阵DY。
再用同样方法对X做硬性k次图卷积，计算DX。
然后训练GCN，要求它训练完的节点表示必须和DX、DY相近，分别用式7、8的loss来约束。我们会认为训练得到的节点表示->相似度矩阵是综合了DX、DY的，更可靠。对其中相似度高的点对进行A的增广->Ahat，用它来正式训练分类模型GCN。
【评价】
不好说。这种方法的弊端是，假设特征相同的节点类别相近。但如果节点分类仅仅靠邻域呢？邻接矩阵增广会破坏邻域。所以天生就是舍弃一部分精度的。
它的实验做得是真的足量。

36.Simplified Graph Convolution with Heterophily   NeurIPS2022
【abstract】
前面都是废话。到3才是正题。
这公式一眼就看出是GPRGNN，但是它没加自环self-loop。
loss要求传播后的表示和原本表示差不多，而且约束第一项的可学习系数要越小越好，以逼迫最终表示不是与原表示一模一样，必须掺杂其他n-hop neib 的表示。
它里面Futher remarks里，第一句话很有意思，说如果K过高，即使不使用自身特征，模型也能从其他邻居中找到自己。原理他没有说，其实就是模型发现了一个找同类的规律，K越高，找到自己的概率越高，这其实不是因为邻居中的确有与自身相近的节点，而是自身一定会是自身的2-hop neib，可能会是自己的k-hop neib。其实还是找到了自己。
这更加体现了我在传播中去除自身表示的动机。
【数据集/任务】
超参数K是1 2 4 8，异质数据分配是60 20 20，同质是2.5 2.5 95， R与n挂钩，n是节点数量。 
acc取10-splits的mean。
效果嘛，整体不如GPRGNN，但是在一两个数据集上效果远超当前水准。ASGC其实不是一个比拼上限的模型，它的优点在于简单快速，所以不需要性能提升，只要别掉太狠就行，这一点上ASGC做到了。
【评价】
看起来简单，但其实还挺有意思的，很多细节我写论文的时候用得着。

37.How does Heterophily Impact the Robustness of Graph Neural Networks? Theoretical Connections and Practical Implications  KDD2022
【blog】
https://zhuanlan.zhihu.com/p/563084781
简单的翻译。
【abstract】
一般来说，对于图数据的攻击分为：addational、delete，即增加和删减。这两者的目的都是改变原有图结构。
攻击图结构对于同质、异质图的效果不同，一般来说，同质图攻击会降低同质率，因为基于同质图的model都没法有效利用异质信息。异质图攻击不一定，得看节点的度，如果度大，则提高同质率，度小反之。
其抵御攻击的手段极其简单，式1。就是分别聚合自身的多层表示、邻居表示，再综合编码(Encoder)。
数据实验中提到了一些我不懂的，只稍做记录：ProGNN GNNGuard GCN-SVD GCN-SMGDC.
soft medoid aggregation with GDC (SMGDC)
【数据集/任务】
用了SVD和SMGDC的抵抗力的确更强，只掉了3~5%左右。

38.Generalizing Graph Neural Networks Beyond Homophily 
【abstract】
是H2GCN的初稿，比现在的效果更低一些，实验数据更少。


39.Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation 
【keywords】
CLP、
【abstract】
模型利用标签传播解决异质图上的节点特征聚合，基本靠三个步骤：
1.先仅仅用普通MLP训练节点特征，得到软标签。
2.根据已知标签的连通性，制造一个全局兼容矩阵。
3.以矩阵为依据，聚合邻居信息。
全局兼容矩阵在BM-GCN、CPGNN里出场过，有了很高的性能提升。
计算类兼容矩阵的方法在上述两个论文里也有提到，就不赘述了。
重点在于3如何定义聚合信息的边权重。
接下来是重点:::
作者采用Eq7计算一个全新的式子，Fij的值代表了这样的意义：根据节点i的软标签可以获得一个邻居类别的分布，假如i的邻居j的类别恰好是高概率，则可以说明ij的边权重eij应该很高。此举有望抵抗个别节点产生的噪声，可以发现类别之间稳定的连接关系。
(回顾BM-GCN和CPGNN，前者为类别连接行为相同的点添加新边，后者通过统计i的邻居是否符合类兼容矩阵的测量值，来加强或减弱i的软标签的确定性)。
总之，基于原本的A获得的F矩阵就是边权重矩阵。然后就是标签传播过程eq8 9 10.
eq8给出了一个希望中的稳定结果，即最终的label B一部分靠MLP预测的软label D，另一部分是最终label B靠A=F传播来的。虽然不知道为什么这个可以，但是他们就这么规定的。
那么最终label是什么呢？我们可以从之前结合了已知label和软label的B开始迭代eq9，直到迭代稳定。这就得证明这个迭代是可收敛的了，也是这个论文最麻烦的地方。作者证明了它收敛。
eq10是老方法，可以看到它的聚合与中心节点完全无关，邻居们只要有边就可以平等地掺和进来，不管什么节点都是一样的聚合。而我们的eq9有结合中心节点信息得到的权重调整A^F。这体现了我们Eq9的优点。
【数据集/任务】
当然要比对CPGNN，没有BM-GCN我失望了。
数据集分段有三种 5 5 90 、 10 10 80、 48 32 20，不分同质异质。结果特别好，但是没有拉开差距。
【评价】
基于类兼容矩阵的第三个模型，也是很好的论文。



40.GLINKX: A SCALABLE UNIFIED FRAMEWORK FOR HOMOPHILOUS AND HETEROPHILOUS GRAPHS ICLR2023
【Keywords】
GLINKX、知识图谱、KG、
【abstract】
它有一个前置论文叫LINKX，眼熟但是忘了，应该是我没看。
先来翻译翻译GLINKX的算法流程algorithm 1：
1.数据是KG，准备好KG预训练模型给的embedding P。这个就是位置嵌入PE。
2.1 仅以train set节点集合，计算节点为单位的邻居类别分布yhat。
2.2 训练学习节点邻居分布.(a)根据节点特征xi，KG嵌入特征pi，参数Θ1，用模型f1计算分布yi。(b)用负采样交叉熵优化模型，loss目标是让yhat和yi分布一致(Eq2)。
2.3 f1的soft label model算是训练好了。通过它来计算包括所有节点的邻居分布y'。这里不掺杂train set的label
3.1 设计新模型f2来预测中心节点的标签yfinal，需要xi, pi, yi'，参数θ2.
3.2 同样用负采样交叉熵优化f2.
可以看到GLINKX免去了传播，用的是1-hop邻居分布+自身表示的方式来预测。不传播就省去了大量算力开销。
【数据集/任务】
实验部分没做太多，也没说很细，数据分的比例都没说。
消融实验做完发现异质图里PE的作用是真滴大，同质图里更需要邻居和自身的表示。
【评价】
不用普通传播的方法，仅用1-hop neib。工程实现上没什么难的，但是值的记住。



41.A CRITICAL LOOK AT THE EVALUATION OF GNNS UNDER HETEROPHILY: ARE WE REALLY MAKING PROGRESS? ICLR2023
【abstract】
好狂的论文标题，说现在的工作都没屁用。不过我也有这样的感觉，好多模型结合了MLP的输出，但模型却拉低了MLP的性能，有锦上添屎的感觉。
接下来将详细记录论文内容。
【摘要/Introduction】
哇，从摘要就给了我一个大震撼，说squirrel和chameleon数据集有问题，有重复节点，会让模型效果提高，去掉重复的会让模型效果大打折扣。然后说给出了新的数据集。
Introduction最后一段很重磅，作者说用他做出来的数据集，所有针对异质图的模型都没有baseline好。同时发现分离自身和邻居特征的确有用，可以提高baseline的性能。
【3 异质图数据的问题】
接下来狠狠批评。
1. squirrel和chameleon来自Wikipedia，actor是wikipedia的共现电影人物，texas,wisconsin,cornell都是WebKB数据集抽的。所以6个数据集一共就3个来源，太少，不足以覆盖不同的异质图类型。
【3.1 squirrel和chameleon】
重复的点和配套邻域太多了，近乎一半，会导致训练过的点在验证集里也存在。
重复点有个特征，只有出度。所以将这部分点过滤掉后，table2显示好多模型有极大的性能下降。这里缺少MLP的对比，否则更有说服力。
值得注意的是，清洗后，最好的模型是FAGCN，GPRGNN还不错，GloGNN和FSGNN掉得太狠了，说明原本的都是虚高。
【3.2 cornell texas wisconsin】
这部分提到了，在这三个数据上的论文复现效果总是有高方差，在调查后发现这三个数据集有好多长尾数据，甚至有一个类只有一个节点的情况。
这种情况用acc衡量模型效果是很奇葩的。
【4 新数据集】
数据集要满足这4个要求：
1.要是异质的。
2.图结构要对数据预测有用。
3.数据来源要多样化。
4.图尺寸要适合，太大了就不好重新评估已有模型，太小了肯定也不行。
对图的同质率计算用2022年的论文Characterizing Graph Datasets for Node Classification: Beyond Homophily-Heterophily Dichotomy 提出的调整后同质率，它可以平衡类不均衡的情况。该论文同时提出label informativeness(LI)，用于测定label所携带的信息量，且用log函数将其固定在01之间。(具体请看上述论文)。该指标可以分辨出两个异质节点强相关的情况。
romance-empire：这是一篇文章，按照词的顺序将词作为节点串起来。
amazon-rating：来自SNAP数据集，节点是商品，连接边代表二者总是同时购买。
Minesweeper：这是扫雷游戏图100*100，20%为雷。与扫雷一样，节点特征是相邻8个格子的雷数量。50%节点的节点特征mask掉了。
Tolokers：基于Toloka众包平台数据，节点为至少参与过13个项目之1的工人，边是两个工人共同工作过。任务是预测哪些工人被某个项目给ban了，即不让他们参与这个项目。节点特征基于工人的基本信息和过往任务统计。22%的工人都被ban了。
Question：数据来自问答网站Yandex Q的医学板块的某一年的问答数据。节点是用户，边是用户在这一年内为另一个用户解答过问题。任务目标是预测哪些用户在一年过后仍然活跃。节点特征为用户描述中的文本进行fasttext嵌入后的平均值。15%用户设定为没有描述，但其实数据里有，作者用一个01二进制表示哪些用户的描述被抹掉。
【数据集/任务】
在上述5个模型中实验ResNet、GCN、SAGE、GAT、H2GCN、CPGNN、GPRGNN、FSGNN、GloGNN、FAGCN、GBK-GNN、JacobiConv，发现性能top3基本在同质模型上，GAT效果最好。
【评价】
大受震撼。很重磅的论文。


42.Characterizing Graph Datasets for Node Classification: Homophily–Heterophily Dichotomy and Beyond 
【abstract】
41和42的作者是一批人。
这里面给了两个重要指标，一个是adjust后的homophily，另一个是label informativeness。
作者指出，常用的同质率公式有缺点，无法衡量不同数据集之间的同质水平。因此，该论文涉及许多数学知识。我只要知道结论就行。
只需要理解h_adj和LI的公式。
【评价】
这个论文没看到期刊接收。


43.ARE GRAPH ATTENTION NETWORKS ATTENTIVE ENOUGH? RETHINKING GRAPH ATTENTION BY CAPTURING HOMOPHILY AND HETEROPHILY       ICLR2023
【KeyWords】
GATv3、
【abstract】
该论文设计的GATv3如下：
Queary是中心节点，Key是邻居节点。用GCN来得到节点与其邻居们的表示之综合，以丰富的信息量来判断相邻两个点的相似程度，而不是v1和v2的仅仅两个节点。并且Eq8直接用中心-邻居表示做点乘，得到标量，这是简单的相似度计算。
Eq11~13给了一个更有意思的计算。其最初想法是Figure2。直接看式子就行。
首先，把所有节点当作query做GCNq，得到Q。再把所有节点当作key做GCNk，得到K。
不管你的两个节点是否连接，我都在Eq12做QK^T，不要脸地硬计算attention。但是总要有个先来后到，有拓扑的attention就该更高一点，于是拿出A(虽然这里标的是Ai但是就是普通的邻接矩阵)来对有直接连接的节点对进行attention加强。这里设a+b=1，那就不好训练了，只能网格搜索，也不能bi-level training，是个可以改进的地方。
【评价】
不评价，没啥可说的，结果肯定不稳定。但是作为全局attention的一个方法，在异质图上表现肯定很好，与“拿k近邻加强A的边”的态度是一样的。
【想法】
GATv3，好就好在它计算了一个不变的全局attention矩阵，并拿A对其加强后使其有用。对我来说，这个全局不变的attention矩阵很有启发。

44.SIGN: Scalable Inception Graph Neural Networks  ICML2020
【abstract】
该论文是要把普通模型扩展到超大图上也能用的程度。
注意符号，这里的W才是A，A=W~=W+I。
直接看式4。
θ是参数，尺寸d*d'，然后concat从0~r层的输出，Ω合并这些参数，尺寸d'(r+1)*c，c是类别数。
这个的优点就是r层传播可以提前算且一次性计算，这归功于非线性变换只做了一次。
我有点记忆混乱了，这个论文2020年的，这个方法已经有很多论文用了，不知道是不是都从这里来的。
【评价】
我反正是没看出来这个东西有什么特别的。



45.HP-GMN: Graph Memory Networks for Heterophilous Graphs ICDM2022
【abstract】
这个论文从全局获得信息来预测。
论文有一个假设，认为不同类型的节点邻域分布较为稳定，且可区分。
论文依此抛出rv1、rv2、rv3的公式，分别代表节点自身特征、邻域soft-label->one-hot label的数量统计、按前面统计的不同类别的邻域节点，统计它们的平均原始特征。
还抛出了和GDC一样的方法(4)扩散矩阵，以收获综合了多层信息的图结构信息。这里没写，rv4就是SX，S是Eq6。
然后用Eq8把所有rv1~4做MLP和concatenation得到qv，称qv为v的局部表示。我们会认为qv是一个全局dict构成的，由dict M的各个向量元素加权求和得到，因此假设M已知，计算qv与M中各向量相似度，并softmax得到相似度归一化权重，得到最终表示eq10的vv，称vv为v的全局表示。这里Eq10的V的第i行是节点i的全局表示。
根据eq11，合并qv局部和vv全局，预测最终label yv。这个的loss是Eq13.
上文的M不是人为设定的。我们希望每个节点的qv至少贴近一个M中的mi，因此M有了学习目标，即M作为向量集合，与所有的qv比较后，要求其得到的单个比较下的最小距离之和最小。距离函数dist可以任意设置。
因为这里有两层min，所以eq14的优化方法要记住，和cnn一样，需要记录每个点在one-batch traning中对应哪个mi。这个的loss是Eq14.
我们还希望M中每个mi的作用平均，以推动M有效工作。我们注意到，Eq9得到的所有节点的局部表示Q对M的相似度记录为S，sij代表节点qj对mi的相似度，因此si'=Σsij是统计mi与多少节点相像。我们此时希望每个mi都大致相同。用熵可以衡量si'是否均匀。计算信息熵的公式是-plogp，它越小越说明信息越多，然而我们希望的是它大，所以取个负数，回到plogp，并让其变小。放心，它的确是个负数，但在离散概率分布里，它有最小值。这个的loss是Eq15。
代码里还有一个loss，是让memory的F范数最小。是怕memory过拟合，太贴近真实节点特征也不好。
上述三段各一个loss，拼起来就完整了。
【数据集/任务】
记得模型一开始要计算soft-label转one-hot label吗？在异质图里，MLP做这个比GCN要靠谱，在同质图里，反而要用GCN代替MLP。所以TABLE III 里出现了HP-GMN(GCN)和(APPNP)的字样。
【评价】
memory方法的确有用。
rv4融合了基于PPR传播的特征，严格来说该论文没有直接嵌入A相关的信息。它的想法很可能脱胎于BM-GCN，先获得soft-label，然后结合以前的memory方法计算所谓的全局嵌入，结合原本的嵌入得到最终预测。最后loss主要用于限制memory，4个loss有3个都是限制memory的。
它的亮点是基本没用高阶信息，就低阶统计的3部分信息。PPR可以算个高阶吧。


46.Meta-Weight Graph Neural Network: Push the Limits Beyond Global Homophily WWW2022
【keywords】
MWGNN、
【blog】
https://zhuanlan.zhihu.com/p/518211903
【abstract】
这篇论文发现了我一直有疑问的问题，就是某一类数据可能是很多个小类数据合并而成的，所以它的分布存在多个峰，你没法用一个规则同时拟合这两个峰。
figure1可以看到，在对chameleon图数据聚类后，两个聚类中的同类节点有明显的特征差异。
【模型】
看3.
1.首先是名为“局部度文件LDP”的东西，统计每个节点的度和邻居度信息，得到X_LDP。作为序列数据，送到GRU里，得到ht。将ht看作Dt，一个体现拓扑结构的分布数据。
2.再为每个节点安排邻居集合，按照邻居的degree排序他们的feature，作为序列送入GRU或是直接做average，得到Xfeature作为X的特征分布，转化为Df。
3.统计每个点到所有点的最短路径距离，以指导中心节点在全图的大概位置，得到Xposition，转化为Dp。
4.分别计算Dt、Df、Dp的attention score w 式6，再归一化，再式7计算加权和，得到W.
W仅仅是一个称为节点本地分布NLD的东西。进而计算eq10、9、8。eq10根据特征X和邻居的度X_LDP，分别获得其对A的调整，eq9根据超参数α综合上述两部分，eq8正式调整邻接矩阵权重A，然后是正常的GCN。
在3.2.2作者融入前人工作，手动调整自环权重和自我、邻居聚合权重，并且还把拓扑嵌入Ht加入了进去。
【数据集/任务】
在chameleon、squirrel上的性能好太多了，双双拉到了75%+。
其他的也不弱。 
【评价】
太数学了。论文是阿里和北大出的，证明了度的复杂程度会直接影响传统模型性能，必须为模型提供基于度变化的自适应卷积方式。



47.Improving Your Graph Neural Networks: A High-Frequency Booster 
【abstract】
作者认为高频信息更需要获得加强处理，且希望新方法可以作为一个插件放到GNN里，而不是硬性、不通用地修改GNN的结构。
作者做出的东西叫Complement LAplacian Regularization (CLAR),补全拉普拉斯正则化。
【说人话】
原先的GCN除了对标签预测有CE交叉熵loss，还要求相邻节点最终聚合的表示相近。后者被认为是Low-pass filter。
作者使用的CLAR，希望创造一个基于loss的high-pass filter，就像上面说的后者一样。但我认为这里是作者强加说辞了，证明得不好。作者在Eq7和8乱bb一通，是在含糊其辞。
以下是我对CLAR有用的理解。
【CLAR】
对于同质图，我们会自然地希望相邻节点表示相似，以保证有些孤立奇点能表现地正常一些。这其实在GCN里隐晦地做到了：两个相邻节点的10-hop邻居聚合GCN，它俩肯定有很多重复的邻居，那么他俩的表示也一定相近。因此，Eq5的Lreg也就不怎么用了。
对于异质图，我们自然希望与上述情况相反，但可惜我们不知道哪些节点是中心的同类节点，因此只能草率地规定：不与中心连接的节点要相似。我们不能每次都算所有远端节点，所以作者在Algorithm 1做采样操作，每次取一部分的远端节点，大概是总节点数的S倍，或是总边数的S~2S倍（这看起来也不少）。此时的loss很大，而且除了让标签预测对、表示相似的远端节点继续相似这两个方法外，没有好的让loss降低的方法。我想过GCN会让所有节点表示相似，那样的话或许标签不匹配产生的loss要比远端相似的loss还要低，就失去了训练的目的了，但是是我肤浅了，在S取1~32的范围内，实验表明loss功能仍然完善。而且除了参数W=0，我也想不到让远端相似的办法了，只能让基于中心-邻域表示相似的节点更加相似，有限地减小loss。
【数据集/任务】
TableIII 展示了效果，对基础模型GCN、GAT、SAGE有提升，但是提升得像是误差一样，而且基本上靠GraphSAGE抬高上限。最可恶的是这个实验压缩了GPRGNN的模型，从10-order变成2-order。这本质上其实是给自己的模型用多余数据，反倒不让其他模型用的意思。
【评价】
烂中烂，而且很可能数据造假。


48.BREAK THE WALL BETWEEN HOMOPHILY AND HETEROPHILY FOR GRAPH REPRESENTATION LEARNING || ICLR2023 reject ||
【KeyWords】
omnipotent GNN、OGNN、OGCN、
【abstract】
作者分析GPRGNN在异质图上不行是因为没有摆脱消息传递机制，H2GCN虽然在图上最优，但是现实数据集上性能变差，是因为没有显式地结合结构信息。
【OGCN】
要处理三部分数据：自我节点特征、邻居特征、结构信息。
“自我”：用MLP。
“邻居”：聚合邻居特征可以是任意GNNs，但是简单GCN的叠加在2层还行，3层性能就废了。一个新的方法(不怎么新)是解耦，就是GPRGNN的那一套。(实际上这一套来自于DAGNN、PPRgo、PPNP，基本就是先MLP成软标签，然后多层传播，拿所有层的输出汇总作为邻域信息。
DAGNN和GPRGNN唯一的区别是在自适应加和处，DAGNN采用一个向量S与不同层邻域输出做向量乘法得到权重，而GPRGNN是不依赖任何信息的权重参数学习。
“结构”：没想到结构的意思是A^k的[v,:]，就是纯粹的统计节点的各阶邻居情况。而且这个方法在LINK和LINKX里就用过。
【如何融合上述三部分？】
【先预处理】
首先是特征转换，即H=XW。这里不用多余的bias或者non-linear transformation，否则效果变差。
其次是邻域特征聚合。用A对H做多层的传播并简单加和，就像GPRGNN一样，但是这里A是否有self-loop没说，看样子是没有。
最后是图结构特征。是多层的结构简单加和，再用W变换，即H_structure=(ΣA)W。这里也不用MLP，因为实验结果没有明显性能提升。
【再自适应融合】
作者采用attention方法，用可训练参数规定三部分的att score P，然后求softmax，再把得分用于加权。最后用ReLU收尾。
融合后的特征用于线性地预测label。
【loss】
loss使用bi-level optimization。
基本思想是，先用训练集优化参数W，再用验证集优化加权参数P。
【数据集/任务】
选择的数据集我都没见过，但是无一例外数据量很大。
有arXiv-year/penn94/twitch-gamer/genius/Coauthor CS/Coauthor Physics/Cora/Citeseer/Pubmed
对比MLP、GCN、GAT、DAGNN、LINKX、H2GCN、MIXHOP、GPRGNN，OGNN有7/9都更好。
Table3的消融实验表明，structure对异质图意外地很有效。
【评价】
！！！这个论文是我想的那样，做了自身、邻居、拓扑的自适应有机融合。但是由于创新性不足，还没有与最新最in的ACM-GCN和GloGNN对比，还没有做6个基础数据集比较，bi-level optimization也没有缘由，所以被拒了。


49. HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS ICLR2022
【KeyWords】
GATv2、
【blog】
https://blog.csdn.net/m0_47256162/article/details/127939970
【abstract】
不说废话。
GAT性能差是有目共睹的。
回顾GAT的做法。先对每个节点用W线性变换，然后concat中心-邻居特征，用参数向量α将其变成标量，再做LeakyReLU，接softmax作为attention score。
GATv2做小小的改进。先concat中心-邻居，再用参数矩阵W线性变换，接LeakyReLU，再用参数向量α变标量，作为att score，接softmax。
为什么这么做呢？因为对GAT发现了一个问题，看figure1，10个query nodes(中心)对10个key nodes(邻居)做attention计算，发现不管哪个query，都只对key8最attention，好像GAT并不能依照中心节点的不同而动态地调整它的attention，而是从全局出发找一个通用的edge-weight adjustment。
分析一下GAT为什么会这样，发现GAT是设计的结构功能不行。参数W对单节点统一变换，它仅仅改变了特征空间的基，其内容本质没变。α面对query||key，对不变的query和变化的key，为了让α顺利工作，必然更关注key的变化，而且是不结合query的关注，这就导致α被key控制，当key不变，query变化时，α不会随着query有太大变化。
反观GATv2，先结合query||key做W，这就是综合分析。然后再做α，就有理有据了。



50.Improving Graph Neural Networks with Simple Architecture Design || 2021
【keywords】
FSGNN、
【code】
https://github.com/sunilkmaurya/FSGNN
【abstract】
不如直接看Algorithm 1，配合Figure 1.
这个算法很合我的心意，因为它有no self-loop。
FSGNN的基本方法就是收集不同种类的邻域信息。一共三类，第一类是1个自身特征，第二类是without self-loop的K个K-hop邻域特征，具体就是用不加自环的Asym来卷积，第三类是with self-loop的K个K-hop。
在我看来，不加自环统计的是各层的邻居分布，加自环统计的是用特征加权来表示的结构信息，且它能保证中心到四周的重要性递减。
然后会做一个正则化，Hop-Normalization，其实就是对所有节点表示或邻居表示都变成单位向量，所以理所当然Eq6用的L2正则化。这个在代码里是选用的。
有了上述2K+1个信息，再加上可学习参数α=[α1~α(2k+1)]。这里要保证α加和是1，所以要过一遍softmax，也是方便找到2K+1里最重要的那些。但是做的不是加权，而是concat，再过一遍ReLU+线性变换，结束。
【数据集/任务】
Cora Citeseer Pubmed Chameleon Wisconsin Texas Cornell Squirrel Actor，选的K有3和8，在异质图上表现很好，而且在chameleon squirrel上好过头了。这可能就是hop多的好处吧，但是是不是好得过分了。。。GPRGNN也就差一个without-self-loop，也没单独为每一层的输出再填W。很担心这个东西是否能训练得了。
【评价】
这个模型让我来说就是太大了，K不能选太大，而且2K+1个W不那么好训练，还有参数α不好搞，论文里初始化为全1了，倒也是个方法。

51.GPNet: Simplifying Graph Neural Networks via Multi-channel Geometric Polynomials 
【abstract】
直接说方法，看3.2Key Designs。
D1：以channel为单位，会设计一个邻域范围倍翻的卷积速度，就像Eq8描述的一样，会以qm的速度往前翻。假如qm=3，那么该channel聚合的邻居hop就是1 4 7 10这样。且聚合方式是sum。（实际没这么简单，但是暂时这么介绍）。当然，你的邻接矩阵A可以是with self-loop，也可以no self-loop，看哪个效果好。实际上俩都应该有效果。
D2：多channel的信息要统合，这里也有一个agg函数Eq9。具体来说，作者在这里尝试了4种agg函数，分别是Max-FP，Min-FP，Avg-FP，Sum-FP。
D3：每个channel配备了原初特征，毕竟邻域再广也肯定要以自己为主，所以在每个channel融合本身特征时，要一个各channel统一的超参数α，以保证原始特征的地位。
D4：另为每个channel配备一个超参数β∈{-1,1}，随着图的具体情况来改变设计。功能就像GPRGNN一样，以指导网络采样高频还是低频。结合D3D4的单channel公式是Eq12.
再多的实验结果我就不细说了。
5.2是参数设置，α的设计范围还是很广的，β除了Texas和Wisconsin设计为-1，其余都是1。
【数据集/任务】
这个东西竟然敢用全监督做预测？是我们对全监督的理解不一样吗？
【评价】
一眼不行，实验做挺多，但是没卵用。


52.Finding Global Homophily in Graph Neural Networks When Meeting Heterophily ICML2022
【keywords】
GloGNN、GloGNN++、
【abstract】
【GloGNN】
Eq1写下了稳定状态下的信息传递公式，这里的左边虽然是Hl，但是只是希望l层时的输入输出相等。
Eq2对X和A分别做MLP，维度都变成label数量。
Eq3将上述两部分按可学习参数α加和，我们可以认为用A来预测是希望从1-hop拓扑层面找到预测证据，例如label0的都连接了节点3，自由地高亮那些有助于判断各个类别的节点。
Eq4加入参数为γ的PPR，重写了Eq1的稳定状态。Z看似是邻接矩阵，其实是可学习的邻接矩阵，希望它能从全局上聚合相似节点，这便是GloGNN一开始说自己是全局聚合的原因。γ是超参数。这个稳态是靠O维持的，真正的稳态下O=0。如果去掉O，Eq4将是一个迭代更新H的公式Eq6.
Eq5给出了目标函数loss，首先肯定希望Eq4成立，即左-右=0。Z是可训练参数矩阵，是全局衡量点与点之间关系的矩阵，我们希望Z别那么极端一些，所以加了F范数，否则绑定太深就是过拟合。我们还希望Z与1~K层GCN的消息传递近似，因为GCN的确是被证实很有效的网络，因此Z实际是从1~K-hop邻居的表示矩阵上做稍微调整得来的。
Eq5结合s.t. Eq4可以写拉格朗日乘子法，求得使Eq5最小且Eq4成立的解析解Zl*。
既然解析解Zl*出来了，那么也就能迭代更新H了。但是Zl里有H*H，外面还有Zl*H，计算困难。经过Section C的简化，得到Eq7的迭代式。论文说了为什么计算简化，虽然Eq7和8看起来还是很复杂，但是Eq8确实简单，里面计算的是H^T*H，尺寸是c*c，c是label数，所以H^T*H很小，做矩阵逆也就简单。剩下的计算只要遵从经典的“从右向左”计算，那么计算量就不值一提了。
【GloGNN++】
改得很少，相比于Eq5，在Eq10只加了个对角阵Σ，用于调整不同维度的重要程度。个人觉得论文的解释不好，Σ的作用是加强分辨率高的维度、减弱分辨率低的维度，是整体上的attention调整。
随后给出了新的Zl*。
【loss】
GloGNN的loss第二重要。
首先肯定要保证稳态出现，第一项好解释。
接下来要限制Z的活动范围，首先一般来说都用L2范数来约束参数矩阵Z不会过多偏向于某一个维度，让各个神经元的工作内容尽量平均，体现在作为拓扑矩阵的Z上，就是为了在聚合时不仅仅关注某几个节点，而是尽量多、尽量平均地从所有节点中吸取信息。这就是第二项Z的F范数的由来，F范数是矩阵所有元素平方求和再开方。
Z还要要求其形式与A的1~K次方的加权和有关。一般来说，拓扑上距离的远近能为节点指明相似节点的位置，GPRGNN就是通过i对不同层信息的加权达到了很好的效果，说明在某些数据集上，k-hop邻域的某个k的确很重要,可以作为指导材料.
【数据集/任务】
多个任务拿下ACM-GCN,特别是在大图领域吊打ACM.
【评价】
该模型说简单也简单.首先融合x和A特征,然后根据PPR的标签传播公式列出稳态公式,再通过loss的三部分限制优化参数.
其不同是,常规稳态公式是靠提供的A进行多次卷积后稳态,然后通过参数矩阵W做AXW进行调整.GloGNN直接将A换成Z,进行全局的传播,找到一种适用于当前PPR的稳态传播的邻接矩阵Z.该传播同样可以做多层,等于是分摊了每一层的压力.
loss的作用有两个, 一个是给前面的soft-label生成网络压力,要求尽量生成符合后续工作的label,即在给定Z下传播一次仍然可以保持label不变的soft-label(但可疑的是这里并没有对该处输出做loss限制);二是让Z尽量符合k-hop A的组合形式,不要有太大改动,这大概是希望Z尽可能在k-hop中寻找相似节点,如果是全局寻找就有点没头苍蝇了.
有趣的是,Z其实可以直接求出来.在其他参数都认为固定时,最优Z是可以算出来的,因此不需要通过训练来获得最优的Z,直接靠那一长串公式Eq7和8计算Z进而获得H,所以需要训练的只有前面算soft-label的参数和k-hop A的权重λ.
让我疑惑的是,稳态公式基于我们认为的真实标签,要想它成立,必定是同质图.异质图想要变成同质图,一靠Z连接k-hop以外的点,二靠寻找固定的哪个hop有较多同质点,加强对应的λ.训练前期,soft-label必定不准,此时Z寻找的点也一定不准,或许k-hop A可以将其拴在临近点寻找,以保证前期Z的稳定性
Figure6的消融实验很值得看。可以看到，邻接矩阵、特征矩阵、本地正则化，在不同数据集中都分别担任了重要的任务。
相比于ACM-GCN,GloGNN没有用任何显式的滤波,当然也可以认为Z是一个功能强大的滤波器.除此外,x和A都用了.



53.Learning How to Propagate Messages in Graph Neural Networks KDD2021
【keywords】
L2P、learning to propagate、
【abstract】
本文的related work介绍了few-shot里用的bi-level optimization优化方法，这是我在No.48 OGNN里看到的方法，是分别交替优化模型参数W和权重参数α的方法。
该论文主要思想是认为每个节点都有它自己的聚合范围，我们可以根据X和A来推断出中心节点的最优聚合范围，这就是“学习如何传播”的中心思想。
基于此，派生出两个模型，Learning to Select(L2S)和Learning to Quit(L2Q)。可以看Figure2，前者是固定GNN深度，并通过Select函数选择其中的某个深度的输出，也可以把Select函数作为Select Score，然后当作attention去做。后者会在每一次向前GNN时计算一次Quit函数，计算出在当前层quit的概率，然后抛一次硬币。这里又出现了stick breaking断棍构造，这是一种构造迪利克雷过程的方法。
【数据集/任务】
略。因为用了bi-level optimization，所以异质图训练数据按48 32 20分割。
【评价】
21年的论文，提升不大，感觉很虚。记住方法就行。


54.GSN: A Graph Neural Network Inspired by Spring Networks
【keywords】
GSN、
【blog】
https://blog.csdn.net/qq_42103091/article/details/125143180
【abstract】
这个模型是结合了弹簧网络的东西，会把网络想象成一个边为弹簧的图。
Eq1提到的胡克定律是F=kx，F是弹簧力，x>=0是弹簧形变长度，k是弹簧劲度系数(弹簧确定时，k是常数)。所以x越大，F越大。经过我不懂的计算，得到2节点系统中两个节点分别受到的弹簧力Ep是与两节点位置的2范数相关的强度，公式表明，有一个常态距离l0，距离比l0大或小时，对两个点会产生等比的力。
作者在Eq2定义了放松时弹簧边的长度l0=Mij，那么可以根据Eq1列出Eij与Mij的关系。自然地，因为边有同质、异质性，所以我们可以认为同质的边处于拉伸状态，异质边处于压缩状态，这样放松的时候，同质聚合，异质分离，达成效果。这就是Eq2的由来。注：α=1是同质，α=-1是异质。
问题是，如何规定初始时边的能量Eij?这个能量很关键，决定了弹簧到底要推、拉多远。
作者直接定义Eij。从P4最后的高亮可见，异质时，Eij无限大；同质时，距离越远Eij越大。这符合我们的要求，Eij无限大会让异质的尽量拉开距离，同质的不会凑到一个点上，但是也会基于初始距离而尽量拉近。
对于α∈[-1,1]的规定，我们不能直接扔一个参数，会过拟合，所以必须靠数据推测。作者提供3个方法：dot concat bilinear。【想法】这个设计也忒好了！
有了α的计算公式，我们就可以计算Eij了，也可以得到Mij了。现在的关系是这样：节点间的距离推出α，α和距离一起分别推出Eij和Mij。
(作者在中间证明了E不会随着节点位置的平移、旋转、反射变换而改变，这些实际上是等距变换，不影响α和L2距离，也就不影响E和M)。
到这里就乱了，我看不懂，但我觉得是作者没写明白。
【GSN流程】
1.先常规X过一遍MLP，得降维的Z0。
2.有了Z0，再过一个MLP或denseLayer，得隐藏嵌入H，用于计算α。
3.根据Eq4，得到更新的Z。Z经过K层GSN更新。
4.最后的ZK用W输出。
【数据集/任务】
略
【评价】
这本是个好论文，但是到中后期解释得太糊涂了，看不懂了。
反正也不是什么可以扩展的论文，就略过了。


55.Graph Neural Networks Inspired by Classical Iterative Algorithms
【keywords】
TWIRLS、
【abstract】
这个论文是魏哲轩大佬的。在青源讲座里56：00讲了。以下是演讲的复述。
GNN的优化函数是要求最终邻居表示Z相近，且与标签相近。前者一般认为是GNN的正则项，不过该公式也是迪利克雷能量的公式，认为后者才是正则项。总之，我们先点明这个公式。
在公式的前者“邻居表示相近”处，可以写成Z^T*L*Z的矩阵形式，L=I-(D^-1/2)A(D^-1/2)是A的拉普拉斯正则化。
TWIRLS将这个L稍作变化，做一个滤波。定义滤波函数γ(·)，要求其值必须半正定(特征值>=0)，否则优化函数非凸，解不出来。
若γ(L)=2I-L=I+(D^-1/2)A(D^-1/2)，则从要求“邻居尽量相同”变成“邻居尽量相异，即特征符号相反”，这是一个异配的要求。（我对此不同意，Z反号不利于与X相似，会拉大第二项的值）【想法】如果这里分成“Z过softmax前相异，softmax后与X相似，则合理。”
若γ(L)=e^(tL)-I，t是温度，则其最优解长得像Heat Kernel，可以用来解释GDC和GraphHeat。
若γ(L)是恒等映射，但对优化函数加一个η(Z)的惩罚项，要求如果Z中某个节点表示的元素有负，则惩罚+∞，这就是逼着Z的最终输出必须是+，这就与ReLU的输出方式不谋而合。
若γ(L)带进了迪利克雷能量方乘，解出的最优解Z*=α(αI+(1-α)γ(L))^-1 x = h(λ)x，你可以发现x左边那一串的矩阵的特征值会在0~1之间，是一种特殊的BernNet。
若先把Z^T*L*Z的外面套一个ρ(·)=开方，再令γ=1/(2ρ(L))，则发现，邻居特征相近的loss会很大，反之则很小，模型逼迫一开始不同的节点继续不同，相同的节点也尽量不同(但是如果一开始就很相同，则没办法了)。如果再精心设计一下（论文给出了公式），设计成相似度一样“越近越大，越远越小，范围在0~1”的样子，那么γ提供了一个符合GAT要求的loss。
【IRIS和TWIRLS】
IRIS，iterative reweighted least squares，迭代重加权最小二乘法。迭代，指迭代边；重加权，指计算边提供的调整过的loss；最小二乘，指依赖2范数。
IRIS这其实是上面GAT举例的更合理改进。
作者在Table1列出了3个ρ的举例 。改的ρ主要是为了避免邻居相近时其反向传播学习率过大的情况。以第一个log为例，邻居相近时，z^2->0，此时ρ对z^2求导是1/e，否则是0，这就与GAT差不多的效果。
【数据集/任务】
略
【评价】
略。看得头大。


56.PROTOGNN: PROTOTYPE-ASSISTED MESSAGE PASSING FRAMEWORK FOR NON-HOMOPHILOUS GRAPHS || ICLR2023 reject ||
【keywords】
PROTOGNN、
【abstract】
之前忘了看哪个论文，发现对图做社区发现(聚类)后，不同社区内的同类节点的特征分布完全不同，这留下了一个扣，你的GNN网络如果强行合并这两个分布，那么只有两个糟糕的结果：(1)学到个中间值，谁都代表不了(2)学到其中一个，另一个完全放弃了。
PROTOGNN从prototypical networks汲取灵感并加上slot attention扩展，认为如果让某类的节点嵌入到该类的多个原型中的任意一个，则有许多空间供给分布不同的同类节点。
【slot attention】
这个方法来自于2020-NeurIPS: Object-Centric Learning with Slot Attention，是用来分割物体的。
一个图的特征input送入网络，随机生成K个slot，并将input和slots都映射到同一个空间里，以审查input的每个像素对不同slot的attention。理想情况下，我们希望各个slot代表图中的不同物体，input嵌入后，对应代表某个图中物体的slot的embedding与那个物体在input中所占据的像素的embedding相同。此时，WeightMean函数会按照attention加和平均input的嵌入，理想情况下，这个平均肯定就和对应的slot相等，将其加到原始slot上，相当于等比扩大，那就是没变。而如果我们还没达到理想情况，则最后这个slot更新的加和会调整slot，我们希望在T层后能得到一个相对稳定的最终slot。
神奇的是，我们的slots不是事先规定的，毕竟这实在有些硬。论文让slots从统一的Gaussian distribution采样，并让均值μ和方差Σ可学习。这是我无法理解的。除非他是想让网络不依赖初始slot。也对，T层slot更新后，slot被input完全调整，给一个带random的initial是避免over-fitting的好方法。
【PROTOGNN】
关键在slot attention，搞懂了就可以直接看Figure2了。
1.单拿出节点特征，过MLP得到嵌入。由于是训练节点，所以我们可以用真实label分类这些嵌入。假如各类的slot已知，并由μ和Σ参数控制slot的分布，我们会通过Eq3计算类C的节点Xc的嵌入对各个类C的原型的attention Mc，希望Xc至少与某一个原型相近。由于作者在计算att时softmax后面又加了个随机干扰项，所以Eq4时需要重新Norm。Eq4的作用是根据attention融合围绕在1~K个C类原型周围的节点，并融合出新的1~K个C类原型。这是一个类似于聚类的过程。
对于最初的slot，可以通过MLP(X)的初始嵌入和真实标签进行分类，然后以各类的嵌入的均值为μ，方差为Σ，随机出属于该类c的K个原型Pc。以后将不再使用μ和Σ，而是直接将原型们作为参数，按照Eq4优化。可见，MLP(X)获得合适的嵌入是构建可靠原型的基础，因此它需要被优化。
2.Eq5，用GNN采集结构特征H，其实就是普通的GNN计算。下面要用到这里生成的任意某层l所有带预测节点的Hl_nontrain
3.Eq6，Hl_nontrain(Fig2里标为Q)与所有类的K*C个原型P(Fig2标为K)经Eq6的余弦相似度计算相似度S，加权求和得到Eq7的Mproto。这是根据原型得到的待预测节点嵌入，可以作为节点H的信息的一部分，我们会将其与本层(第l层)的输出Hl融合，Eq7通过某个aggregate函数融合出新的Hl。Hl会参与到下一层GNN的计算，下一层的H(l+1)也将重复本步骤(第3步)的整套流程，生成agg后的H(l+1)。
4. loss分3部分。(1)交叉熵，把3.的结果拿出来接MLP做标签预测的loss。(2)我们希望，相较于其他类的原型，本属于类c的点i的嵌入hi能与类c的K个原型的某一个最相近。因此，3的结果接另一个MLP，再与所有的类的K个原型P计算余弦相似度，并看看各类中最相似的原型是哪个，sic代表节点i的c类中与原型的最大相似度。我们当然希望真正的类得到所有类的sic中最大的sic，于是在Eq9的loss中，可以看到，正样本sici越大，loss越大，负样本sic'越小，loss越大，整体我们希望loss变大。 (3)Eq10要求各个原型最好正交。
【数据集/任务】
略
【评价】
slot attention是很重要的技术，结合原型网络的方法真的很nice。
作者将原型网络的原型与GNN的节点嵌入做余弦相似度，并以此融合原型表示，生成由原型而来的节点嵌入，再综合GNN嵌入和原型嵌入获得新的嵌入，作为下一层GNN的输入。
MLP和GNN的嵌入固然不同，MLP的肯定更差。作者用MLP是希望仅仅通过原始表示做坐标变换后更方便地找到各类的小的聚类，并且要求隔壁GNN的H也能遵循它的坐标变换规律，当GNN聚类邻居后，节点表示将与它融合的那些节点所属的原型相近，结合原型P的attention加权求和，暂时不知道这个原型而来的新嵌入对GNN有什么作用。

57.PBGAN: Path Based Graph Attention Network for Heterophily ICPR2022
【keywords】
PBGAN、
【abstract】
肯定有一个类似的论文我看过，应该是struc2vec。但是这个论文给它做了加强。
模型内容在Eq4~10.
1.先确定你要计算的范围k-hop子图。然后计算中心节点到范围内邻居的最短路径。
2.Eq4~5要在X基础上增加一些特征。Eq4是从中心节点i按照random walk with restart(RWR)的方式在子图上行走，最终到达各个节点的概率Di。ei是向量，维度与子图节点数量一致。因为是子图，所以虽然解析解Di有求逆，但不难。Eq5除了Di还增加了Vi的度，这个指标在很多论文里也很重要，加上。(估计是做实验随手加上发现有用就留着了)
3.Vi现在要在子图上做att了。之前找到了所有最短路径，现在我说要算aij，找到路径path_ij，用Eq6一个简单的RNN计算，再用Eq7简单得到aij，这都是常规做法。注意，可以简化计算，因为你在这条路径上经过的所有点的最短路径也是这条路径的一部分，所以对这一路上的节点只要得到RNN的对应输出就行，别重复计算。
4.最后加权求和就没什么好说的了，Eq8最简单，Eq9加上了multi-head attention，Eq10是最后的readout。
【数据集/任务】
【评价】
怎么说呢，不好说，但是想法超越了struc2vec，从同配跨越到了异配。



58.LOW-RANK GRAPH NEURAL NETWORKS INSPIRED BY THE WEAK-BALANCE THEORY IN SOCIAL NETWORKS ICLR2023
【keywords】
LRGNN、
【code】
https://anonymous.4open.science/r/lrgnn-4551/
【abstract】
sign social network，SSN，符号社交网络，即边有带符号的权重，例如边代表两个人有关系，+代表友谊，-代表敌意。这个就很和同质、异质相似。
弱平衡理论：是一种实用理论，是结构平衡理论的改进，消除了“敌人的敌人就是朋友”的模式，保持了“朋友的敌人就是敌人”、“朋友的朋友就是朋友”、“敌人的朋友就是敌人”。
弱平衡理论会导致低秩矩阵：因为弱平衡理论是在给邻接矩阵加边，加的越多信息越坍缩(信息量变少)，即“本来邻居的邻居我不知道是什么，但是根据理论我能确定”，信息量变少=一个矩阵的秩变低，几个维度就能表示所有节点的情况。
剩下的就是我看不懂的了，也不用看懂，看了也没用。
【数据集/任务】
略
【评价】
据Openreview评论，该论文与GloGNN相似性大，不同在于能得到低秩解。除此外没了。
不需要看，把GloGNN和CPGNN看了就行。
文章被拒了。


59.NCGNN: Node-Level Capsule Graph Neural Network for Semisupervised Classification IEEE2022
【abstract】
略。胶囊网络相关的我是一眼不看。


60.New Benchmarks for Learning on Non-Homophilous Graphs WWW2021
【keywords】
class-wise homophily、新的数据集、
【code】
https://github.com/CUAI/Non-Homophily-Benchmarks.
【abstract】
开篇批评了9大数据集的问题。
提到了一个基于类不平衡现象提出的class-wise度量同质性的方法Eq2~3。
提到了调整数据集facebook100、Pokec、snap-patents、arxiv-year、snap-patents
【评价】
可算知道56引用是哪来的了，这个论文里来的啊。



61.Label-Wise Graph Convolutional Network for Heterophilic Graphs PMLR2022
【keywords】
LW-GCN、
【code】
https://github.com/EnyanDai/LWGCN)
【abstract】
论文2021年发了一版没上，新版改了个题目改了内容上了PMLR。
【3 理论部分】
这里论证了度对图效果的影响，需要仔细观看。结论是，度越小，效果越差。
首先讨论异质图假设：（1）d正则化图，即每个节点有d个邻居、每个类的节点邻域分布相同。（2）不同类的节点，其异质邻居总是特征分布不同、特征维度互相独立。
P4最上方高亮要做一下解释。μik代表class i 周围某一个class k 的节点特征均值，显然是个向量。μi=Σμik/C是对class i 周围环境的类特征均值的均值。显然，如果μik-μi很大，则μik很有特点，与其他class容易分开。顺推，σi计算的是μik对于k的标准差，σi越大越说明各个k的μik差异大，代表class i周围的类的节点特征是否足够鲜明。所以σi反而越大越好。
Theorem1：
对于两个类i和j，我们要求（1）ij各自的邻域同质特征要有区分度，至少要比其他类k区分明显，即我们希望不同类ij的其他邻域类分布k的平均特征μik、μjk相似，且（2）σi>σii，即类i的邻域类i分布要足够稳定。这俩σ其实根本不搭嘎，前一个算的是μ的标准差，后一个是类内μ的标准差。
当满足(1)(2)时，随着同质率h的减小，通过平均加和的GCN层(Z=D^-1 AX)获得的邻域表示的可判别性(即不同领域的区分程度)，会先减小到h=1/C(C是label类型量)再触底反弹。当h=1/C且度d<高亮值时，邻域聚合表示就基本无法判别了。
Theorem1评价：h=1/C是一个认为连接真随机时的同质率。其实刨除那些数学限制，直观来看，想象两个场景：(1)同质率>1/C，度大，此时度可以充分展示同质率下的非随机连接关系，因此任何节点在“连接最多的节点是同类节点”这个特征上特别明显。(2)同质率=1/C、d<高亮值时，那就是纯随机连接了，此时邻域提供不了任何帮助，自然是最低，且d因为太低，因此无法为中心节点展示一个“纯随机连接”的特征，所以邻域方差巨大，明明是同类节点，却因邻域的方差大而导致GCN后自身特征的方差巨大。这就很好解释为什么这时候的性能最低、表示最无法区分了。
【证明：Appendix C】
由于论文之前给了一个理想的异质图假设(对我来说过于理想了)，所以以下的证明比较顺利。
以下基于h>1/C条件。
Eq12给出一次GCN的邻域聚合信息zv，是简单的平均加和。
Eq13求不同中心类下邻域聚合zv的期望Ei
=E(zv|yv=i)，利用同质率h计算，很简单的公式。
Eq14对Ei和Ej计算1范数Δij，如果Δij大，则说明一层GCN下，不同类的表示能区分，反之反之。
Eq15~16分别是h>1/C时Δij的上下界，公式显示，随着h下降、|μii-μjj|>|μik-μjk|满足时，上下界会同步减小。即越倾向随机连接，异类节点越不容易区分。
Eq17~21计算了邻居特征聚合的方差，Eq21显示，在σi>σii时，h的系数会变负，所以方差下界会随着h减小而增大。
以上两个结果说明，h以1/C这个纯随机连接的同质率为界，这个界是性能最低的时候。
以下基于h<1/C条件
Eq22~23证明在|μii-μjj|>|μik-μjk|满足时，Δij随着h下降而增加。节点特征逐渐可分。
Eq24证明，在σi>σii时，方差随着h继续减小而减小。
【数据集/任务】
【评价】
又一个比较好证明了邻域与同质率对节点分类效果影响分析的论文。其模型倒不是很重要。



62.THE IMPACT OF NEIGHBORHOOD DISTRIBUTION IN GRAPH CONVOLUTIONAL NETWORKS ICLR2023
【keywords】
【abstract】
直接说结论。
作者研究了节点邻域的特征和拓扑，发现GCN的性能不是纯粹由同质性决定，而是要看图的结构是否足够满足邻域分布的可分辨性。
这个结论与No.61的一点很符合，即度d越小，性能越差，即图结构无法满足邻域分布可分辨。
【GCN-PND】
直接看模型。这个模型高度借鉴U-GCN和LAGCN(这个论文没看呢)。
1.Eq8是一个常见的拆分，对自环和邻居权重做超参数α的调整。仅做一次聚合，获得1-hop信息。
2.Eq9 计算knn形成的拓扑Sknn，通过门槛值ρ修剪正则化后的原拓扑A，得AD。(奇葩的是作者说异质图设ρ=1，这不就全删了吗？？？)。然后AC=AD+λSknn，λ超参数。
3.Eq10 softmax后平均正则化AC。
4.Eq11 循环K次，K是希望的邻居hop。利用的拓扑是AC，且没有W映射。
5.Eq12 比较麻烦。对于Eq11得到的Z0~ZK，共K+1个表示。我们希望，每个节点都能独立地从K+1个表示中学到聚合权重，因此对于节点i，对于Zk，设wik是节点i对zik所给的权重。Eq12通过i专属的学习参数ui对zik降维。
【数据集/任务】
略，效果不错，但不能说提升多高。
【评价】
总体思路遵从U-GCN，后面的信息聚合权重计算很花算力，而且超参数ρ的存在不能让模型自适应同质、异质图，超参太多，总有一个效果好的。
模型是多余的，有用的是对邻域的理论研究，占了8页。

63.Local Augmentation for Graph Neural Networks ICML2022
【keywords】
LA-GNN、LAGNN、
【abstract】
忘了变分推断、ELBO是什么，就看以下blog
https://blog.csdn.net/ltz0120/article/details/111032478
作者训练一个CVAE(conditional VAE，条件VAE)，从名字可以看出，该CVAE可以根据给定条件调整输出，即给一个X输出一个Y，而不是给一个X还原出一个X。
在该论文里，给的是(Xv,Xu)对，给一个Xv，给出一个Xu。
有了CVAE，作者将其描述为“局部增强”、“预训练模型”，并将生成的向量用于接下来的计算。
Eq6显示了LAGCN在CVAE后的操作。可以看出，作者直接让CVAE生成的东西当作中心节点v的一个特征了。这是有道理的，它是来自邻居的节点特征，用邻居描述了自己。
【数据集/任务】
肯定有提升，不用看的。
【评价】
（1）用CVAE收集新的节点特征是一个很好的创新点，虽然不知道为什么可以。（2）这个结构可以与任何模型结合。（3）CVAE可以看作是图的预训练模型一样的东西(这一点我极度不赞同)。



64.Revisiting Homophily Ratio: A Relation-Aware Graph Neural Network for Homophily and Heterophily MDPI2023
【abstract】
【数据集/任务】
【评价】
扫了一眼，创新内容不足，baseline对比不足，不看。


65.NODE CLASSIFICATION BEYOND HOMOPHILY: TOWARDS A GENERAL SOLUTION || ICLR2023 reject ||
【keywords】
ALT-global、ALT-local、
【abstract】
【ALT-global】
基本思路是，根据对偶滤波器(如figure1所示)和偏置，我们可以结合三者得出低通、高通、全通的信号，并通过ω调整低通/高通比例，通过η调整全通量。
【ALT-global 分析】
作者说他们并没有对全通采用简单的MLP，消融实验中MLP效果不好。
称所有先用MLP处理节点特征，再采用GNNs的模型，为Graph-Augmented Multi-layer Perceptron（GA-MLP）。
作者再Lemma1证明了，不管主体GNNs是何种，它自带一个滤波器，我们再加上对偶滤波和偏置，能做到自适应滤波。是很简单的证明，证明了只需要对A做ω的调整，就可以自适应滤波。(然而，它并不是任何滤波器都可以造出来，只是低通高通的简单自适应调整罢了)
然而然而！这点工作还不至于上ICLR。ALT-global的局限在于，它还是一个对所有节点统一一个滤波器的模型。因此有了local版本、
【ALT-local】
我们总说GNN的Lx=UTΦUx，Φ是对角矩阵。如果Φ除了对角外也有非0元素呢？它们起到什么作用？
Proposition 1 证明了，它们的作用是针对某唯一节点的“调制”。
这就给了local化的思路。
Eq3a~3d是ALT-local的公式，看起来和global一样，但是ω换成了W.
W不能是硬设的，否则会过拟合，所以W从Eq4a~4b来，是一个很简单的公式。但是Eq4b的公式可能有性能问题，它要计算的MLP次数是nodes*nodes，很费劲。Eq4a用的是GNN，说明是要衡量两个节点和其邻居的信息来判断两个节点的相似程度，进而作为边的权重。
有趣的是，Eq4a的GNN不普通，它被设定为一个2层高通GNN，公式为Eq5a~5b。作者解释为高通更有判别性
(到这里我已经感觉到有些硬堆方法了，没有解释，有些不舒服)
【数据集/任务】
ALT作为一个扩展性质的外围框架，对各个模型肯定是大部分都有提升的。
【评价】
难以评价。到local那里就有些解释不通了。停在global那里挺好的，不用太多参数就能自适应，相比于FAGCN的优势是有个偏置，也就仅此而已了。local有些硬堆，让人不舒服。

66.Simplifying approach to Node Classification in Graph Neural Networks ||  Journal of Computational Science2022 || 18
【keywords】
FSGNN、
【code】
https://github.com/sunilkmaurya/FSGNN/blob/main/model.py
【abstract】
这个是No.50的修改版，增加了一点实验和分析。
FSGNN是一个综合K-hop邻居信息的模型，通过原始信息、无环k-hop传播、有环k-hop传播，得到共2K+1个信息。
在Table 3 中，FSGNN(Homo/Hetero)代表：对于homo，只采用原始+有环信息；对于Hetero，只采用原始+无环信息。FSGNN(ALL)则是不论数据集，统统采用三部分数据。
疑惑的是，这里不遮掩了，直接把hop-normalization叫做L2 norm，也没有了algorithm 1的算法过程总结.
【评价】
亮点是有环+无环、正则化。

67.Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods || NeruIPS2021 ||
【keywords】
LINKX、
【abstract】
这个论文是No.40 GLINKX的前身，其优点是通过特征工程手段，极大压缩了计算难度，避免了图卷积操作，并获得了不错的性能，因此可以在大型图上做计算。
模型看Figure1就行，公式在Eq6~8.
对于A作为数据的一部分，需要说明原因。
W∈(nodes,class)，softmax(AW)做的事可以说是在虚空地标记wij，如果节点i对于判断任意节点是否为j类很有用，则调大wij。w[:,j]可以看作是对类j的一次全图标记，高亮图中所有利于判断类j的节点。此时，经过A的传播，如果某一节点的周围总是有类j的高亮节点，那么该节点大概率是类j。
LINKX将softmax(AW)解释为是一种2hop方法，这是将W的高亮行为认定为“非虚空标记”，认为它的标记也是看节点邻居是否是类j或利于判断类j，才导致的高亮行为。这是一种误判，还是那回事，W纯粹是虚空标记，没有任何依靠，为了解释W的行为，LINKX强行将其解释为了一种依靠同质性的高亮标记，是不合理的。
【数据集/任务】
【评价】
看起来这个才是第一个用A+X的模型，而不是GloGNN。




68.CAN SINGLE-PASS CONTRASTIVE LEARNING WORK FOR BOTH HOMOPHILIC AND HETEROPHILIC GRAPH? ICLR2023 拒稿
【keywords】
SP-GCL、对比学习、
【openreview】
https://openreview.net/forum?id=XE0cIoi-sZ1
拒稿
pros：实验证实了方法优秀
cons：理论与方法有距离，对异质图上的应用没有说明，重要设计没有解释。
【abstract】
【4.1聚合特征分析】
Eq1假设了特征xi的由来，标签yi会控制一个随机生成的向量+一个只针对i的偏置。因此，如果看xi的分布，它是一个由yi决定的高斯分布。除此外，作者假设邻居们的分布只与yi有关，这个假设是常用且合理的。
Lemma1说明了几件事：(1)节点嵌入Zi与其期望相近(2)节点内积与其期望内积相近(3)相同标签y的节点倾向于集中嵌入到某一区域(4)对参数W的常用初始化(kaiming、Lecun)，W相对紧。
【4.2单通道图对比损失】
主要介绍怎么采样、loss公式。来源不明，而且这loss能保证>0吗？
【4.3线性分类器的性能保证】
证明了同类节点嵌入总是内积更大，而且若嵌入后邻接节点更加相似，则高维嵌入会更加紧凑。
【SP-GCL】
模型很简单，看Algorithm1.
1.过GNN，得表示H。
2.过MLP，得表示Z，这里要有L2正则化。
3.随机采样b个点作为种子集合S，再对每个种子收集它们的T-hop邻居，凑成邻居集合P。这是针对大型图的采样。
4.对每个种子b，从P中找到相似度最大的K个成为Spos，再从整个节点集V里找Sneg做反例。
5.用Eq5做对比学习，更新参数。Eq9是Eq5的无偏估计，对大型图适用。
【数据集/任务】
略
【评价】
只看实验的话，效果很好。
但是看过程就一团乱麻。Eq1的yi为什么是-1和1，xi为什么是Eq1的样子？正负例采样为什么不一样？为什么正例在邻居T里选？负例随便选俩就行？
现在知道的是，在Eq1的xi公式和邻域仅由中心标签决定的两个假设下，该论文证明该模型有用。
可是我看到的是一个强烈依靠初始嵌入、期望特征与标签强挂钩、要求近的更近远的更远的模型。
麻了。当作没看过吧。这垃圾论文真是佛了。

69.Feature selection: Key to enhance node classification with graph neural networks || CAAI2022 || 1
【abstract】
这个论文对我有启发性。借由No.50的FSGNN的做法，作者发现X、AX、(A+I)X、A^2X、(A+I)^2X、A^3X、(A+I)^3X这7个特征的综合学习可以有很好的效果，而选择其中使得性能最好的几个，更是让性能有大幅提升(Table1)。
所以如何设计选择特征的下游模型就很重要了。
作者选择bi-level loss，称为Dual-Net GNN Architecture。
【4.1 model description】
特征网络上面已经给出了。下面给出分类网络和选择网络。
【4.1.1分类网络】
用于最终的节点分类，用简单的2层MLP，参数为θ，函数以fc表示，m代表选择网络选出的特征，X是必须的节点初始特征。
具体来说，会对m中包含的几个特征做独立的线性变换，然后加和，再映射到最后的标签空间。
【4.1.2 选择网络】
它也是两层MLP，参数为φ，函数以fs表示。
fs的输入是一个2K+1维度的one-hot vector，给出不同的one-hot，代表选择不同的m、不同的特征组合。要求fs输出的值为标量，代表该组合的能量，即影射出模型性能。
【4.2 loss】
fc和fs都有loss，都接收参数X，m，各自的参数θ和φ。
无论在哪个m下，都要求Lc尽量小。
对于Ls，我们知道，那个性能最优的m对应的Lc总是最小，我们希望fs能预测出这个Lc，因此将m这个选择集合用one-hot向量表示。
【Dual-Net GNN】
#第一阶段stage1，循环E1次
1 从待选特征中随机选个子集。
2 拿这个子集去训练分类网络，得到Lc。
3 拿m和Lc去训练选择网络。
#第二阶段stage2
1 m不随机，而是均匀设置
2 我们知道，如果一个输入维度很重要，那么其产生的梯度就会很大，因为它一变，输出就变化很大。作者利用这个特性，结合可以较好预测Lc的简单的选择函数fs，在fs上对均匀的m的每个维度计算梯度，找到梯度最大的top-p个，作为性能最好的。这是一个武断的判断，我们只是武断地认为p内的特征包含了导致最优性能的特征们，因此还要在p范围内重新执行stage1去优化选择网络和分类网络。
#第三阶段stage3
1.此时范围已经缩的够小了，我们可以武断地认为使得Lc最小的那个对应的m*是最优组合。
2.针对这个最优的唯一m*，再去优化一下分类网络，优化Lc。
【实验】
实验结果还挺好，异质图效果很好，同质图性能不差。
【评价】
这更像是一个工程用法，筛选特征用的。虽然模型难看、傻瓜，但是简单、好用。
【想法】
必须有一个更好的筛选特征的方法，它这种只是权宜之计。不过的确是我看到的唯一一个有筛特征想法的论文。


70.A Generalization of Transformer Networks to Graphs || AAAI2021 || 300+
【keywords】
GTN、
【blog】
https://zhuanlan.zhihu.com/p/365129455
【abstract】
【1 开篇】
1.1作者认为全局信息聚合虽然抛弃了稀疏性和局部上下文，但是是一个很好的归纳。作者在希望全局聚合的同时，不想放弃稀疏性和局部上下文。
作者例举了Graph-Bert提出的特定于图的位置特征、2003年的技术——节点拉普拉斯位置向量、Leskovec的position-aware相对可学习的位置信息、门控图序列神经网络的虚拟节点等技术。
论文主要围绕Graph-Bert做讨论。 细数GTN、HGT等模型对位置的编码，发现这些实验的结论是，位置嵌入的性能提升并不普遍，只对一些无监督学习的任务有作用。
总之，Transformer可以以更有效的方式使用，主要贡献：
1.将transformer推广到任意同质图，并提出有结构特征输入的Graph transformer。。
2.用图数据的拉普拉斯特征向量融合节点位置特征，这个想法来自于前人的研究。我不懂这个，所以要细看。有证据表明，在任意同质图上，拉普拉斯的位置特征比其他编码方式要好。
3.这一点就不算贡献了，说性能优于baseline。你不优于你怎么办啊，不优于你论文都发不出来。
【2 分析技术】
【2.1 图稀疏性】
在NLP任务里使用Transformer，里面的Attention会让单词互相关注，这是为了扩大每个单词的感受野，以抵抗有限上下文的稀少信息。然而，在graph里有丰富的信息，所以不太需要全局或扩大太多的邻域。(这个想法不对，图抛掉图结构本质还是一堆特征遵循分布的节点，全局结构信息不重要，但是全局衡量下的特征信息很重要)
【2.2 位置编码】
NLP的transformer会按照已知上下文相对于中心的距离设置位置编码，它很重要。
graph不好设计位置编码，因为其邻居的位置等价，即对称性。
所谓拉普拉斯位置编码，其实是正则化后的对称邻接矩阵I-DAD的第i行当作节点i的位置编码，用它直接加到vi的特征xi上。我本以为这个方法是GloGNN自创的，结果早就有^^。
【2.3 模型结构】
Figure1.分俩版本，看你的边有没有特征。
以一层为例：
1.Eq2是对节点信息和边信息做线性变换。
2.线性变换拉普拉斯特征表示λi，并直接加入节点表示。注，这一步只在input时操作，以后就不加了。
3.这里是普通的transformer，看Figure1图就行。
【评价】
网络评价为水论文重灾区。


71.GRAPH-BERT: Only Attention is Needed for Learning Graph Representations || 160+
【abstract】
这个模型不采用GNN卷积。强烈依靠高同质率。
Figure1可以看懂模型方法。
1.采样子图，是全局计算亲密度S后的top-k节点的induced graph。亲密度采用PageRank的方法Eq1，Sij就是i对j的亲密度。让人好奇在异质图他要怎么办。
2.其特征分为4部分:(1)中心节点和周围节点；(2)WL方法的绝对位置嵌入(Eq3，一个hash函数)；(3)基于相似度的邻居相对位置嵌入，与(2)的不同在于(3)会把邻居的位置表示按相似度排序；(4)基于hop的相对位置嵌入，会记录各个邻居到中心的距离，以此为嵌入。其中，(2)是全局嵌入，(3)是局部嵌入，(4)可以看作全局的局部嵌入，是前两者的平衡。
3.2的这些表示节点的嵌入，在该论文里采用“加和”的方式融合到一起。抽样子图里的每个点都有上述加和嵌入，假设有k个邻居，那么加上自己，k+1个节点的嵌入会组成一个矩阵，称为H0，会送入Eq7中计算，这是一个QKV的l层attention。
4.最终表示会如Eq9的最后一个式子一样，经Fusion变为正式嵌入。论文里Fusion是把k+1个表示平均加和了，得到Z。
5.作为一个pre-training model，作者设计任务为：1.将Z经过MLP或其他方法后获得原始特征X，其loss为Eq10。2.将Z做内积，得到节点之间的相似度，正则化后，要求结果=1中计算的亲密度。这俩任务要求Z包含原始节点信息，也包含原始结构信息。
【评价】
看看可以。


72.Understanding non-linearity in graph neural networks from the bayesian inference || NeurIPS2022 || 10
【abstract】
论文利用贝叶斯推断得出结论：在节点特征信息远大于结构信息时，ReLU效果最好。
【评价】
纯理论研究论文，不要看，知道结果就行。

73.BOOST THEN CONVOLVE: GRADIENT BOOSTING MEETS GRAPH NEURAL NETWORKS || ICLR2021 accept || 33
【keywords】
BGNN、
【blog】
https://zhuanlan.zhihu.com/p/386000261
【abstract】
这是一个和AdaGCN一个思路的模型，但是使用GBDT做boosting。
回忆AdaGCN，它是一个L层结构，每一层的参数都继承于上一层，但是输入的A的范围会越来越深，并与Adaboost一样，动态调整节点的loss权重。可以说，现在看来，这是一个没有什么道理的做法。
BGNN结合的是GBDT，GBDT是不断使用decision tree，每分一次，就把那些分错了的做权重加强，然后再分一次。
过程直接看Algorithm1：
#训练第1个GBDT树
1.根据Eq3找到一棵使得loss最小的决策树fi=h。
2.因为GBDT模型f是一个由多个树的结果加和合成的模型，因此Eq2会将新的判决h加到老模型f上，因而形成新的GBDT模型f。
#训练l步GNN
4.X'=f(X)其实是给了一个软标签
5.基于X'，做GNN_θ，找到一个参数θ和X'，θ使得模型更优。
6.新的X'-f(X)会成为一个残差，这个残差会在下一个循环作为输入X，进而找到下一个GBDT树。因此这里回到1.
注：如果先做完k个GBDT树，再把结果X'扔进GNN，就会退化成Res-GNN。现在是GBDT与GNN交替前进。
【评价】
新颖的方法，因为新颖所以过了ICLR。

74.COMBINING LABEL PROPAGATION AND SIMPLE MODELS OUT-PERFORMS GRAPH NEURAL NETWORKS || ICLR2021 || 205
【keywords】
Autoscale、Diff-scale、OGB、GNN-free、
【blog】
https://zhuanlan.zhihu.com/p/558630840
【abstract】
这个论文的强大在于它用极少的参数、极快的速度，仅靠标签传播，在半监督直推式学习图节点分类任务中达到接近或超越SOTA的效果。
模型架构分两部分，1先做简单的MLP分类，2做误差修正和预测平滑，得到最终分类。图结构数据仅用在2和1的特征增强中。
【1 基础的简单预测】
用MLP结合位置特征做粗浅的分类。
【2 用残差传播做误差修正】
训练节点带标签，那么1就有一些标签残差。作者认为这个残差可以通过传播手段预测出待预测节点的残差，传播后得到的残差和软标签结合可以做一些预测的修正。
传播方法在论文高亮里，这是Zhou等人2004年的标签传播方法。Eq3可以证明这个误差传播公式的2范数的左边总是比右边小，且在我们的初始设置下，迭代后的误差的2范数总是偏小（这很合理，把误差平摊了后，2范数必然减小）。因此，传播后的误差总是整体有偏(节点上或大或小)，不能与真实误差贴合。
作者给出两个对E的放缩方法：
2.1.autoscale：思路是对E单位化。
2.2.scaled fixed diffusion：我们其实知道一些节点的真实误差值，那我们用消息传播的方式，固定已知label的E，不断传播，直到稳定。可以理解为消息源是固定的。
2.3 平滑操作
因为这个论文挺早的，所以还停留在同质那块，因此自然地，将用E误差调整过的Z平滑符合当时的研究。
Z是软标签，我们将其中已知的标签按真实的使用，变成G，然后同上一样地做传播，直到G收敛。
【实验】
几个数据集估计都是同质图，性能都很好，与SOTA你追我赶。
【评价】
纵观整个模型，就只有最初的软标签那里需要参数，后面都是单纯的计算和传播。怪不得能发论文，太nb了。

75.Robust Optimization as Data Augmentation for Large-scale Graphs || CVPR2022 || 22
【keywords】
FLAG、
【blog】
https://zhuanlan.zhihu.com/p/527957286
【abstract】
略看。是一个基于训练过程中对节点特征梯度造成扰动而形成数据增强的方法。核心方法叫Projected Gradient Descent，PGD，其公式很简单，Eq3~4，核心是对一个x加扰动δ，这个δ会根据训练时间更新自己，具体就是用loss对δ做梯度计算，更新δ到新的δ作为下一个batch的新扰动。所以说这扰动是在对x进行微调，找到一个使其更好分类的微型变化。
直接应用PGD可能太慢，所以作者使用了multi-sale augmentation，其实就是工程上的训练调整。然而这种调整造成了模型参数θ的次优，这有些像强化学习里off-policy+并行导致的采样与实际分布不符的情况。
对其进行改进，累积多次loss对θ的导数，一起更新，可以有效减轻上述问题。此时的方法称为FLAG。
FLAG与PGD对比，发现PGD的扰动总是集中在某个固定值周围，FLAG比较均匀，近的远的都有，这个比较符合实际。
【实验】
对GCN、GraphSAGE、GAT、DeeperGCN用FLAG加强，在大数据集ogbn- produces proteins arxiv ddi collab上，都有一定提高。
【评价】
这个论文为什么在CVPR上？CVPR是CV的会议。。。据说该论文时间很长了，被其他会议拒绝了一圈。

76.Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters ||  ||
【abstract】
【实验】
【评价】


77.Learning with local and global consistency || 2003
【keywords】
标签传播、
【blog】
https://zhuanlan.zhihu.com/p/387725992?utm_id=0
【abstract】
该论文推导出了标签传播的计算公式。
1.首先，对于G=(V，E)，通过一个高斯核来计算有拓扑连接的相邻节点的距离矩阵W。这个距离因为是高斯核函数，所以统统>0、<1。
2.再通过S=D-1/2WD-1/2，为W正则化。
3.设定标签传播时，每次只传播α部分的标签，因此设计传播公式为F(t+1)=αSF(t)+(1-α)Y，其中F(0)=Y。在该传播公式F(t+1)稳定后，则认定节点标签中概率最大的那一个就是节点标签。
4.将该传播公式展开成F(t)和F(0)=Y的公式，可得到Eq1。已知S所有元素在01之间，因此S特征值也是01之间，又知0<α<1，因此可以求得展开公式的一些特点：第一项为0；第二项是一个极限为0的无穷矩阵序列求和，可以简化。
简化后F(t)=(1-α)(I-αS)^-1 * Y，这个F(t)是t->∞的值。因为1-α对所有元素放缩，所以对最终选定label无影响，所以可以去掉。
【评价】
早期标签传播nb模型。

78.Graph Inductive Biases in Transformers without Message Passing
【abstract】
是图分类任务，不看。
【实验】
【评价】


79.Self-attention Dual Embedding for Graphs with Heterophily || 2023 ||
【keywords】
SADE-GCN、
【abstract】
这个论文还没有发表，暂时在arxiv上。用self-attention控制X和A的融合，是我昨天刚想出来的，就被人做了。只是一个很小的改进，但是看样子有很好的效果，且没有过多改进，就很可疑。不过可以学学一个简单改进是怎么能水出19页论文的。
！！！attention用的是QKV的self-attention。公式在Eq2~3.
参照Fig1，a是整体结构，A和X分别送入SADE-GCN的L层SAGC，并在最后进行att融合。注意到，这个过程中X和A的embedding没有一点交流，只有在最后才合到一起。
3.1.1点出我在toy code里发现的现象，MLP在许多数据集上远超LINK，也就是说X和A各有优势，A不是简单的X的补充或者平级。必须依照不同的图给不同的融合策略。
我复现了该论文的代码，还没来得及测试，其中涉及超参数众多，不好调。
【实验】
异质图上全面领先GloGNN和ACM-GCN+。讲道理，不知道为什么GloGNN++在自己论文上比ACM-GCN好，在第三方实验中比ACM-GCN+弱，特别是squirrel和chameleon。
【评价】
鉴定为偷我的想法。但是他的结构设计很不符合想法，QK^T作为A的权重结合V做传播？谁教你这么设计的，为什么这么搞？没有解释。
不管怎么样，他将X和A都作为节点的特征并分别在QK调整过的A上传播的确很有意思，每一步都高通滤波。他的解释很少，但是实验结果够好。

80.To Join or Not to Join: The Illusion of Privacy in Social
Networks with Mixed Public and Private User Profiles
【keywords】
LINK、
【评价】
不看。

81.Edge Directionality Improves Learning on Heterophilic Graphs ||  ||
【keywords】
Dir-GNN、
【abstract】
GNN之所以用无向图，是为了在早期使用谱图理论时需要保证A的非奇异性。然而，MPNN和其他论文都在非对称传播上有很大的性能提高，加上以前总在同质图上计算，有向无向并不太影响同质图效果，所以有向转无向的规矩被默认使用了。
现在要打破这个束缚，讨论最原始的有向图模型，以实现异质图的突破。
【适合有向图的同质性度量】
如果有向转无向，那么每个连通的节点都有得计算。但如果是有向，那么许多节点只有入度or出度，没法统一衡量。而且简单地用一个数字代表复杂的有向图的同质性，很粗糙。
作者选择用H=C*C的类兼容矩阵。hkl是H的k行l列元素，其大小代表了k类为头、l类为尾的边数量在所有以k类为头的边中所占的比例。
【加权的同质性度量】
如Eq3所示，统计有向带权邻接矩阵S=(N*N)的数据，sij是节点i到j的概率，概率大概是1/i的出度，或者数据集自带的概率。统计i的出度的概率和作为分母，统计出度与i的类相同的概率为分子，得到节点i独有的同质率。对所有节点做同质率计算，然后加和平均，就是带权的同质性度量。
基于S，我们可以更加细化前文的H=(C*C)，把统计从边权重相同的统计变成不同的。hkl即k行l列仍然代表k类链接l类的概率，Eq4可见，统计所有节点i为k类的出度权重之和，再统计i为k类，j为l类的出度权重之和，二者相除为hkl的值。
如果把有向图的入度当成出度，出度当成入度，则是论文里S=A和S=AT的意思。两种情况下的h不同。2hop，即无向图的S=A^2、有向图的S=AA^T，也可以计算同质率。一个看法，如果从1-hop到2-hop，同质率增加了，那么或许从2-hop处聚合信息是合理的选择。
从Table1可见，改成有向图后，同质率多少是有点增加的，而且异质图的2hop同质率（ATA和AAT）大幅提高。
Figure2可见，(a)图标记了许多数据集的有向、无向同质率，变成有向后总是有更高的同质率。(b)图是融合dir方法和GraphSAGE应对大图挑战的Dir-SAGE的模型训练，α=0代表所有有向边为原版，α=1代表有向边方向相反，α=0.5代表同时使用正反方向的信息。结果发现，validation acc在使用双向信息的时候效果最好，且是大大提高。
【DirGNN】
所以，这个模型就是同时对有向图的正反两个方向分别做卷积再融合嵌入的过程。Eq6形式很简单。
作者在Eq7改造了传统的MPNN。传统MPNN依靠无向的S=A并附上对称拉普拉斯变换。然而A本来是有向的，同样分为入和出两个方向，得到S→和S←。作者计算所有结点的入度D→和出度D←，作为对称拉普拉斯变换时左右D的材料，导致(S→)ij=aij/sqrt(di→dj←)。于是，MPNN的一次卷积被分成两次，一次入一次出，有了Eq7.
作者还在接下来计算了2-layer的Dir-GNN展开后的公式。
【实验】
在大数据集上的实验比较多，对基础模型GCN、SAGE、GAT都有提高，特别是Dir-GCN在chameleon和squirrel上的性能超过了ACM-GCN。
【评价】
好，善。但是做的实验太少了，九大数据集没测试完。


82.NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification || NeurIPS2022 || 29
【keywords】
NODEFormer、
【code】
https://github.com/qitianwu/NodeFormer.
【abstract】
GNN最近一年一直关注over-squashing的问题。over-squashing就是说一个1-hop邻居的2-hop邻居太多，在聚合的时候就会过度压缩2-hop邻居的信息到1-hop上，导致中心接收1-hop信息时，1-hop的2-hop邻居有效信息无法以较大权重获得。
一个可行的方法是：增加拓扑。如果那些2-hop邻居除了那个唯一的1-hop邻居通路外，还有其他路径通向中心，则有更多机会获得有用的2-hop邻居。可惜这种方法因为关于2-hop邻居，所以一定涉及2次复杂度，在大图里很难有较好效果。
NODEFormer是一种新的MPNN方案，可以在任意节点之间传递节点信号。它利用核化的Gumbel-Softmax算子达到这一效果。
【核化消息传递】
作者定义了一个全图的attention网络。通过Eq2，可以计算任意中心节点u和全图其他节点v的α得分，其余v将以特征zv为基础，与zu比对，并得出得分。u会以softmax的形式计算各个v的att占比。
这个公式在大图上会失效，大图的节点数N太大，这个公式又是个全图公式，softmax很可能抹平所有节点的attention，导致没有区别。
作者做出的改变在Eq3。作者发现，WQzu和WKzv的点乘就是余弦相似度，它可以被其他互相似度公式K代替。于是，结合了attention和消息聚合的公式就是Eq3。具体选哪一种K还没有说，但是所有核方法一定是可行的。
【Positive Random Features(PRF)】
这是作者选用的核函数。在优化角度，它并没有什么特别，任何核函数都能做优化。
根据Eq5的简化公式，得知，在核函数的作用下，求和符号可以挪动。这就有两个优点了，1.本来kv计算好了，要和qu点乘一次再求exp，现在直接求，省去了点乘，有加速。2.从两层嵌套求和的N^2复杂度变成单独的两个求和，这其实是无关于核函数改进的优化。3.不用存储α score矩阵了。23其实很牵强，没用核函数也能做到。
Eq5由于全局消息聚合，所以有过度正则化的问题，即没用的节点虽然权重小，但架不住多。因此，作者要从全连通图中提取一个稀疏结构，类似于knn提取前k个相似节点做邻居一样。
【3.1 可微随机结构学习】
不知道什么叫可微随机结构。
Eq2是全局的老式attention计算公式，能得到相对于中心节点u对于任意节点v之间的边uv的attention αuv。这其实可以作为节点u周围节点出现的概率，即p(v|u)=αuv。因此，原则上我们可以通过随机采样的方式给节点生成一个新的图，并在此基础上做GCN。然而，反向传播没法用，因为你下次采样的图就不一样了，这条路行不通。
我们观察Eq6.在attention计算中，作者加入了一个干扰项gv(gw=gv s.t.w=v)。这是在干嘛？这是在以原始分布的基础上搞一个扭曲，这个扭曲是基于gumbel distribution的采样，所以这是要通过外部的采样做一个可微分(可学习)、不过拟合的、对分布的扭曲。该采样是节点之间独立的，并且每个节点只采一次，注意，只采一次的意思是采样的gv在任意att计算中都是一个值。理论上采样一次，但是为了稳定，作者采样5次并取平均。
【3.2】
Q:gumbel distribution作为一个连续采样，能否为离散分布的attention保持适定性(存在解、唯一解、解连续)？
A:适定性由特征维度、温度有关。
Q:Gumbel-softmax能否收敛为真实分布？
A:当满足1.映射的维度足够大2.温度->0时，对潜在拓扑采样的分布会收敛到原始分类分布。
【NodeFormer模型】
Figure1是模型框架，分为红蓝绿三部分。
红：这部分是上面理论的应用部分，通过核化gumbel-softmax算子对每个节点的消息聚合过程做更新，过程是核化gumbel-softmax+消息聚合->外部信息汇入->再重复。
蓝(可选)：引导gumbel-softamx算子把随机的边权重调整过程引向图自带的拓扑。指向的loss是基于原有拓扑A和调整拓扑A_hat的相似度loss。
绿(可选)：仅通过A做所有节点的嵌入，可以说是一种基于原有图拓扑的传播(以我的理解)。与蓝一样，也是为了加强原有拓扑的重要性。
【实验】
只应用于大图。
【评价】
该工作可以看出不是全图计算的类型，它可以逐点计算，因此它本身是为超大图准备的模型。
创新点在于：先把整张图所有节点之间基于核方法的attention算出来，作为节点的邻域分布，然后再在核化gumbel-softmax算子的帮助下用重参数方法可微地调整这个邻域分布，并引导这个分布向已知的拓扑方向转换。
【gumbel distribution和与gumbel distribution完全没关系的gumbel-softmax】
gumbel distribution是一个古老的分布函数，p(x)=e^(-z-e^(-z))/β，z=(x-μ)/β。
可以看到，该分布由参数μ和β控制，类似于正态分布的μ和σ的作用。
用该分布的好处是它具有长尾性，所以概率极小事件在gumbel distribution里也有一定的概率，相比于正态分布，极端事件的概率更大。
gumbel softmax的工作方式如No83讲解。

83. Categorical Reparameterization with Gumbel-Softmax || from google ||
【keywords】
gumbel-softmax、
【blog】
https://spaces.ac.cn/archives/6705
【abstract】
该技术用于离散分布的重参数采样。
连续分布做采样是因为没法做积分，分布太复杂积不了，所以只能靠采样。而采样又没法反向传播，所以用了重参数方法。
那么到了离散分布，积分就是求和，这回不难了吧？其实还是难，主要是类别太多的时候计算量很大，例如在NodeFormer里，那个节点量是10^4级的，选or不选，是2^N问题，这个量就天文数字了，根本没法做求和。
所以还是要采样。
首先是初级的，gumbel max。
它做到的仅仅是重采样，而不解决离散带来的无法求导问题。在blog中，Eq8给了一个看不懂的公式，它的结果等价于依照离散概率p1~pk采样一个类别。先依均匀分布采样k个ε1~εk，经过Eq8与pi融合，使得结果最大的那个i就是依照pi采样的那个类。它是重采样这点毋庸置疑，不过ε采样能与pi采样结果相同得证明一番（略）。
然后是高级的，gumbel softmax。
这里就能求导了。
内核仍然是Eq8的样式，但是外面先由温度T限制，再套上softmax。是很简单的gumbel max光滑化。
所以，gumbel softmax和gumbel distribution没有任何关系！只是非光滑的gumbel max用了gumbel这个名字而已。gumbel max可以被证明该重参数技巧可以让pi被抽到的概率还是pi。
gumbel softmax相比于抛弃gumbel max的直接(非重参数)采样，是用温度T逼近one-hot采样。如果仅仅用原有的pi做softmax([p1,p2...pk]/T)的退火，随着T->，只会让max_i pi的softmax值为1，导致得到唯一的采样。



84.Examining the Effects of Degree Distribution and Homophily in Graph Learning Models || 2023 || 0
【abstract】
cSBM是一个图生成工具，用于生成测试不同场景下图模型功能的工具。
最好用的叫GraphWorld。但是作者觉得这个功能在 度分布 和 同质率 设计上有缺陷，所以设计了几个新的。小论文，共6页，没必要看。
